{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nutanix Cloud Native Open Source Documentation","text":"<p>Welcome to the official home dedicated to documenting how to run Cloud Native applications on Nutanix. Here, you'll find information and code to enable the best-in-class Hybrid Cloud experience using the Nutanix Cloud Platform and Kubernetes.</p> <p>All Nutanix Cloud Native project documentation is open sourced in Markdown and transformed into HTML/CSS pages through MKDocs automation.</p> <p>Please consider contributing via the documentation source.</p>"},{"location":"anthos/architecture/","title":"Architecture","text":"<p>Google Anthos can be deployed on the Nutanix Cloud Infrastructure running Nutanix AHV hypervisor using the Anthos clusters on bare metal installation method. </p> <p>The Google Anthos nodes will be running as virtual machines on the Nutanix AHV hypervisor. The virtual machines need to run a validated operating system and version.  The list of validated operating systems to run Google Anthos on Nutanix AHV can be found in the Nutanix section of the partner platforms page.</p> <p>The Nutanix CSI driver can be used to consume the storage provided by the Nutanix Cloud Infrastructure through Kubernetes Persistent Volumes. </p> <p>See the diagram below for a high-level architecture overview:                 </p> <p>Refer to the Nutanix HCI Now Runs as Google Cloud's Anthos article for more information on the partnership.</p>"},{"location":"anthos/install/manual/","title":"Google Anthos Clusters on Bare Metal Manual Installation on Nutanix AHV","text":"<p>Note: The following Google Anthos versions have been tested on Nutanix AHV:</p> Google Anthos versions 1.6.x - 1.9.x, 1.13.x, 1.14.x <p>When installing Google Anthos clusters on bare metal on Nutanix AHV, it is required to manually create the virtual machines through Prism Central. </p> <p>These virtual machines need to run a validated operating system. The list of validated operating systems can be found in the Nutanix section of the partner platforms page.</p> <p>Google Anthos clusters on bare metal requires the following virtual machines types to be provisioned:</p> <ul> <li>Admin workstation (see admin workstation prerequisites)</li> <li>Cluster nodes (see cluster node machine prerequisites)</li> </ul> <p>Note</p> <p>The number of cluster node virtual machines that need to be created depend on the deployment type for the controlplane and the amount of worker nodes required for the application.</p> <p>Refer to the Anthos clusters on bare metal quickstart to get a detailed overview of the installation procedure of Google Anthos. </p> <p>Note</p> <p>When deploying the Anthos nodes on a Nutanix IPAM-managed subnet, make sure the <code>controlPlaneVIP</code>, <code>ingressVIP</code> and <code>addressPools</code> are not part of one of the IP-pools to prevent IP conflicts. </p>"},{"location":"capx/latest/credential_management/","title":"Credential Management","text":"<p>Cluster API Provider Nutanix Cloud Infrastructure (CAPX) interacts with Nutanix Prism Central (PC) APIs to manage the required Kubernetes cluster infrastructure resources.</p> <p>PC credentials are required to authenticate to the PC APIs. CAPX currently supports two mechanisms to supply the required credentials:</p> <ul> <li>Credentials injected into the CAPX manager deployment</li> <li>Workload cluster specific credentials</li> </ul>"},{"location":"capx/latest/credential_management/#credentials-injected-into-the-capx-manager-deployment","title":"Credentials injected into the CAPX manager deployment","text":"<p>By default, credentials will be injected into the CAPX manager deployment when CAPX is initialized. See the getting started guide for more information on the initialization.</p> <p>Upon initialization a <code>nutanix-creds</code> secret will automatically be created in the <code>capx-system</code> namespace. This secret will contain the values supplied via the <code>NUTANIX_USER</code> and <code>NUTANIX_PASSWORD</code> parameters. </p> <p>The <code>nutanix-creds</code> secret will be used for workload cluster deployment if no other credential is supplied.</p>"},{"location":"capx/latest/credential_management/#example","title":"Example","text":"<p>An example of the automatically created <code>nutanix-creds</code> secret can be found below: <pre><code>---\napiVersion: v1\nkind: Secret\ntype: Opaque\nmetadata:\nname: nutanix-creds\nnamespace: capx-system\nstringData:\ncredentials: |\n[\n{\n\"type\": \"basic_auth\", \n\"data\": { \n\"prismCentral\":{\n\"username\": \"&lt;nutanix-user&gt;\",\n\"password\": \"&lt;nutanix-password&gt;\"\n},\n\"prismElements\": null\n}\n}\n]\n</code></pre></p>"},{"location":"capx/latest/credential_management/#workload-cluster-specific-credentials","title":"Workload cluster specific credentials","text":"<p>Users can override the credentials injected in CAPX manager deployment by supplying a credential specific to a workload cluster. The credentials can be supplied by creating a secret in the same namespace as the <code>NutanixCluster</code> namespace. </p> <p>The secret can be referenced by adding a <code>credentialRef</code> inside the <code>prismCentral</code> attribute contained in the <code>NutanixCluster</code>.  The secret will also be deleted when the <code>NutanixCluster</code> is deleted.</p> <p>Note: There is a 1:1 relation between the secret and the <code>NutanixCluster</code> object. </p>"},{"location":"capx/latest/credential_management/#example_1","title":"Example","text":"<p>Create a secret in the namespace of the <code>NutanixCluster</code>:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\nname: \"&lt;my-secret&gt;\"\nnamespace: \"&lt;nutanixcluster-namespace&gt;\"\nstringData:\ncredentials: |\n[\n{\n\"type\": \"basic_auth\", \n\"data\": { \n\"prismCentral\":{\n\"username\": \"&lt;nutanix-user&gt;\",\n\"password\": \"&lt;nutanix-password&gt;\"\n},\n\"prismElements\": null\n}\n}\n]\n</code></pre> <p>Add a <code>prismCentral</code> and corresponding <code>credentialRef</code> to the <code>NutanixCluster</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixCluster\nmetadata:\nname: \"&lt;nutanixcluster-name&gt;\"\nnamespace: \"&lt;nutanixcluster-namespace&gt;\"\nspec:\nprismCentral:\n...\ncredentialRef:\nname: \"&lt;my-secret&gt;\"\nkind: Secret\n...\n</code></pre> <p>See the NutanixCluster documentation for all supported configuration parameters for the <code>prismCentral</code> and <code>credentialRef</code> attribute.</p>"},{"location":"capx/latest/getting_started/","title":"Getting Started","text":"<p>This is a guide on getting started with Cluster API Provider Nutanix Cloud Infrastructure (CAPX). To learn more about cluster API in more depth, check out the Cluster API book.</p> <p>For more information on how install the Nutanix CSI Driver on a CAPX cluster, visit Nutanix CSI Driver installation with CAPX.</p> <p>For more information on how CAPX handles credentials, visit Credential Management.</p> <p>For more information on the port requirements for CAPX, visit Port Requirements.</p>"},{"location":"capx/latest/getting_started/#production-workflow","title":"Production Workflow","text":""},{"location":"capx/latest/getting_started/#build-os-image-for-nutanixmachinetemplate-resource","title":"Build OS image for NutanixMachineTemplate resource","text":"<p>Cluster API Provider Nutanix Cloud Infrastructure (CAPX) uses the Image Builder project to build OS images used for the Nutanix machines. </p> <p>Follow the steps detailed in Building CAPI Images for Nutanix Cloud Platform (NCP) to use Image Builder on the Nutanix Cloud Platform.</p> <p>For a list of operating systems visit the OS image Configuration page.</p>"},{"location":"capx/latest/getting_started/#prerequisites-for-using-cluster-api-provider-nutanix-cloud-infrastructure","title":"Prerequisites for using Cluster API Provider Nutanix Cloud Infrastructure","text":"<p>The Cluster API installation section provides an overview of all required prerequisites:</p> <ul> <li>Common Prerequisites</li> <li>Install and/or configure a Kubernetes cluster</li> <li>Install clusterctl</li> <li>(Optional) Enabling Feature Gates</li> </ul> <p>Make sure these prerequisites have been met before moving to the Configure and Install Cluster API Provider Nutanix Cloud Infrastructure step.</p>"},{"location":"capx/latest/getting_started/#configure-and-install-cluster-api-provider-nutanix-cloud-infrastructure","title":"Configure and Install Cluster API Provider Nutanix Cloud Infrastructure","text":"<p>To initialize Cluster API Provider Nutanix Cloud Infrastructure, <code>clusterctl</code> requires the following variables, which should be set in either <code>~/.cluster-api/clusterctl.yaml</code> or as environment variables. <pre><code>NUTANIX_ENDPOINT: \"\"    # IP or FQDN of Prism Central\nNUTANIX_USER: \"\"        # Prism Central user\nNUTANIX_PASSWORD: \"\"    # Prism Central password\nNUTANIX_INSECURE: false # or true\n\nKUBERNETES_VERSION: \"v1.22.9\"\nWORKER_MACHINE_COUNT: 3\nNUTANIX_SSH_AUTHORIZED_KEY: \"\"\n\nNUTANIX_PRISM_ELEMENT_CLUSTER_NAME: \"\"\nNUTANIX_MACHINE_TEMPLATE_IMAGE_NAME: \"\"\nNUTANIX_SUBNET_NAME: \"\"\n</code></pre></p> <p>You can also see the required list of variables by running the following: <pre><code>clusterctl generate cluster mycluster -i nutanix --list-variables           \nRequired Variables:\n  - CONTROL_PLANE_ENDPOINT_IP\n  - KUBERNETES_VERSION\n  - NUTANIX_ENDPOINT\n  - NUTANIX_MACHINE_TEMPLATE_IMAGE_NAME\n  - NUTANIX_PASSWORD\n  - NUTANIX_PRISM_ELEMENT_CLUSTER_NAME\n  - NUTANIX_SSH_AUTHORIZED_KEY\n  - NUTANIX_SUBNET_NAME\n  - NUTANIX_USER\n\nOptional Variables:\n  - CONTROL_PLANE_ENDPOINT_PORT      (defaults to \"6443\")\n  - CONTROL_PLANE_MACHINE_COUNT      (defaults to 1)\n  - KUBEVIP_LB_ENABLE                (defaults to \"false\")\n  - KUBEVIP_SVC_ENABLE               (defaults to \"false\")\n  - NAMESPACE                        (defaults to current Namespace in the KubeConfig file)\n  - NUTANIX_INSECURE                 (defaults to \"false\")\n  - NUTANIX_MACHINE_BOOT_TYPE        (defaults to \"legacy\")\n  - NUTANIX_MACHINE_MEMORY_SIZE      (defaults to \"4Gi\")\n  - NUTANIX_MACHINE_VCPU_PER_SOCKET  (defaults to \"1\")\n  - NUTANIX_MACHINE_VCPU_SOCKET      (defaults to \"2\")\n  - NUTANIX_PORT                     (defaults to \"9440\")\n  - NUTANIX_SYSTEMDISK_SIZE          (defaults to \"40Gi\")\n  - WORKER_MACHINE_COUNT             (defaults to 0)\n</code></pre></p> <p>Note</p> <p>To prevent duplicate IP assignments, it is required to assign an IP-address to the <code>CONTROL_PLANE_ENDPOINT_IP</code> variable that is not part of the Nutanix IPAM or DHCP range assigned to the subnet of the CAPX cluster. </p> <p>Now you can instantiate Cluster API with the following: <pre><code>clusterctl init -i nutanix\n</code></pre></p>"},{"location":"capx/latest/getting_started/#deploy-a-workload-cluster-on-nutanix-cloud-infrastructure","title":"Deploy a workload cluster on Nutanix Cloud Infrastructure","text":"<p><pre><code>export TEST_CLUSTER_NAME=mytestcluster1\nexport TEST_NAMESPACE=mytestnamespace\nCONTROL_PLANE_ENDPOINT_IP=x.x.x.x clusterctl generate cluster ${TEST_CLUSTER_NAME} \\\n    -i nutanix \\\n    --target-namespace ${TEST_NAMESPACE}  \\\n    --kubernetes-version v1.22.9 \\\n    --control-plane-machine-count 1 \\\n    --worker-machine-count 3 &gt; ./cluster.yaml\nkubectl create ns ${TEST_NAMESPACE}\nkubectl apply -f ./cluster.yaml -n ${TEST_NAMESPACE}\n</code></pre> To customize the configuration of the default <code>cluster.yaml</code> file generated by CAPX, visit the  NutanixCluster and  NutanixMachineTemplate documentation.</p>"},{"location":"capx/latest/getting_started/#access-a-workload-cluster","title":"Access a workload cluster","text":"<p>To access resources on the cluster, you can get the kubeconfig with the following: <pre><code>clusterctl get kubeconfig ${TEST_CLUSTER_NAME} -n ${TEST_NAMESPACE} &gt; ${TEST_CLUSTER_NAME}.kubeconfig\nkubectl --kubeconfig ./${TEST_CLUSTER_NAME}.kubeconfig get nodes \n</code></pre></p>"},{"location":"capx/latest/getting_started/#install-cni-on-workload-a-cluster","title":"Install CNI on workload a cluster","text":"<p>You must deploy a Container Network Interface (CNI) based pod network add-on so that your pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed.</p> <p>Note</p> <p>Take care that your pod network must not overlap with any of the host networks. You are likely to see problems if there is any overlap. If you find a collision between your network plugin's preferred pod network and some of your host networks, you must choose a suitable alternative CIDR block to use instead. It can be configured inside the <code>cluster.yaml</code> generated by <code>clusterctl generate cluster</code> before applying it.</p> <p>Several external projects provide Kubernetes pod networks using CNI, some of which also support Network Policy.</p> <p>See a list of add-ons that implement the Kubernetes networking model. At time of writing, the most common are Calico and Cilium.</p> <p>Follow the specific install guide for your selected CNI and install only one pod network per cluster.</p> <p>Once a pod network has been installed, you can confirm that it is working by checking that the CoreDNS pod is running in the output of <code>kubectl get pods --all-namespaces</code>.</p>"},{"location":"capx/latest/getting_started/#kube-vip-settings","title":"Kube-vip settings","text":"<p>Kube-vip is a true load balancing solution for the Kubernetes control plane. It distributes API requests across control plane nodes. It also has the capability to provide load balancing for Kubernetes services.</p> <p>You can tweak kube-vip settings by using the following properties:</p> <ul> <li><code>KUBEVIP_LB_ENABLE</code></li> </ul> <p>This setting allows control plane load balancing using IPVS. See Control Plane Load-Balancing documentation for further information.</p> <ul> <li><code>KUBEVIP_SVC_ENABLE</code> </li> </ul> <p>This setting enables a service of type LoadBalancer. See Kubernetes Service Load Balancing documentation for further information.</p> <ul> <li><code>KUBEVIP_SVC_ELECTION</code></li> </ul> <p>This setting enables Load Balancing of Load Balancers. See Load Balancing Load Balancers for further information.</p>"},{"location":"capx/latest/getting_started/#delete-a-workload-cluster","title":"Delete a workload cluster","text":"<p>To remove a workload cluster from your management cluster, remove the cluster object and the provider will clean-up all resources. </p> <pre><code>kubectl delete cluster ${TEST_CLUSTER_NAME} -n ${TEST_NAMESPACE}\n</code></pre> <p>Note</p> <p>Deleting the entire cluster template with <code>kubectl delete -f ./cluster.yaml</code> may lead to pending resources requiring manual cleanup.</p>"},{"location":"capx/latest/pc_certificates/","title":"Certificate Trust","text":"<p>CAPX invokes Prism Central APIs using the HTTPS protocol. CAPX has different methods to handle the trust of the Prism Central certificates:</p> <ul> <li>Enable certificate verification (default)</li> <li>Configure an additional trust bundle</li> <li>Disable certificate verification</li> </ul> <p>See the respective sections below for more information.</p> <p>Note</p> <p>For more information about replacing Prism Central certificates, see the Nutanix AOS Security Guide.</p>"},{"location":"capx/latest/pc_certificates/#enable-certificate-verification-default","title":"Enable certificate verification (default)","text":"<p>By default CAPX will perform certificate verification when invoking Prism Central API calls. This requires Prism Central to be configured with a publicly trusted certificate authority.  No additional configuration is required in CAPX.</p>"},{"location":"capx/latest/pc_certificates/#configure-an-additional-trust-bundle","title":"Configure an additional trust bundle","text":"<p>CAPX allows users to configure an additional trust bundle. This will allow CAPX to verify certificates that are not issued by a publicy trusted certificate authority. </p> <p>To configure an additional trust bundle, the <code>NUTANIX_ADDITIONAL_TRUST_BUNDLE</code> environment variable needs to be set. The value of the <code>NUTANIX_ADDITIONAL_TRUST_BUNDLE</code> environment variable contains the trust bundle (PEM format) in base64 encoded format. See the Configuring the trust bundle environment variable section for more information.</p> <p>It is also possible to configure the additional trust bundle manually by creating a custom <code>cluster-template</code>. See the Configuring the additional trust bundle manually  section for more information</p> <p>The <code>NUTANIX_ADDITIONAL_TRUST_BUNDLE</code> environment variable can be set when initializing the CAPX provider or when creating a workload cluster. If the <code>NUTANIX_ADDITIONAL_TRUST_BUNDLE</code> is configured when the CAPX provider is initialized, the additional trust bundle will be used for every CAPX workload cluster. If it is only configured when creating a workload cluster, it will only be applicable for that specific workload cluster.</p>"},{"location":"capx/latest/pc_certificates/#configuring-the-trust-bundle-environment-variable","title":"Configuring the trust bundle environment variable","text":"<p>Create a PEM encoded file containing the root certificate and all intermediate certificates. Example: <pre><code>$ cat cert.crt\n-----BEGIN CERTIFICATE-----\n&lt;certificate string&gt;\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n&lt;certificate string&gt;\n-----END CERTIFICATE-----\n</code></pre></p> <p>Use a <code>base64</code> tool to encode these contents in base64. The command below will provide a <code>base64</code> string. <pre><code>$ cat cert.crt | base64\n&lt;base64 string&gt;\n</code></pre></p> <p>Note</p> <p>Make sure the <code>base64</code> string does not contain any newlines (<code>\\n</code>). If the output string contains newlines, remove them manually or check the manual of the <code>base64</code> tool on how to generate a <code>base64</code> string without newlines. </p> <p>Use the <code>base64</code> string as value for the <code>NUTANIX_ADDITIONAL_TRUST_BUNDLE</code> environment variable. <pre><code>$ export NUTANIX_ADDITIONAL_TRUST_BUNDLE=\"&lt;base64 string&gt;\"\n</code></pre></p>"},{"location":"capx/latest/pc_certificates/#configuring-the-additional-trust-bundle-manually","title":"Configuring the additional trust bundle manually","text":"<p>To configure the additional trust bundle manually without using the <code>NUTANIX_ADDITIONAL_TRUST_BUNDLE</code> environment variable present in the default <code>cluster-template</code> files, it is required to:</p> <ul> <li>Create a <code>ConfigMap</code> containing the additional trust bundle.</li> <li>Configure the <code>prismCentral.additionalTrustBundle</code> object in the <code>NutanixCluster</code> spec.</li> </ul>"},{"location":"capx/latest/pc_certificates/#creating-the-additional-trust-bundle-configmap","title":"Creating the additional trust bundle ConfigMap","text":"<p>CAPX supports two different formats for the ConfigMap containing the additional trust bundle. The first one is to add the additional trust bundle as a multi-line string in the <code>ConfigMap</code>, the second option is to add the trust bundle in <code>base64</code> encoded format. See the examples below.</p> <p>Multi-line string example: <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: user-ca-bundle\nnamespace: ${NAMESPACE}\ndata:\nca.crt: |\n-----BEGIN CERTIFICATE-----\n&lt;certificate string&gt;\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n&lt;certificate string&gt;\n-----END CERTIFICATE-----\n</code></pre></p> <p><code>base64</code> example:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: user-ca-bundle\nnamespace: ${NAMESPACE}\nbinaryData:\nca.crt: &lt;base64 string&gt;\n</code></pre> <p>Note</p> <p>The <code>base64</code> string needs to be added as <code>binaryData</code>.</p>"},{"location":"capx/latest/pc_certificates/#configuring-the-nutanixcluster-spec","title":"Configuring the NutanixCluster spec","text":"<p>When the additional trust bundle <code>ConfigMap</code> is created, it needs to be referenced in the <code>NutanixCluster</code> spec. Add the <code>prismCentral.additionalTrustBundle</code> object in the <code>NutanixCluster</code> spec as shown below. Make sure the correct additional trust bundle <code>ConfigMap</code> is referenced.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixCluster\nmetadata:\nname: ${CLUSTER_NAME}\nnamespace: ${NAMESPACE}\nspec:\n...\nprismCentral:\n...\nadditionalTrustBundle:\nkind: ConfigMap\nname: user-ca-bundle\ninsecure: false\n</code></pre> <p>Note</p> <p>the default value of <code>prismCentral.insecure</code> attribute is <code>false</code>. It can be omitted when an additional trust bundle is configured. </p> <p>If <code>prismCentral.insecure</code> attribute is set to <code>true</code>, all certificate verification will be disabled. </p>"},{"location":"capx/latest/pc_certificates/#disable-certificate-verification","title":"Disable certificate verification","text":"<p>Note</p> <p>Disabling certificate verification is not recommended for production purposes and should only be used for testing.</p> <p>Certificate verification can be disabled by setting the <code>prismCentral.insecure</code> attribute to <code>true</code> in the <code>NutanixCluster</code> spec. Certificate verification will be disabled even if an additional trust bundle is configured. </p> <p>Disabled certificate verification example:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixCluster\nmetadata:\nname: ${CLUSTER_NAME}\nnamespace: ${NAMESPACE}\nspec:\ncontrolPlaneEndpoint:\nhost: ${CONTROL_PLANE_ENDPOINT_IP}\nport: ${CONTROL_PLANE_ENDPOINT_PORT=6443}\nprismCentral:\n...\ninsecure: true\n...\n</code></pre>"},{"location":"capx/latest/port_requirements/","title":"Port Requirements","text":"<p>CAPX uses the ports documented below to create workload clusters. </p> <p>Note<p>This page only documents the ports specifically required by CAPX and does not provide the full overview of all ports required in the CAPI framework.</p> </p>"},{"location":"capx/latest/port_requirements/#management-cluster","title":"Management cluster","text":"Source Destination Protocol Port Description Management cluster External Registries TCP 443 Pull container images from CAPX public registries Management cluster Prism Central TCP 9440 Management cluster communication to Prism Central"},{"location":"capx/latest/port_requirements/#public-registries-utilized-when-using-capx","title":"Public registries utilized when using CAPX","text":"Registry name ghcr.io"},{"location":"capx/latest/troubleshooting/","title":"Troubleshooting","text":""},{"location":"capx/latest/troubleshooting/#clusterctl-failed-with-github-rate-limit-error","title":"Clusterctl failed with GitHub rate limit error","text":"<p>By design Clusterctl fetches artifacts from repositories hosted on GitHub, this operation is subject to GitHub API rate limits.</p> <p>While this is generally okay for the majority of users, there is still a chance that some users (especially developers or CI tools) hit this limit:</p> <pre><code>Error: failed to get repository client for the XXX with name YYY: error creating the GitHub repository client: failed to get GitHub latest version: failed to get the list of versions: rate limit for github api has been reached. Please wait one hour or get a personal API tokens a assign it to the GITHUB_TOKEN environment variable\n</code></pre> <p>As explained in the error message, you can increase your API rate limit by creating a GitHub personal token and setting a <code>GITHUB_TOKEN</code> environment variable using the token.</p>"},{"location":"capx/latest/validated_integrations/","title":"Validated Integrations","text":"<p>Validated integrations are a defined set of specifically tested configurations between technologies that represent the most common combinations that Nutanix customers are using or deploying with CAPX. For these integrations, Nutanix has directly, or through certified partners, exercised a full range of platform tests as part of the product release process.</p>"},{"location":"capx/latest/validated_integrations/#integration-validation-policy","title":"Integration Validation Policy","text":"<p>Nutanix follows the version validation policies below:</p> <ul> <li> <p>Validate at least one active AOS LTS (long term support) version. Validated AOS LTS version for a specific CAPX version is listed in the AOS section.</p> <p>Note</p> <p>Typically the latest LTS release at time of CAPX release except when latest is initial release in train (eg x.y.0). Exact version depends on timing and customer adoption.</p> </li> <li> <p>Validate the latest AOS STS (short term support) release at time of CAPX release.</p> </li> <li> <p>Validate at least one active Prism Central (PC) version. Validated PC version for a specific CAPX version is listed in the Prism Central section.</p> <p>Note</p> <p>Typically the the latest PC release at time of CAPX release except when latest is initial release in train (eg x.y.0). Exact version depends on timing and customer adoption.</p> </li> <li> <p>At least one active Cluster-API (CAPI) version. Validated CAPI version for a specific CAPX version is listed in the Cluster-API section.</p> <p>Note</p> <p>Typically the the latest Cluster-API release at time of CAPX release except when latest is initial release in train (eg x.y.0). Exact version depends on timing and customer adoption.</p> </li> </ul>"},{"location":"capx/latest/validated_integrations/#validated-versions","title":"Validated versions","text":""},{"location":"capx/latest/validated_integrations/#cluster-api","title":"Cluster-API","text":"CAPX CAPI v1.1.4+ CAPI v1.2.x v1.0.x Yes Yes v0.5.x Yes Yes <p>See the Validated Kubernetes Versions page for more information on CAPI validated versions.</p>"},{"location":"capx/latest/validated_integrations/#aos","title":"AOS","text":"CAPX 5.20.4.5 (LTS) 6.1.1.5 (STS) v1.0.x Yes Yes v0.5.x Yes Yes"},{"location":"capx/latest/validated_integrations/#prism-central","title":"Prism Central","text":"CAPX 2022.1.0.2 pc.2022.6 v1.0.x Yes Yes v0.5.x Yes Yes"},{"location":"capx/latest/addons/install_csi_driver/","title":"Nutanix CSI Driver installation with CAPX","text":"<p>The Nutanix CSI driver is fully supported on CAPI/CAPX deployed clusters where all the nodes meet the Nutanix CSI driver prerequisites.</p> <p>There are three methods to install the Nutanix CSI driver on a CAPI/CAPX cluster:</p> <ul> <li>Helm</li> <li>ClusterResourceSet</li> <li>CAPX Flavor</li> </ul> <p>For more information, check the next sections.</p>"},{"location":"capx/latest/addons/install_csi_driver/#capi-workload-cluster-prerequisites-for-the-nutanix-csi-driver","title":"CAPI Workload cluster prerequisites for the Nutanix CSI Driver","text":"<p>Kubernetes workers need the following prerequisites to use the Nutanix CSI Drivers:</p> <ul> <li>iSCSI initiator package (for Volumes based block storage)</li> <li>NFS client package (for Files based storage)</li> </ul> <p>These packages may already be present in the image you use with your infrastructure provider or you can also rely on your bootstrap provider to install them. More info is available in the Prerequisites docs.</p> <p>The package names and installation method will also vary depending on the operating system you plan to use.</p> <p>In the example below, <code>kubeadm</code> bootstrap provider is used to deploy these packages on top of an Ubuntu 20.04 image. The <code>kubeadm</code> bootstrap provider allows defining <code>preKubeadmCommands</code> that will be launched before Kubernetes cluster creation. These <code>preKubeadmCommands</code> can be defined both in <code>KubeadmControlPlane</code> for master nodes and in <code>KubeadmConfigTemplate</code> for worker nodes.</p> <p>In the example with an Ubuntu 20.04 image, both <code>KubeadmControlPlane</code> and <code>KubeadmConfigTemplate</code> must be modified as in the example below:</p> <pre><code>spec:\ntemplate:\nspec:\n# .......\npreKubeadmCommands:\n- echo \"before kubeadm call\" &gt; /var/log/prekubeadm.log\n- apt update\n- apt install -y nfs-common open-iscsi\n- systemctl enable --now iscsid\n</code></pre>"},{"location":"capx/latest/addons/install_csi_driver/#install-the-nutanix-csi-driver-with-helm","title":"Install the Nutanix CSI Driver with Helm","text":"<p>A recent Helm version is needed (tested with Helm v3.10.1).</p> <p>The example below must be applied on a ready workload cluster. The workload cluster's kubeconfig can be retrieved and used to connect with the following command:</p> <pre><code>clusterctl get kubeconfig $CLUSTER_NAME -n $CLUSTER_NAMESPACE &gt; $CLUSTER_NAME-KUBECONFIG\nexport KUBECONFIG=$(pwd)/$CLUSTER_NAME-KUBECONFIG\n</code></pre> <p>Once connected to the cluster, follow the CSI documentation. </p> <p>First, install the nutanix-csi-snapshot chart followed by the nutanix-csi-storage chart.</p> <p>See an example below:</p> <pre><code>#Add the official Nutanix Helm repo and get the latest update\nhelm repo add nutanix https://nutanix.github.io/helm/\nhelm repo update\n\n# Install the nutanix-csi-snapshot chart\nhelm install nutanix-csi-snapshot nutanix/nutanix-csi-snapshot -n ntnx-system --create-namespace\n\n# Install the nutanix-csi-storage chart\nhelm install nutanix-storage nutanix/nutanix-csi-storage -n ntnx-system --set createSecret=false\n</code></pre> <p>Warning</p> <p>For correct Nutanix CSI driver deployment, a fully functional CNI deployment must be present.</p>"},{"location":"capx/latest/addons/install_csi_driver/#install-the-nutanix-csi-driver-with-clusterresourceset","title":"Install the Nutanix CSI Driver with <code>ClusterResourceSet</code>","text":"<p>The <code>ClusterResourceSet</code> feature was introduced to automatically apply a set of resources (such as CNI/CSI) defined by administrators to matching created/existing workload clusters.</p>"},{"location":"capx/latest/addons/install_csi_driver/#enabling-the-clusterresourceset-feature","title":"Enabling the <code>ClusterResourceSet</code> feature","text":"<p>At the time of writing, <code>ClusterResourceSet</code> is an experimental feature that must be enabled during the initialization of a management cluster with the <code>EXP_CLUSTER_RESOURCE_SET</code> feature gate.</p> <p>To do this, add <code>EXP_CLUSTER_RESOURCE_SET: \"true\"</code> in the <code>clusterctl</code> configuration file or just <code>export EXP_CLUSTER_RESOURCE_SET=true</code> before initializing the management cluster with <code>clusterctl init</code>.</p> <p>If the management cluster is already initialized, the <code>ClusterResourceSet</code> can be enabled by changing the configuration of the <code>capi-controller-manager</code> deployment in the <code>capi-system</code> namespace.</p> <pre><code>kubectl edit deployment -n capi-system capi-controller-manager\n</code></pre> <p>Locate the section below:</p> <pre><code>  - args:\n- --leader-elect\n- --metrics-bind-addr=localhost:8080\n- --feature-gates=MachinePool=false,ClusterResourceSet=true,ClusterTopology=false\n</code></pre> <p>Then replace <code>ClusterResourceSet=false</code> with <code>ClusterResourceSet=true</code>.</p> <p>Note</p> <p>Editing the <code>deployment</code> resource will cause Kubernetes to automatically start new versions of the containers with the feature enabled.</p>"},{"location":"capx/latest/addons/install_csi_driver/#prepare-the-nutanix-csi-clusterresourceset","title":"Prepare the Nutanix CSI <code>ClusterResourceSet</code>","text":""},{"location":"capx/latest/addons/install_csi_driver/#create-the-configmap-for-the-cni-plugin","title":"Create the <code>ConfigMap</code> for the CNI Plugin","text":"<p>First, create a <code>ConfigMap</code> that contains a YAML manifest with all resources to install the Nutanix CSI driver.</p> <p>Since the Nutanix CSI Driver is provided as a Helm chart, use <code>helm</code> to extract it before creating the <code>ConfigMap</code>. See an example below:</p> <pre><code>helm repo add nutanix https://nutanix.github.io/helm/\nhelm repo update\n\nkubectl create ns ntnx-system --dry-run=client -o yaml &gt; nutanix-csi-namespace.yaml\nhelm template nutanix-csi-snapshot nutanix/nutanix-csi-snapshot -n ntnx-system &gt; nutanix-csi-snapshot.yaml\nhelm template nutanix-csi-snapshot nutanix/nutanix-csi-storage -n ntnx-system &gt; nutanix-csi-storage.yaml\n\nkubectl create configmap nutanix-csi-crs --from-file=nutanix-csi-namespace.yaml --from-file=nutanix-csi-snapshot.yaml --from-file=nutanix-csi-storage.yaml\n</code></pre>"},{"location":"capx/latest/addons/install_csi_driver/#create-the-clusterresourceset","title":"Create the <code>ClusterResourceSet</code>","text":"<p>Next, create the <code>ClusterResourceSet</code> resource that will map the <code>ConfigMap</code> defined above to clusters using a <code>clusterSelector</code>.</p> <p>The <code>ClusterResourceSet</code> needs to be created inside the management cluster. See an example below:</p> <pre><code>---\napiVersion: addons.cluster.x-k8s.io/v1alpha3\nkind: ClusterResourceSet\nmetadata:\nname: nutanix-csi-crs\nspec:\nclusterSelector:\nmatchLabels:\ncsi: nutanix resources:\n- kind: ConfigMap\nname: nutanix-csi-crs\n</code></pre> <p>The <code>clusterSelector</code> field controls how Cluster API will match this <code>ClusterResourceSet</code> on one or more workload clusters. In the example scenario, the <code>matchLabels</code> approach is being used where the <code>ClusterResourceSet</code> will be applied to all workload clusters having the <code>csi: nutanix</code> label present. If the label isn't present, the <code>ClusterResourceSet</code> won't apply to that workload cluster.</p> <p>The <code>resources</code> field references the <code>ConfigMap</code> created above, which contains the manifests for installing the Nutanix CSI driver.</p>"},{"location":"capx/latest/addons/install_csi_driver/#assign-the-clusterresourceset-to-a-workload-cluster","title":"Assign the <code>ClusterResourceSet</code> to a workload cluster","text":"<p>Assign this <code>ClusterResourceSet</code> to the workload cluster by adding the correct label to the <code>Cluster</code> resource.</p> <p>This can be done before workload cluster creation by editing the output of the <code>clusterctl generate cluster</code> command or by modifying an already deployed workload cluster.</p> <p>In both cases, <code>Cluster</code> resources should look like this:</p> <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\nmetadata:\nname: workload-cluster-name\nnamespace: workload-cluster-namespace\nlabels:\ncsi: nutanix\n# ...\n</code></pre> <p>Warning</p> <p>For correct Nutanix CSI driver deployment, a fully functional CNI deployment must be present.</p>"},{"location":"capx/latest/addons/install_csi_driver/#install-the-nutanix-csi-driver-with-a-capx-flavor","title":"Install the Nutanix CSI Driver with a CAPX flavor","text":"<p>The CAPX provider can utilize a flavor to automatically deploy the Nutanix CSI using a <code>ClusterResourceSet</code>.</p>"},{"location":"capx/latest/addons/install_csi_driver/#prerequisites","title":"Prerequisites","text":"<p>The following requirements must be met:</p> <ul> <li>The operating system must meet the Nutanix CSI OS prerequisites.</li> <li>The Management cluster must be installed with the <code>CLUSTER_RESOURCE_SET</code> feature gate.</li> </ul>"},{"location":"capx/latest/addons/install_csi_driver/#installation","title":"Installation","text":"<p>Specify the <code>csi</code> flavor during workload cluster creation. See an example below:</p> <pre><code>clusterctl generate cluster my-cluster -f csi\n</code></pre> <p>Additional environment variables are required:</p> <ul> <li><code>WEBHOOK_CA</code>: Base64 encoded CA certificate used to sign the webhook certificate</li> <li><code>WEBHOOK_CERT</code>: Base64 certificate for the webhook validation component</li> <li><code>WEBHOOK_KEY</code>: Base64 key for the webhook validation component</li> </ul> <p>The three components referenced above can be automatically created and referenced using this script:</p> <pre><code>source scripts/gen-self-cert.sh\n</code></pre> <p>The certificate must reference the following names:</p> <ul> <li>csi-snapshot-webhook</li> <li>csi-snapshot-webhook.ntnx-sytem</li> <li>csi-snapshot-webhook.ntnx-sytem.svc</li> </ul> <p>Warning</p> <p>For correct Nutanix CSI driver deployment, a fully functional CNI deployment must be present.</p>"},{"location":"capx/latest/addons/install_csi_driver/#nutanix-csi-driver-configuration","title":"Nutanix CSI Driver Configuration","text":"<p>After the driver is installed, it must be configured for use by minimally defining a <code>Secret</code> and <code>StorageClass</code>.</p> <p>This can be done manually in the workload clusters or by using a <code>ClusterResourceSet</code> in the management cluster as explained above.</p> <p>See the Official CSI Driver documentation on the Nutanix Portal for more configuration information. </p>"},{"location":"capx/latest/experimental/autoscaler/","title":"Using Autoscaler in combination with CAPX","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>Autoscaler can be used in combination with Cluster API to automatically add or remove machines in a cluster. </p> <p>Autoscaler can be used in different deployment scenarios. This page will provide an overview of multiple autoscaler deployment scenarios in combination with CAPX. See the Testing section to see how scale-up/scale-down events can be triggered to validate the autoscaler behaviour.</p> <p>More in-depth information on Autoscaler functionality can be found in the Kubernetes documentation.</p> <p>All Autoscaler configuration parameters can be found here.</p>"},{"location":"capx/latest/experimental/autoscaler/#scenario-1-management-cluster-managing-an-external-workload-cluster","title":"Scenario 1: Management cluster managing an external workload cluster","text":"<p>In this scenario, Autoscaler will be running on a management cluster and it will manage an external workload cluster. See the management cluster managing an external workload cluster section of Kubernetes documentation for more information.</p>"},{"location":"capx/latest/experimental/autoscaler/#steps","title":"Steps","text":"<ol> <li> <p>Deploy a management cluster and workload cluster. The CAPI quickstart can be used as a starting point.</p> <p>Note<p>Make sure a CNI is installed in the workload cluster.</p> </p> </li> <li> <p>Download the example Autoscaler deployment file.</p> </li> <li>Modify the <code>deployment.yaml</code> file:<ul> <li>Change the namespace of all resources to the namespaces of the workload cluster.</li> <li>Choose an autoscale image.</li> <li>Change the following parameters in the <code>Deployment</code> resource: <pre><code>        spec:\ncontainers:\nname: cluster-autoscaler\ncommand:\n- /cluster-autoscaler\nargs:\n- --cloud-provider=clusterapi\n- --kubeconfig=/mnt/kubeconfig/kubeconfig.yml\n- --clusterapi-cloud-config-authoritative\n- -v=1\nvolumeMounts:\n- mountPath: /mnt/kubeconfig\nname: kubeconfig\nreadOnly: true\n...\nvolumes:\n- name: kubeconfig\nsecret:\nsecretName: &lt;workload cluster name&gt;-kubeconfig\nitems:\n- key: value\npath: kubeconfig.yml\n</code></pre></li> </ul> </li> <li>Apply the <code>deployment.yaml</code> file. <pre><code>kubectl apply -f deployment.yaml\n</code></pre></li> <li>Add the annotations to the workload cluster <code>MachineDeployment</code> resource.</li> <li>Test Autoscaler. Go to the Testing section.</li> </ol>"},{"location":"capx/latest/experimental/autoscaler/#scenario-2-autoscaler-running-on-workload-cluster","title":"Scenario 2: Autoscaler running on workload cluster","text":"<p>In this scenario, Autoscaler will be deployed on top of the workload cluster directly. In order for Autoscaler to work, it is required that the workload cluster resources are moved from the management cluster to the workload cluster.</p>"},{"location":"capx/latest/experimental/autoscaler/#steps_1","title":"Steps","text":"<ol> <li>Deploy a management cluster and workload cluster. The CAPI quickstart can be used as a starting point.</li> <li>Get the kubeconfig file for the workload cluster and use this kubeconfig to login to the workload cluster.  <pre><code>clusterctl get kubeconfig &lt;workload cluster name&gt; -n &lt;workload cluster namespace &gt; /path/to/kubeconfig\n</code></pre></li> <li>Install a CNI in the workload cluster.</li> <li>Initialise the CAPX components on top of the workload cluster: <pre><code>clusterctl init --infrastructure nutanix\n</code></pre></li> <li>Migrate the workload cluster custom resources to the workload cluster. Run following command from the management cluster: <pre><code>clusterctl move -n &lt;workload cluster ns&gt;  --to-kubeconfig /path/to/kubeconfig\n</code></pre></li> <li>Verify if the cluster has been migrated by running following command on the workload cluster: <pre><code>kubectl get cluster -A </code></pre></li> <li>Download the example autoscaler deployment file.</li> <li>Create the Autoscaler namespace: <pre><code>kubectl create ns autoscaler\n</code></pre></li> <li>Apply the <code>deployment.yaml</code> file <pre><code>kubectl apply -f deployment.yaml\n</code></pre></li> <li>Add the annotations to the workload cluster <code>MachineDeployment</code> resource.</li> <li>Test Autoscaler. Go to the Testing section.</li> </ol>"},{"location":"capx/latest/experimental/autoscaler/#testing","title":"Testing","text":"<ol> <li>Deploy an example Kubernetes application. For example, the one used in the Kubernetes HorizontalPodAutoscaler Walkthrough. <pre><code>kubectl apply -f https://k8s.io/examples/application/php-apache.yaml </code></pre></li> <li>Increase the amount of replicas of the application to trigger a scale-up event: <pre><code>kubectl scale deployment php-apache --replicas 100\n</code></pre></li> <li> <p>Decrease the amount of replicas of the application again to trigger a scale-down event.</p> <p>Note</p> <p>In case of issues check the logs of the Autoscaler pods.</p> </li> <li> <p>After a while CAPX, will add more machines. Refer to the Autoscaler configuration parameters to tweak the behaviour and timeouts.</p> </li> </ol>"},{"location":"capx/latest/experimental/autoscaler/#autoscaler-node-group-annotations","title":"Autoscaler node group annotations","text":"<p>Autoscaler uses following annotations to define the upper and lower boundries of the managed machines:</p> Annotation Example Value Description cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size 5 Maximum amount of machines in this node group cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size 1 Minimum amount of machines in this node group <p>These annotations must be applied to the <code>MachineDeployment</code> resources of a CAPX cluster. </p>"},{"location":"capx/latest/experimental/autoscaler/#example","title":"Example","text":"<pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: MachineDeployment\nmetadata:\nannotations:\ncluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \"5\"\ncluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \"1\"\n</code></pre>"},{"location":"capx/latest/experimental/capx_multi_pe/","title":"Creating a workload CAPX cluster spanning Prism Element clusters","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>This page will explain how to deploy CAPX-based Kubernetes clusters where worker nodes are spanning multiple Prism Element (PE) clusters. </p> <p>Note<p>All the PE clusters must be managed by the same Prism Central (PC) instance.</p> </p> <p>The topology will look like this: </p> <ul> <li>One PC managing multiple PE's</li> <li>One CAPI management cluster</li> <li>One CAPI workload cluster with multiple <code>MachineDeployment</code>resources</li> </ul> <p>Refer to the CAPI quickstart to get started with CAPX. </p> <p>To create workload clusters spanning multiple Prism Element clusters, it is required to create a <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resource for each Prism Element cluster. The Prism Element specific parameters (name/UUID, subnet,...) are referenced in the <code>NutanixMachineTemplate</code>. </p>"},{"location":"capx/latest/experimental/capx_multi_pe/#steps","title":"Steps","text":"<ol> <li>Create a management cluster that has the CAPX infrastructure provider deployed.</li> <li>Create a <code>cluster.yml</code> file containing the workload cluster definition. Refer to the steps defined in the CAPI quickstart guide to create an example <code>cluster.yml</code> file.</li> <li> <p>Add additional <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resources.</p> <p>By default there is only one machine template and machine deployment defined. To add nodes residing on another Prism Element cluster, a new <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resource needs to be added to the yaml file. The autogenerated <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resource definitions can be used as a baseline.</p> <p>Make sure to modify the <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> parameters.</p> </li> <li> <p>Apply the modified <code>cluster.yml</code> file to the management cluster.</p> </li> </ol>"},{"location":"capx/latest/experimental/oidc/","title":"OIDC integration","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>Kubernetes allows users to authenticate using various authentication mechanisms. One of these mechanisms is OIDC. Information on how Kubernetes interacts with OIDC providers can be found in the OpenID Connect Tokens section of the official Kubernetes documentation. </p> <p>Follow the steps below to configure a CAPX cluster to use an OIDC identity provider.</p>"},{"location":"capx/latest/experimental/oidc/#steps","title":"Steps","text":"<ol> <li>Generate a <code>cluster.yaml</code> file with the required CAPX cluster configuration. Refer to the Getting Started page for more information on how to generate a <code>cluster.yaml</code> file. Do not apply the <code>cluster.yaml</code> file. </li> <li>Edit the <code>cluster.yaml</code> file and search for the <code>KubeadmControlPlane</code> resource.</li> <li>Modify/add the <code>spec.kubeadmConfigSpec.clusterConfiguration.apiServer.extraArgs</code> attribute and add the required API server parameters. See the example below.</li> <li>Apply the <code>cluster.yaml</code> file </li> <li>Log in with the OIDC provider once the cluster is provisioned</li> </ol>"},{"location":"capx/latest/experimental/oidc/#example","title":"Example","text":"<pre><code>kind: KubeadmControlPlane\nspec:\nkubeadmConfigSpec:\nclusterConfiguration:\napiServer:\nextraArgs:\n...\noidc-client-id: &lt;oidc-client-id&gt;\noidc-issuer-url: &lt;oidc-issuer-url&gt;\n...\n</code></pre>"},{"location":"capx/latest/experimental/proxy/","title":"Proxy configuration","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>CAPX can be configured to use a proxy to connect to external networks. This proxy configuration needs to be applied to control plane and worker nodes. </p> <p>Follow the steps below to configure a CAPX cluster to use a proxy.</p>"},{"location":"capx/latest/experimental/proxy/#steps","title":"Steps","text":"<ol> <li>Generate a <code>cluster.yaml</code> file with the required CAPX cluster configuration. Refer to the Getting Started page for more information on how to generate a <code>cluster.yaml</code> file. Do not apply the <code>cluster.yaml</code> file. </li> <li>Edit the <code>cluster.yaml</code> file and modify the following resources as shown in the example below to add the proxy configuration.<ol> <li><code>KubeadmControlPlane</code>: <ul> <li>Add the proxy configuration to the <code>spec.kubeadmConfigSpec.files</code> list. Do not modify other items in the list.</li> <li>Add <code>systemctl</code> commands to apply the proxy config in <code>spec.kubeadmConfigSpec.preKubeadmCommands</code>. Do not modify other items in the list.</li> </ul> </li> <li><code>KubeadmConfigTemplate</code>: <ul> <li>Add the proxy configuration to the <code>spec.template.spec.files</code> list. Do not modify other items in the list.</li> <li>Add <code>systemctl</code> commands to apply the proxy config in <code>spec.template.spec.preKubeadmCommands</code>. Do not modify other items in the list.</li> </ul> </li> </ol> </li> <li>Apply the <code>cluster.yaml</code> file </li> </ol>"},{"location":"capx/latest/experimental/proxy/#example","title":"Example","text":"<pre><code>---\n# controlplane proxy settings\nkind: KubeadmControlPlane\nspec:\nkubeadmConfigSpec:\nfiles:\n- content: |\n[Service]\nEnvironment=\"HTTP_PROXY=&lt;my-http-proxy-configuration&gt;\"\nEnvironment=\"HTTPS_PROXY=&lt;my-https-proxy-configuration&gt;\"\nEnvironment=\"NO_PROXY=&lt;my-no-proxy-configuration&gt;\"\nowner: root:root\npath: /etc/systemd/system/containerd.service.d/http-proxy.conf\n...\npreKubeadmCommands:\n- sudo systemctl daemon-reload\n- sudo systemctl restart containerd\n...\n---\n# worker proxy settings\nkind: KubeadmConfigTemplate\nspec:\ntemplate:\nspec:\nfiles:\n- content: |\n[Service]\nEnvironment=\"HTTP_PROXY=&lt;my-http-proxy-configuration&gt;\"\nEnvironment=\"HTTPS_PROXY=&lt;my-https-proxy-configuration&gt;\"\nEnvironment=\"NO_PROXY=&lt;my-no-proxy-configuration&gt;\"\nowner: root:root\npath: /etc/systemd/system/containerd.service.d/http-proxy.conf\n...\npreKubeadmCommands:\n- sudo systemctl daemon-reload\n- sudo systemctl restart containerd\n...\n</code></pre>"},{"location":"capx/latest/experimental/vpc/","title":"Creating a workload CAPX cluster in a Nutanix Flow VPC","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>Note</p> <p>Nutanix Flow VPCs are only validated with CAPX 1.1.3+</p> <p>Nutanix Flow Virtual Networking allows users to create Virtual Private Clouds (VPCs) with Overlay networking.  The steps below will illustrate how a CAPX cluster can be deployed inside an overlay subnet (NAT) inside a VPC while the management cluster resides outside of the VPC.</p>"},{"location":"capx/latest/experimental/vpc/#steps","title":"Steps","text":"<ol> <li>Request a floating IP</li> <li>Link the floating IP to an internal IP address inside the overlay subnet that will be used to deploy the CAPX cluster. This address will be assigned to the CAPX loadbalancer. To prevent IP conflicts, make sure the IP address is not part of the IP-pool defined in the subnet. </li> <li>Generate a <code>cluster.yaml</code> file with the required CAPX cluster configuration where the <code>CONTROL_PLANE_ENDPOINT_IP</code> is set to the floating IP requested in the first step. Refer to the Getting Started page for more information on how to generate a <code>cluster.yaml</code> file. Do not apply the <code>cluster.yaml</code> file. </li> <li>Edit the <code>cluster.yaml</code> file and search for the <code>KubeadmControlPlane</code> resource.</li> <li>Modify the <code>spec.kubeadmConfigSpec.files.*.content</code> attribute and change the <code>kube-vip</code> definition similar to the example below.</li> <li>Apply the <code>cluster.yaml</code> file.</li> <li>When the CAPX workload cluster is deployed, it will be reachable via the floating IP.</li> </ol>"},{"location":"capx/latest/experimental/vpc/#example","title":"Example","text":"<pre><code>kind: KubeadmControlPlane\nspec:\nkubeadmConfigSpec:\nfiles:\n- content: |\napiVersion: v1\nkind: Pod\nmetadata:\nname: kube-vip\nnamespace: kube-system\nspec:\ncontainers:\n- env:\n- name: address\nvalue: \"&lt;internal overlay subnet address&gt;\"                  \n</code></pre>"},{"location":"capx/latest/tasks/modify_machine_configuration/","title":"Modifying Machine Configurations","text":"<p>Since all attributes of the <code>NutanixMachineTemplate</code> resources are immutable, follow the Updating Infrastructure Machine Templates procedure to modify the configuration of machines in an existing CAPX cluster. See the NutanixMachineTemplate documentation for all supported configuration parameters.</p> <p>Note</p> <p>Manually modifying existing and linked <code>NutanixMachineTemplate</code> resources will not trigger a rolling update of the machines. </p> <p>Note</p> <p>Do not modify the virtual machine configuration of CAPX cluster nodes manually in Prism/Prism Central.  CAPX will not automatically revert the configuration change but performing scale-up/scale-down/upgrade operations will override manual modifications. Only use the <code>Updating Infrastructure Machine</code> procedure referenced above to perform configuration changes.</p>"},{"location":"capx/latest/types/nutanix_cluster/","title":"NutanixCluster","text":"<p>The <code>NutanixCluster</code> resource defines the configuration of a CAPX Kubernetes cluster. </p> <p>Example of a <code>NutanixCluster</code> resource:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixCluster\nmetadata:\nname: ${CLUSTER_NAME}\nnamespace: ${NAMESPACE}\nspec:\ncontrolPlaneEndpoint:\nhost: ${CONTROL_PLANE_ENDPOINT_IP}\nport: ${CONTROL_PLANE_ENDPOINT_PORT=6443}\nprismCentral:\naddress: ${NUTANIX_ENDPOINT}\nadditionalTrustBundle:\nkind: ConfigMap\nname: user-ca-bundle\ncredentialRef:\nkind: Secret\nname: ${CLUSTER_NAME}\ninsecure: ${NUTANIX_INSECURE=false}\nport: ${NUTANIX_PORT=9440}\n</code></pre>"},{"location":"capx/latest/types/nutanix_cluster/#nutanixcluster-spec","title":"NutanixCluster spec","text":"<p>The table below provides an overview of the supported parameters of the <code>spec</code> attribute of a <code>NutanixCluster</code> resource.</p>"},{"location":"capx/latest/types/nutanix_cluster/#configuration-parameters","title":"Configuration parameters","text":"Key Type Description controlPlaneEndpoint object Defines the host IP and port of the CAPX Kubernetes cluster. controlPlaneEndpoint.host string Host IP to be assigned to the CAPX Kubernetes cluster. controlPlaneEndpoint.port int Port of the CAPX Kubernetes cluster. Default: <code>6443</code> prismCentral object (Optional) Prism Central endpoint definition. prismCentral.address string IP/FQDN of Prism Central. prismCentral.port int Port of Prism Central. Default: <code>9440</code> prismCentral.insecure bool Disable Prism Central certificate checking. Default: <code>false</code> prismCentral.credentialRef object Reference to credentials used for Prism Central connection. prismCentral.credentialRef.kind string Kind of the credentialRef. Allowed value: <code>Secret</code> prismCentral.credentialRef.name string Name of the secret containing the Prism Central credentials. prismCentral.credentialRef.namespace string (Optional) Namespace of the secret containing the Prism Central credentials. prismCentral.additionalTrustBundle object Reference to the certificate trust bundle used for Prism Central connection. prismCentral.additionalTrustBundle.kind string Kind of the additionalTrustBundle. Allowed value: <code>ConfigMap</code> prismCentral.additionalTrustBundle.name string Name of the <code>ConfigMap</code> containing the Prism Central trust bundle. prismCentral.additionalTrustBundle.namespace string (Optional) Namespace of the <code>ConfigMap</code> containing the Prism Central trust bundle. <p>Note</p> <p>To prevent duplicate IP assignments, it is required to assign an IP-address to the <code>controlPlaneEndpoint.host</code> variable that is not part of the Nutanix IPAM or DHCP range assigned to the subnet of the CAPX cluster.</p>"},{"location":"capx/latest/types/nutanix_machine_template/","title":"NutanixMachineTemplate","text":"<p>The <code>NutanixMachineTemplate</code> resource defines the configuration of a CAPX Kubernetes VM. </p> <p>Example of a <code>NutanixMachineTemplate</code> resource.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixMachineTemplate\nmetadata:\nname: \"${CLUSTER_NAME}-mt-0\"\nnamespace: \"${NAMESPACE}\"\nspec:\ntemplate:\nspec:\nproviderID: \"nutanix://${CLUSTER_NAME}-m1\"\n# Supported options for boot type: legacy and uefi\n# Defaults to legacy if not set\nbootType: ${NUTANIX_MACHINE_BOOT_TYPE=legacy}\nvcpusPerSocket: ${NUTANIX_MACHINE_VCPU_PER_SOCKET=1}\nvcpuSockets: ${NUTANIX_MACHINE_VCPU_SOCKET=2}\nmemorySize: \"${NUTANIX_MACHINE_MEMORY_SIZE=4Gi}\"\nsystemDiskSize: \"${NUTANIX_SYSTEMDISK_SIZE=40Gi}\"\nimage:\ntype: name\nname: \"${NUTANIX_MACHINE_TEMPLATE_IMAGE_NAME}\"\ncluster:\ntype: name\nname: \"${NUTANIX_PRISM_ELEMENT_CLUSTER_NAME}\"\nsubnet:\n- type: name\nname: \"${NUTANIX_SUBNET_NAME}\"\n# Adds additional categories to the virtual machines.\n# Note: Categories must already be present in Prism Central\n# additionalCategories:\n#   - key: AppType\n#     value: Kubernetes\n# Adds the cluster virtual machines to a project defined in Prism Central.\n# Replace NUTANIX_PROJECT_NAME with the correct project defined in Prism Central\n# Note: Project must already be present in Prism Central.\n# project:\n#   type: name\n#   name: \"NUTANIX_PROJECT_NAME\"\n</code></pre>"},{"location":"capx/latest/types/nutanix_machine_template/#nutanixmachinetemplate-spec","title":"NutanixMachineTemplate spec","text":"<p>The table below provides an overview of the supported parameters of the <code>spec</code> attribute of a <code>NutanixMachineTemplate</code> resource.</p>"},{"location":"capx/latest/types/nutanix_machine_template/#configuration-parameters","title":"Configuration parameters","text":"Key Type Description bootType string Boot type of the VM. Depends on the OS image used. Allowed values: <code>legacy</code>, <code>uefi</code>. Default: <code>legacy</code> vcpusPerSocket int Amount of vCPUs per socket. Default: <code>1</code> vcpuSockets int Amount of vCPU sockets. Default: <code>2</code> memorySize string Amount of Memory. Default: <code>4Gi</code> systemDiskSize string Amount of storage assigned to the system disk. Default: <code>40Gi</code> image object Reference to the OS image used for the system disk. image.type string Type to identify the OS image. Allowed values: <code>name</code> and <code>uuid</code> image.name string Name or UUID of the image. cluster object Reference to the Prism Element cluster. cluster.type string Type to identify the Prism Element cluster. Allowed values: <code>name</code> and <code>uuid</code> cluster.name string Name or UUID of the Prism Element cluster. subnets list Reference to the subnets to be assigned to the VMs. subnets.[].type string Type to identify the subnet. Allowed values: <code>name</code> and <code>uuid</code> subnets.[].name string Name or UUID of the subnet. additionalCategories list Reference to the categories to be assigned to the VMs. These categories already exist in Prism Central. additionalCategories.[].key string Key of the category. additionalCategories.[].value string Value of the category. project object Reference to the project. This project must already exist in Prism Central. project.type string Type to identify the project. Allowed values: <code>name</code> and <code>uuid</code> project.name string Name or UUID of the project."},{"location":"capx/v0.5.x/credential_management/","title":"Credential Management","text":"<p>Cluster API Provider Nutanix Cloud Infrastructure (CAPX) interacts with Nutanix Prism Central (PC) APIs to manage the required Kubernetes cluster infrastructure resources.</p> <p>PC credentials are required to authenticate to the PC APIs. CAPX currently supports two mechanisms to supply the required credentials:</p> <ul> <li>Credentials injected into the CAPX manager deployment</li> <li>Workload cluster specific credentials</li> </ul>"},{"location":"capx/v0.5.x/credential_management/#credentials-injected-into-the-capx-manager-deployment","title":"Credentials injected into the CAPX manager deployment","text":"<p>By default, credentials will be injected into the CAPX manager deployment when CAPX is initialized. See the getting started guide for more information on the initialization.</p> <p>Upon initialization a <code>nutanix-creds</code> secret will automatically be created in the <code>capx-system</code> namespace. This secret will contain the values supplied via the <code>NUTANIX_USER</code> and <code>NUTANIX_PASSWORD</code> parameters. </p> <p>The <code>nutanix-creds</code> secret will be used for workload cluster deployment if no other credential is supplied.</p>"},{"location":"capx/v0.5.x/credential_management/#example","title":"Example","text":"<p>An example of the automatically created <code>nutanix-creds</code> secret can be found below: <pre><code>---\napiVersion: v1\nkind: Secret\ntype: Opaque\nmetadata:\nname: nutanix-creds\nnamespace: capx-system\nstringData:\nNUTANIX_USER: \"&lt;nutanix-user&gt;\"\nNUTANIX_PASSWORD: \"&lt;nutanix-password&gt;\"\n</code></pre></p>"},{"location":"capx/v0.5.x/credential_management/#workload-cluster-specific-credentials","title":"Workload cluster specific credentials","text":"<p>Users can override the credentials injected in CAPX manager deployment by supplying a credential specific to a workload cluster. The credentials can be supplied by creating a secret in the same namespace as the <code>NutanixCluster</code> namespace. </p> <p>The secret can be referenced by adding a <code>credentialRef</code> inside the <code>prismCentral</code> attribute contained in the <code>NutanixCluster</code>.  The secret will also be deleted when the <code>NutanixCluster</code> is deleted.</p> <p>Note: There is a 1:1 relation between the secret and the <code>NutanixCluster</code> object. </p>"},{"location":"capx/v0.5.x/credential_management/#example_1","title":"Example","text":"<p>Create a secret in the namespace of the <code>NutanixCluster</code>:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\nname: \"&lt;my-secret&gt;\"\nnamespace: \"&lt;nutanixcluster-namespace&gt;\"\nstringData:\nNUTANIX_PASSWORD: \"&lt;nutanix-password&gt;\"\nNUTANIX_USER: \"&lt;nutanix-user&gt;\"\n</code></pre> <p>Add <code>credentialRef</code> to the <code>NutanixCluster</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixCluster\nmetadata:\nname: \"&lt;nutanixcluster-name&gt;\"\nnamespace: \"&lt;nutanixcluster-namespace&gt;\"\nspec:\nprismCentral:\ncredentialRef:\nname: \"&lt;my-secret&gt;\"\nkind: Secret\n...\n</code></pre>"},{"location":"capx/v0.5.x/getting_started/","title":"Getting Started","text":"<p>This is a guide on getting started with Cluster API Provider Nutanix Cloud Infrastructure (CAPX). To learn more about cluster API in more depth, check out the Cluster API book.</p> <p>For more information on how install the Nutanix CSI Driver on a CAPX cluster, visit Nutanix CSI Driver installation with CAPX.</p> <p>For more information on how CAPX handles credentials, visit Credential Management.</p> <p>For more information on the port requirements for CAPX, visit Port Requirements.</p>"},{"location":"capx/v0.5.x/getting_started/#production-workflow","title":"Production Workflow","text":""},{"location":"capx/v0.5.x/getting_started/#build-os-image-for-nutanixmachinetemplate-resource","title":"Build OS image for NutanixMachineTemplate resource","text":"<p>To build an OS image for NutanixMachineTemplate, visit Nutanix OS Image Builder.</p>"},{"location":"capx/v0.5.x/getting_started/#configure-and-install-cluster-api-provider-nutanix-cloud-infrastructure","title":"Configure and Install Cluster API Provider Nutanix Cloud Infrastructure","text":"<p>To initialize Cluster API Provider Nutanix Cloud Infrastructure, <code>clusterctl</code> requires the following variables, which should be set in either <code>~/.cluster-api/clusterctl.yaml</code> or as environment variables. <pre><code>NUTANIX_ENDPOINT: \"\"    # IP or FQDN of Prism Central\nNUTANIX_USER: \"\"        # Prism Central user\nNUTANIX_PASSWORD: \"\"    # Prism Central password\nNUTANIX_INSECURE: false # or true\n\nKUBERNETES_VERSION: \"v1.22.9\"\nWORKER_MACHINE_COUNT: 3\nNUTANIX_SSH_AUTHORIZED_KEY: \"\"\n\nNUTANIX_PRISM_ELEMENT_CLUSTER_NAME: \"\"\nNUTANIX_MACHINE_TEMPLATE_IMAGE_NAME: \"\"\nNUTANIX_SUBNET_NAME: \"\"\n</code></pre></p> <p>You can also see the required list of variables by running the following: <pre><code>clusterctl generate cluster mycluster -i nutanix --list-variables           \nRequired Variables:\n  - CONTROL_PLANE_ENDPOINT_IP\n  - KUBERNETES_VERSION\n  - NUTANIX_ENDPOINT\n  - NUTANIX_MACHINE_TEMPLATE_IMAGE_NAME\n  - NUTANIX_PASSWORD\n  - NUTANIX_PRISM_ELEMENT_CLUSTER_NAME\n  - NUTANIX_SSH_AUTHORIZED_KEY\n  - NUTANIX_SUBNET_NAME\n  - NUTANIX_USER\n\nOptional Variables:\n  - CONTROL_PLANE_ENDPOINT_PORT      (defaults to \"6443\")\n  - CONTROL_PLANE_MACHINE_COUNT      (defaults to 1)\n  - KUBEVIP_LB_ENABLE                (defaults to \"false\")\n  - KUBEVIP_SVC_ENABLE               (defaults to \"false\")\n  - NAMESPACE                        (defaults to current Namespace in the KubeConfig file)\n  - NUTANIX_INSECURE                 (defaults to \"false\")\n  - NUTANIX_MACHINE_BOOT_TYPE        (defaults to \"legacy\")\n  - NUTANIX_MACHINE_MEMORY_SIZE      (defaults to \"4Gi\")\n  - NUTANIX_MACHINE_VCPU_PER_SOCKET  (defaults to \"1\")\n  - NUTANIX_MACHINE_VCPU_SOCKET      (defaults to \"2\")\n  - NUTANIX_PORT                     (defaults to \"9440\")\n  - NUTANIX_SYSTEMDISK_SIZE          (defaults to \"40Gi\")\n  - WORKER_MACHINE_COUNT             (defaults to 0)\n</code></pre></p> <p>Note</p> <p>To prevent duplicate IP assignments, it is required to assign an IP-address to the <code>CONTROL_PLANE_ENDPOINT_IP</code> variable that is not part of the Nutanix IPAM or DHCP range assigned to the subnet of the CAPX cluster.</p> <p>Now you can instantiate Cluster API with the following: <pre><code>clusterctl init -i nutanix\n</code></pre></p>"},{"location":"capx/v0.5.x/getting_started/#deploy-a-workload-cluster-on-nutanix-cloud-infrastructure","title":"Deploy a workload cluster on Nutanix Cloud Infrastructure","text":"<pre><code>export TEST_CLUSTER_NAME=mytestcluster1\nexport TEST_NAMESPACE=mytestnamespace\nCONTROL_PLANE_ENDPOINT_IP=x.x.x.x clusterctl generate cluster ${TEST_CLUSTER_NAME} \\\n    -i nutanix \\\n    --target-namespace ${TEST_NAMESPACE}  \\\n    --kubernetes-version v1.22.9 \\\n    --control-plane-machine-count 1 \\\n    --worker-machine-count 3 &gt; ./cluster.yaml\nkubectl create ns ${TEST_NAMESPACE}\nkubectl apply -f ./cluster.yaml -n ${TEST_NAMESPACE}\n</code></pre>"},{"location":"capx/v0.5.x/getting_started/#access-a-workload-cluster","title":"Access a workload cluster","text":"<p>To access resources on the cluster, you can get the kubeconfig with the following: <pre><code>clusterctl get kubeconfig ${TEST_CLUSTER_NAME} -n ${TEST_NAMESPACE} &gt; ${TEST_CLUSTER_NAME}.kubeconfig\nkubectl --kubeconfig ./${TEST_CLUSTER_NAME}.kubeconfig get nodes \n</code></pre></p>"},{"location":"capx/v0.5.x/getting_started/#install-cni-on-workload-a-cluster","title":"Install CNI on workload a cluster","text":"<p>You must deploy a Container Network Interface (CNI) based pod network add-on so that your pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed.</p> <p>Note</p> <p>Take care that your pod network must not overlap with any of the host networks. You are likely to see problems if there is any overlap. If you find a collision between your network plugin's preferred pod network and some of your host networks, you must choose a suitable alternative CIDR block to use instead. It can be configured inside the <code>cluster.yaml</code> generated by <code>clusterctl generate cluster</code> before applying it.</p> <p>Several external projects provide Kubernetes pod networks using CNI, some of which also support Network Policy.</p> <p>See a list of add-ons that implement the Kubernetes networking model. At time of writing, the most common are Calico and Cilium.</p> <p>Follow the specific install guide for your selected CNI and install only one pod network per cluster.</p> <p>Once a pod network has been installed, you can confirm that it is working by checking that the CoreDNS pod is running in the output of <code>kubectl get pods --all-namespaces</code>.</p>"},{"location":"capx/v0.5.x/getting_started/#kube-vip-settings","title":"Kube-vip settings","text":"<p>Kube-vip is a true load balancing solution for the Kubernetes control plane. It distributes API requests across control plane nodes. It also has the capability to provide load balancing for Kubernetes services.</p> <p>You can tweak kube-vip settings by using the following properties:</p> <ul> <li><code>KUBEVIP_LB_ENABLE</code></li> </ul> <p>This setting allows control plane load balancing using IPVS. See Control Plane Load-Balancing documentation for further information.</p> <ul> <li><code>KUBEVIP_SVC_ENABLE</code> </li> </ul> <p>This setting enables a service of type LoadBalancer. See Kubernetes Service Load Balancing documentation for further information.</p>"},{"location":"capx/v0.5.x/getting_started/#delete-a-workload-cluster","title":"Delete a workload cluster","text":"<p>To remove a workload cluster from your management cluster, remove the cluster object and the provider will clean-up all resources. </p> <pre><code>kubectl delete cluster ${TEST_CLUSTER_NAME} -n $(TEST_NAMESPACE)\n</code></pre> <p>Note</p> <p>Deleting the entire cluster template with <code>kubectl delete -f ./cluster.yaml</code> may lead to pending resources requiring manual cleanup.</p>"},{"location":"capx/v0.5.x/port_requirements/","title":"Port Requirements","text":"<p>CAPX uses the ports documented below to create workload clusters. </p> <p>Note<p>This page only documents the ports specifically required by CAPX and does not provide the full overview of all ports required in the CAPI framework.</p> </p>"},{"location":"capx/v0.5.x/port_requirements/#management-cluster","title":"Management cluster","text":"Source Destination Protocol Port Description Management cluster External Registries TCP 443 Pull container images from CAPX public registries Management cluster Prism Central TCP 9440 Management cluster communication to Prism Central"},{"location":"capx/v0.5.x/port_requirements/#public-registries-utilized-when-using-capx","title":"Public registries utilized when using CAPX","text":"Registry name ghcr.io"},{"location":"capx/v0.5.x/troubleshooting/","title":"Troubleshooting","text":""},{"location":"capx/v0.5.x/troubleshooting/#clusterctl-failed-with-github-rate-limit-error","title":"Clusterctl failed with GitHub rate limit error","text":"<p>By design Clusterctl fetches artifacts from repositories hosted on GitHub, this operation is subject to GitHub API rate limits.</p> <p>While this is generally okay for the majority of users, there is still a chance that some users (especially developers or CI tools) hit this limit:</p> <pre><code>Error: failed to get repository client for the XXX with name YYY: error creating the GitHub repository client: failed to get GitHub latest version: failed to get the list of versions: rate limit for github api has been reached. Please wait one hour or get a personal API tokens a assign it to the GITHUB_TOKEN environment variable\n</code></pre> <p>As explained in the error message, you can increase your API rate limit by creating a GitHub personal token and setting a <code>GITHUB_TOKEN</code> environment variable using the token.</p>"},{"location":"capx/v0.5.x/validated_integrations/","title":"Validated Integrations","text":"<p>Validated integrations are a defined set of specifically tested configurations between technologies that represent the most common combinations that Nutanix customers are using or deploying with CAPX. For these integrations, Nutanix has directly, or through certified partners, exercised a full range of platform tests as part of the product release process.</p>"},{"location":"capx/v0.5.x/validated_integrations/#integration-validation-policy","title":"Integration Validation Policy","text":"<p>Nutanix follows the version validation policies below:</p> <ul> <li> <p>Validate at least one active AOS LTS (long term support) version. Validated AOS LTS version for a specific CAPX version is listed in the AOS section.</p> <p>Note</p> <p>Typically the latest LTS release at time of CAPX release except when latest is initial release in train (eg x.y.0). Exact version depends on timing and customer adoption.</p> </li> <li> <p>Validate the latest AOS STS (short term support) release at time of CAPX release.</p> </li> <li> <p>Validate at least one active Prism Central (PC) version. Validated PC version for a specific CAPX version is listed in the Prism Central section.</p> <p>Note</p> <p>Typically the the latest PC release at time of CAPX release except when latest is initial release in train (eg x.y.0). Exact version depends on timing and customer adoption.</p> </li> <li> <p>At least one active Cluster-API (CAPI) version. Validated CAPI version for a specific CAPX version is listed in the Cluster-API section.</p> <p>Note</p> <p>Typically the the latest Cluster-API release at time of CAPX release except when latest is initial release in train (eg x.y.0). Exact version depends on timing and customer adoption.</p> </li> </ul>"},{"location":"capx/v0.5.x/validated_integrations/#validated-versions","title":"Validated versions","text":""},{"location":"capx/v0.5.x/validated_integrations/#cluster-api","title":"Cluster-API","text":"CAPX CAPI v1.1.4+ CAPI v1.2.x v0.5.x Yes Yes <p>See the Validated Kubernetes Versions page for more information on CAPI validated versions.</p>"},{"location":"capx/v0.5.x/validated_integrations/#aos","title":"AOS","text":"CAPX 5.20.4.5 (LTS) 6.1.1.5 (STS) v0.5.x Yes Yes"},{"location":"capx/v0.5.x/validated_integrations/#prism-central","title":"Prism Central","text":"CAPX 2022.1.0.2 pc.2022.6 v0.5.x Yes Yes"},{"location":"capx/v0.5.x/addons/install_csi_driver/","title":"Nutanix CSI Driver installation with CAPX","text":"<p>The Nutanix CSI driver is fully supported on CAPI/CAPX deployed clusters where all the nodes meet the Nutanix CSI driver prerequisites.</p> <p>There are three methods to install the Nutanix CSI driver on a CAPI/CAPX cluster:</p> <ul> <li>Helm</li> <li>ClusterResourceSet</li> <li>CAPX Flavor</li> </ul> <p>For more information, check the next sections.</p>"},{"location":"capx/v0.5.x/addons/install_csi_driver/#capi-workload-cluster-prerequisites-for-the-nutanix-csi-driver","title":"CAPI Workload cluster prerequisites for the Nutanix CSI Driver","text":"<p>Kubernetes workers need the following prerequisites to use the Nutanix CSI Drivers:</p> <ul> <li>iSCSI initiator package (for Volumes based block storage)</li> <li>NFS client package (for Files based storage)</li> </ul> <p>These packages may already be present in the image you use with your infrastructure provider or you can also rely on your bootstrap provider to install them. More info is available in the Prerequisites docs.</p> <p>The package names and installation method will also vary depending on the operating system you plan to use.</p> <p>In the example below, <code>kubeadm</code> bootstrap provider is used to deploy these packages on top of an Ubuntu 20.04 image. The <code>kubeadm</code> bootstrap provider allows defining <code>preKubeadmCommands</code> that will be launched before Kubernetes cluster creation. These <code>preKubeadmCommands</code> can be defined both in <code>KubeadmControlPlane</code> for master nodes and in <code>KubeadmConfigTemplate</code> for worker nodes.</p> <p>In the example with an Ubuntu 20.04 image, both <code>KubeadmControlPlane</code> and <code>KubeadmConfigTemplate</code> must be modified as in the example below:</p> <pre><code>spec:\ntemplate:\nspec:\n# .......\npreKubeadmCommands:\n- echo \"before kubeadm call\" &gt; /var/log/prekubeadm.log\n- apt update\n- apt install -y nfs-common open-iscsi\n- systemctl enable --now iscsid\n</code></pre>"},{"location":"capx/v0.5.x/addons/install_csi_driver/#install-the-nutanix-csi-driver-with-helm","title":"Install the Nutanix CSI Driver with Helm","text":"<p>A recent Helm version is needed (tested with Helm v3.10.1).</p> <p>The example below must be applied on a ready workload cluster. The workload cluster's kubeconfig can be retrieved and used to connect with the following command:</p> <pre><code>clusterctl get kubeconfig $CLUSTER_NAME -n $CLUSTER_NAMESPACE &gt; $CLUSTER_NAME-KUBECONFIG\nexport KUBECONFIG=$(pwd)/$CLUSTER_NAME-KUBECONFIG\n</code></pre> <p>Once connected to the cluster, follow the CSI documentation. </p> <p>First, install the nutanix-csi-snapshot chart followed by the nutanix-csi-storage chart.</p> <p>See an example below:</p> <pre><code>#Add the official Nutanix Helm repo and get the latest update\nhelm repo add nutanix https://nutanix.github.io/helm/\nhelm repo update\n\n# Install the nutanix-csi-snapshot chart\nhelm install nutanix-csi-snapshot nutanix/nutanix-csi-snapshot -n ntnx-system --create-namespace\n\n# Install the nutanix-csi-storage chart\nhelm install nutanix-storage nutanix/nutanix-csi-storage -n ntnx-system --set createSecret=false\n</code></pre> <p>Warning</p> <p>For correct Nutanix CSI driver deployment, a fully functional CNI deployment must be present.</p>"},{"location":"capx/v0.5.x/addons/install_csi_driver/#install-the-nutanix-csi-driver-with-clusterresourceset","title":"Install the Nutanix CSI Driver with <code>ClusterResourceSet</code>","text":"<p>The <code>ClusterResourceSet</code> feature was introduced to automatically apply a set of resources (such as CNI/CSI) defined by administrators to matching created/existing workload clusters.</p>"},{"location":"capx/v0.5.x/addons/install_csi_driver/#enabling-the-clusterresourceset-feature","title":"Enabling the <code>ClusterResourceSet</code> feature","text":"<p>At the time of writing, <code>ClusterResourceSet</code> is an experimental feature that must be enabled during the initialization of a management cluster with the <code>EXP_CLUSTER_RESOURCE_SET</code> feature gate.</p> <p>To do this, add <code>EXP_CLUSTER_RESOURCE_SET: \"true\"</code> in the <code>clusterctl</code> configuration file or just <code>export EXP_CLUSTER_RESOURCE_SET=true</code> before initializing the management cluster with <code>clusterctl init</code>.</p> <p>If the management cluster is already initialized, the <code>ClusterResourceSet</code> can be enabled by changing the configuration of the <code>capi-controller-manager</code> deployment in the <code>capi-system</code> namespace.</p> <pre><code>kubectl edit deployment -n capi-system capi-controller-manager\n</code></pre> <p>Locate the section below:</p> <pre><code>  - args:\n- --leader-elect\n- --metrics-bind-addr=localhost:8080\n- --feature-gates=MachinePool=false,ClusterResourceSet=true,ClusterTopology=false\n</code></pre> <p>Then replace <code>ClusterResourceSet=false</code> with <code>ClusterResourceSet=true</code>.</p> <p>Note</p> <p>Editing the <code>deployment</code> resource will cause Kubernetes to automatically start new versions of the containers with the feature enabled.</p>"},{"location":"capx/v0.5.x/addons/install_csi_driver/#prepare-the-nutanix-csi-clusterresourceset","title":"Prepare the Nutanix CSI <code>ClusterResourceSet</code>","text":""},{"location":"capx/v0.5.x/addons/install_csi_driver/#create-the-configmap-for-the-cni-plugin","title":"Create the <code>ConfigMap</code> for the CNI Plugin","text":"<p>First, create a <code>ConfigMap</code> that contains a YAML manifest with all resources to install the Nutanix CSI driver.</p> <p>Since the Nutanix CSI Driver is provided as a Helm chart, use <code>helm</code> to extract it before creating the <code>ConfigMap</code>. See an example below:</p> <pre><code>helm repo add nutanix https://nutanix.github.io/helm/\nhelm repo update\n\nkubectl create ns ntnx-system --dry-run=client -o yaml &gt; nutanix-csi-namespace.yaml\nhelm template nutanix-csi-snapshot nutanix/nutanix-csi-snapshot -n ntnx-system &gt; nutanix-csi-snapshot.yaml\nhelm template nutanix-csi-snapshot nutanix/nutanix-csi-storage -n ntnx-system &gt; nutanix-csi-storage.yaml\n\nkubectl create configmap nutanix-csi-crs --from-file=nutanix-csi-namespace.yaml --from-file=nutanix-csi-snapshot.yaml --from-file=nutanix-csi-storage.yaml\n</code></pre>"},{"location":"capx/v0.5.x/addons/install_csi_driver/#create-the-clusterresourceset","title":"Create the <code>ClusterResourceSet</code>","text":"<p>Next, create the <code>ClusterResourceSet</code> resource that will map the <code>ConfigMap</code> defined above to clusters using a <code>clusterSelector</code>.</p> <p>The <code>ClusterResourceSet</code> needs to be created inside the management cluster. See an example below:</p> <pre><code>---\napiVersion: addons.cluster.x-k8s.io/v1alpha3\nkind: ClusterResourceSet\nmetadata:\nname: nutanix-csi-crs\nspec:\nclusterSelector:\nmatchLabels:\ncsi: nutanix resources:\n- kind: ConfigMap\nname: nutanix-csi-crs\n</code></pre> <p>The <code>clusterSelector</code> field controls how Cluster API will match this <code>ClusterResourceSet</code> on one or more workload clusters. In the example scenario, the <code>matchLabels</code> approach is being used where the <code>ClusterResourceSet</code> will be applied to all workload clusters having the <code>csi: nutanix</code> label present. If the label isn't present, the <code>ClusterResourceSet</code> won't apply to that workload cluster.</p> <p>The <code>resources</code> field references the <code>ConfigMap</code> created above, which contains the manifests for installing the Nutanix CSI driver.</p>"},{"location":"capx/v0.5.x/addons/install_csi_driver/#assign-the-clusterresourceset-to-a-workload-cluster","title":"Assign the <code>ClusterResourceSet</code> to a workload cluster","text":"<p>Assign this <code>ClusterResourceSet</code> to the workload cluster by adding the correct label to the <code>Cluster</code> resource.</p> <p>This can be done before workload cluster creation by editing the output of the <code>clusterctl generate cluster</code> command or by modifying an already deployed workload cluster.</p> <p>In both cases, <code>Cluster</code> resources should look like this:</p> <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\nmetadata:\nname: workload-cluster-name\nnamespace: workload-cluster-namespace\nlabels:\ncsi: nutanix\n# ...\n</code></pre> <p>Warning</p> <p>For correct Nutanix CSI driver deployment, a fully functional CNI deployment must be present.</p>"},{"location":"capx/v0.5.x/addons/install_csi_driver/#install-the-nutanix-csi-driver-with-a-capx-flavor","title":"Install the Nutanix CSI Driver with a CAPX flavor","text":"<p>The CAPX provider can utilize a flavor to automatically deploy the Nutanix CSI using a <code>ClusterResourceSet</code>.</p>"},{"location":"capx/v0.5.x/addons/install_csi_driver/#prerequisites","title":"Prerequisites","text":"<p>The following requirements must be met:</p> <ul> <li>The operating system must meet the Nutanix CSI OS prerequisites.</li> <li>The Management cluster must be installed with the <code>CLUSTER_RESOURCE_SET</code> feature gate.</li> </ul>"},{"location":"capx/v0.5.x/addons/install_csi_driver/#installation","title":"Installation","text":"<p>Specify the <code>csi</code> flavor during workload cluster creation. See an example below:</p> <pre><code>clusterctl generate cluster my-cluster -f csi\n</code></pre> <p>Additional environment variables are required:</p> <ul> <li><code>WEBHOOK_CA</code>: Base64 encoded CA certificate used to sign the webhook certificate</li> <li><code>WEBHOOK_CERT</code>: Base64 certificate for the webhook validation component</li> <li><code>WEBHOOK_KEY</code>: Base64 key for the webhook validation component</li> </ul> <p>The three components referenced above can be automatically created and referenced using this script:</p> <pre><code>source scripts/gen-self-cert.sh\n</code></pre> <p>The certificate must reference the following names:</p> <ul> <li>csi-snapshot-webhook</li> <li>csi-snapshot-webhook.ntnx-sytem</li> <li>csi-snapshot-webhook.ntnx-sytem.svc</li> </ul> <p>Warning</p> <p>For correct Nutanix CSI driver deployment, a fully functional CNI deployment must be present.</p>"},{"location":"capx/v0.5.x/addons/install_csi_driver/#nutanix-csi-driver-configuration","title":"Nutanix CSI Driver Configuration","text":"<p>After the driver is installed, it must be configured for use by minimally defining a <code>Secret</code> and <code>StorageClass</code>.</p> <p>This can be done manually in the workload clusters or by using a <code>ClusterResourceSet</code> in the management cluster as explained above.</p> <p>See the Official CSI Driver documentation on the Nutanix Portal for more configuration information. </p>"},{"location":"capx/v0.5.x/experimental/autoscaler/","title":"Using Autoscaler in combination with CAPX","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>Autoscaler can be used in combination with Cluster API to automatically add or remove machines in a cluster. </p> <p>Autoscaler can be used in different deployment scenarios. This page will provide an overview of multiple autoscaler deployment scenarios in combination with CAPX. See the Testing section to see how scale-up/scale-down events can be triggered to validate the autoscaler behaviour.</p> <p>More in-depth information on Autoscaler functionality can be found in the Kubernetes documentation.</p> <p>All Autoscaler configuration parameters can be found here.</p>"},{"location":"capx/v0.5.x/experimental/autoscaler/#scenario-1-management-cluster-managing-an-external-workload-cluster","title":"Scenario 1: Management cluster managing an external workload cluster","text":"<p>In this scenario, Autoscaler will be running on a management cluster and it will manage an external workload cluster. See the management cluster managing an external workload cluster section of Kubernetes documentation for more information.</p>"},{"location":"capx/v0.5.x/experimental/autoscaler/#steps","title":"Steps","text":"<ol> <li> <p>Deploy a management cluster and workload cluster. The CAPI quickstart can be used as a starting point.</p> <p>Note<p>Make sure a CNI is installed in the workload cluster.</p> </p> </li> <li> <p>Download the example Autoscaler deployment file.</p> </li> <li>Modify the <code>deployment.yaml</code> file:<ul> <li>Change the namespace of all resources to the namespaces of the workload cluster.</li> <li>Choose an autoscale image.</li> <li>Change the following parameters in the <code>Deployment</code> resource: <pre><code>        spec:\ncontainers:\nname: cluster-autoscaler\ncommand:\n- /cluster-autoscaler\nargs:\n- --cloud-provider=clusterapi\n- --kubeconfig=/mnt/kubeconfig/kubeconfig.yml\n- --clusterapi-cloud-config-authoritative\n- -v=1\nvolumeMounts:\n- mountPath: /mnt/kubeconfig\nname: kubeconfig\nreadOnly: true\n...\nvolumes:\n- name: kubeconfig\nsecret:\nsecretName: &lt;workload cluster name&gt;-kubeconfig\nitems:\n- key: value\npath: kubeconfig.yml\n</code></pre></li> </ul> </li> <li>Apply the <code>deployment.yaml</code> file. <pre><code>kubectl apply -f deployment.yaml\n</code></pre></li> <li>Add the annotations to the workload cluster <code>MachineDeployment</code> resource.</li> <li>Test Autoscaler. Go to the Testing section.</li> </ol>"},{"location":"capx/v0.5.x/experimental/autoscaler/#scenario-2-autoscaler-running-on-workload-cluster","title":"Scenario 2: Autoscaler running on workload cluster","text":"<p>In this scenario, Autoscaler will be deployed on top of the workload cluster directly. In order for Autoscaler to work, it is required that the workload cluster resources are moved from the management cluster to the workload cluster.</p>"},{"location":"capx/v0.5.x/experimental/autoscaler/#steps_1","title":"Steps","text":"<ol> <li>Deploy a management cluster and workload cluster. The CAPI quickstart can be used as a starting point.</li> <li>Get the kubeconfig file for the workload cluster and use this kubeconfig to login to the workload cluster.  <pre><code>clusterctl get kubeconfig &lt;workload cluster name&gt; -n &lt;workload cluster namespace &gt; /path/to/kubeconfig\n</code></pre></li> <li>Install a CNI in the workload cluster.</li> <li>Initialise the CAPX components on top of the workload cluster: <pre><code>clusterctl init --infrastructure nutanix\n</code></pre></li> <li>Migrate the workload cluster custom resources to the workload cluster. Run following command from the management cluster: <pre><code>clusterctl move -n &lt;workload cluster ns&gt;  --to-kubeconfig /path/to/kubeconfig\n</code></pre></li> <li>Verify if the cluster has been migrated by running following command on the workload cluster: <pre><code>kubectl get cluster -A </code></pre></li> <li>Download the example autoscaler deployment file.</li> <li>Create the Autoscaler namespace: <pre><code>kubectl create ns autoscaler\n</code></pre></li> <li>Apply the <code>deployment.yaml</code> file <pre><code>kubectl apply -f deployment.yaml\n</code></pre></li> <li>Add the annotations to the workload cluster <code>MachineDeployment</code> resource.</li> <li>Test Autoscaler. Go to the Testing section.</li> </ol>"},{"location":"capx/v0.5.x/experimental/autoscaler/#testing","title":"Testing","text":"<ol> <li>Deploy an example Kubernetes application. For example, the one used in the Kubernetes HorizontalPodAutoscaler Walkthrough. <pre><code>kubectl apply -f https://k8s.io/examples/application/php-apache.yaml </code></pre></li> <li>Increase the amount of replicas of the application to trigger a scale-up event: <pre><code>kubectl scale deployment php-apache --replicas 100\n</code></pre></li> <li> <p>Decrease the amount of replicas of the application again to trigger a scale-down event.</p> <p>Note</p> <p>In case of issues check the logs of the Autoscaler pods.</p> </li> <li> <p>After a while CAPX, will add more machines. Refer to the Autoscaler configuration parameters to tweak the behaviour and timeouts.</p> </li> </ol>"},{"location":"capx/v0.5.x/experimental/autoscaler/#autoscaler-node-group-annotations","title":"Autoscaler node group annotations","text":"<p>Autoscaler uses following annotations to define the upper and lower boundries of the managed machines:</p> Annotation Example Value Description cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size 5 Maximum amount of machines in this node group cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size 1 Minimum amount of machines in this node group <p>These annotations must be applied to the <code>MachineDeployment</code> resources of a CAPX cluster. </p>"},{"location":"capx/v0.5.x/experimental/autoscaler/#example","title":"Example","text":"<pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: MachineDeployment\nmetadata:\nannotations:\ncluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \"5\"\ncluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \"1\"\n</code></pre>"},{"location":"capx/v0.5.x/experimental/capx_multi_pe/","title":"Creating a workload CAPX cluster spanning Prism Element clusters","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>This page will explain how to deploy CAPX-based Kubernetes clusters where worker nodes are spanning multiple Prism Element (PE) clusters. </p> <p>Note<p>All the PE clusters must be managed by the same Prism Central (PC) instance.</p> </p> <p>The topology will look like this: </p> <ul> <li>One PC managing multiple PE's</li> <li>One CAPI management cluster</li> <li>One CAPI workload cluster with multiple <code>MachineDeployment</code>resources</li> </ul> <p>Refer to the CAPI quickstart to get started with CAPX. </p> <p>To create workload clusters spanning multiple Prism Element clusters, it is required to create a <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resource for each Prism Element cluster. The Prism Element specific parameters (name/UUID, subnet,...) are referenced in the <code>NutanixMachineTemplate</code>. </p>"},{"location":"capx/v0.5.x/experimental/capx_multi_pe/#steps","title":"Steps","text":"<ol> <li>Create a management cluster that has the CAPX infrastructure provider deployed.</li> <li>Create a <code>cluster.yml</code> file containing the workload cluster definition. Refer to the steps defined in the CAPI quickstart guide to create an example <code>cluster.yml</code> file.</li> <li> <p>Add additional <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resources.</p> <p>By default there is only one machine template and machine deployment defined. To add nodes residing on another Prism Element cluster, a new <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resource needs to be added to the yaml file. The autogenerated <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resource definitions can be used as a baseline.</p> <p>Make sure to modify the <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> parameters.</p> </li> <li> <p>Apply the modified <code>cluster.yml</code> file to the management cluster.</p> </li> </ol>"},{"location":"capx/v1.0.x/credential_management/","title":"Credential Management","text":"<p>Cluster API Provider Nutanix Cloud Infrastructure (CAPX) interacts with Nutanix Prism Central (PC) APIs to manage the required Kubernetes cluster infrastructure resources.</p> <p>PC credentials are required to authenticate to the PC APIs. CAPX currently supports two mechanisms to supply the required credentials:</p> <ul> <li>Credentials injected into the CAPX manager deployment</li> <li>Workload cluster specific credentials</li> </ul>"},{"location":"capx/v1.0.x/credential_management/#credentials-injected-into-the-capx-manager-deployment","title":"Credentials injected into the CAPX manager deployment","text":"<p>By default, credentials will be injected into the CAPX manager deployment when CAPX is initialized. See the getting started guide for more information on the initialization.</p> <p>Upon initialization a <code>nutanix-creds</code> secret will automatically be created in the <code>capx-system</code> namespace. This secret will contain the values supplied via the <code>NUTANIX_USER</code> and <code>NUTANIX_PASSWORD</code> parameters. </p> <p>The <code>nutanix-creds</code> secret will be used for workload cluster deployment if no other credential is supplied.</p>"},{"location":"capx/v1.0.x/credential_management/#example","title":"Example","text":"<p>An example of the automatically created <code>nutanix-creds</code> secret can be found below: <pre><code>---\napiVersion: v1\nkind: Secret\ntype: Opaque\nmetadata:\nname: nutanix-creds\nnamespace: capx-system\nstringData:\ncredentials: |\n[\n{\n\"type\": \"basic_auth\", \n\"data\": { \n\"prismCentral\":{\n\"username\": \"&lt;nutanix-user&gt;\",\n\"password\": \"&lt;nutanix-password&gt;\"\n},\n\"prismElements\": null\n}\n}\n]\n</code></pre></p>"},{"location":"capx/v1.0.x/credential_management/#workload-cluster-specific-credentials","title":"Workload cluster specific credentials","text":"<p>Users can override the credentials injected in CAPX manager deployment by supplying a credential specific to a workload cluster. The credentials can be supplied by creating a secret in the same namespace as the <code>NutanixCluster</code> namespace. </p> <p>The secret can be referenced by adding a <code>credentialRef</code> inside the <code>prismCentral</code> attribute contained in the <code>NutanixCluster</code>.  The secret will also be deleted when the <code>NutanixCluster</code> is deleted.</p> <p>Note: There is a 1:1 relation between the secret and the <code>NutanixCluster</code> object. </p>"},{"location":"capx/v1.0.x/credential_management/#example_1","title":"Example","text":"<p>Create a secret in the namespace of the <code>NutanixCluster</code>:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\nname: \"&lt;my-secret&gt;\"\nnamespace: \"&lt;nutanixcluster-namespace&gt;\"\nstringData:\ncredentials: |\n[\n{\n\"type\": \"basic_auth\", \n\"data\": { \n\"prismCentral\":{\n\"username\": \"&lt;nutanix-user&gt;\",\n\"password\": \"&lt;nutanix-password&gt;\"\n},\n\"prismElements\": null\n}\n}\n]\n</code></pre> <p>Add a <code>prismCentral</code> and corresponding <code>credentialRef</code> to the <code>NutanixCluster</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixCluster\nmetadata:\nname: \"&lt;nutanixcluster-name&gt;\"\nnamespace: \"&lt;nutanixcluster-namespace&gt;\"\nspec:\nprismCentral:\n...\ncredentialRef:\nname: \"&lt;my-secret&gt;\"\nkind: Secret\n...\n</code></pre> <p>See the NutanixCluster documentation for all supported configuration parameters for the <code>prismCentral</code> and <code>credentialRef</code> attribute.</p>"},{"location":"capx/v1.0.x/getting_started/","title":"Getting Started","text":"<p>This is a guide on getting started with Cluster API Provider Nutanix Cloud Infrastructure (CAPX). To learn more about cluster API in more depth, check out the Cluster API book.</p> <p>For more information on how install the Nutanix CSI Driver on a CAPX cluster, visit Nutanix CSI Driver installation with CAPX.</p> <p>For more information on how CAPX handles credentials, visit Credential Management.</p> <p>For more information on the port requirements for CAPX, visit Port Requirements.</p>"},{"location":"capx/v1.0.x/getting_started/#production-workflow","title":"Production Workflow","text":""},{"location":"capx/v1.0.x/getting_started/#build-os-image-for-nutanixmachinetemplate-resource","title":"Build OS image for NutanixMachineTemplate resource","text":"<p>To build an OS image for NutanixMachineTemplate, visit Nutanix OS Image Builder.</p>"},{"location":"capx/v1.0.x/getting_started/#configure-and-install-cluster-api-provider-nutanix-cloud-infrastructure","title":"Configure and Install Cluster API Provider Nutanix Cloud Infrastructure","text":"<p>To initialize Cluster API Provider Nutanix Cloud Infrastructure, <code>clusterctl</code> requires the following variables, which should be set in either <code>~/.cluster-api/clusterctl.yaml</code> or as environment variables. <pre><code>NUTANIX_ENDPOINT: \"\"    # IP or FQDN of Prism Central\nNUTANIX_USER: \"\"        # Prism Central user\nNUTANIX_PASSWORD: \"\"    # Prism Central password\nNUTANIX_INSECURE: false # or true\n\nKUBERNETES_VERSION: \"v1.22.9\"\nWORKER_MACHINE_COUNT: 3\nNUTANIX_SSH_AUTHORIZED_KEY: \"\"\n\nNUTANIX_PRISM_ELEMENT_CLUSTER_NAME: \"\"\nNUTANIX_MACHINE_TEMPLATE_IMAGE_NAME: \"\"\nNUTANIX_SUBNET_NAME: \"\"\n</code></pre></p> <p>You can also see the required list of variables by running the following: <pre><code>clusterctl generate cluster mycluster -i nutanix --list-variables           \nRequired Variables:\n  - CONTROL_PLANE_ENDPOINT_IP\n  - KUBERNETES_VERSION\n  - NUTANIX_ENDPOINT\n  - NUTANIX_MACHINE_TEMPLATE_IMAGE_NAME\n  - NUTANIX_PASSWORD\n  - NUTANIX_PRISM_ELEMENT_CLUSTER_NAME\n  - NUTANIX_SSH_AUTHORIZED_KEY\n  - NUTANIX_SUBNET_NAME\n  - NUTANIX_USER\n\nOptional Variables:\n  - CONTROL_PLANE_ENDPOINT_PORT      (defaults to \"6443\")\n  - CONTROL_PLANE_MACHINE_COUNT      (defaults to 1)\n  - KUBEVIP_LB_ENABLE                (defaults to \"false\")\n  - KUBEVIP_SVC_ENABLE               (defaults to \"false\")\n  - NAMESPACE                        (defaults to current Namespace in the KubeConfig file)\n  - NUTANIX_INSECURE                 (defaults to \"false\")\n  - NUTANIX_MACHINE_BOOT_TYPE        (defaults to \"legacy\")\n  - NUTANIX_MACHINE_MEMORY_SIZE      (defaults to \"4Gi\")\n  - NUTANIX_MACHINE_VCPU_PER_SOCKET  (defaults to \"1\")\n  - NUTANIX_MACHINE_VCPU_SOCKET      (defaults to \"2\")\n  - NUTANIX_PORT                     (defaults to \"9440\")\n  - NUTANIX_SYSTEMDISK_SIZE          (defaults to \"40Gi\")\n  - WORKER_MACHINE_COUNT             (defaults to 0)\n</code></pre></p> <p>Note</p> <p>To prevent duplicate IP assignments, it is required to assign an IP-address to the <code>CONTROL_PLANE_ENDPOINT_IP</code> variable that is not part of the Nutanix IPAM or DHCP range assigned to the subnet of the CAPX cluster. </p> <p>Now you can instantiate Cluster API with the following: <pre><code>clusterctl init -i nutanix\n</code></pre></p>"},{"location":"capx/v1.0.x/getting_started/#deploy-a-workload-cluster-on-nutanix-cloud-infrastructure","title":"Deploy a workload cluster on Nutanix Cloud Infrastructure","text":"<p><pre><code>export TEST_CLUSTER_NAME=mytestcluster1\nexport TEST_NAMESPACE=mytestnamespace\nCONTROL_PLANE_ENDPOINT_IP=x.x.x.x clusterctl generate cluster ${TEST_CLUSTER_NAME} \\\n    -i nutanix \\\n    --target-namespace ${TEST_NAMESPACE}  \\\n    --kubernetes-version v1.22.9 \\\n    --control-plane-machine-count 1 \\\n    --worker-machine-count 3 &gt; ./cluster.yaml\nkubectl create ns ${TEST_NAMESPACE}\nkubectl apply -f ./cluster.yaml -n ${TEST_NAMESPACE}\n</code></pre> To customize the configuration of the default <code>cluster.yaml</code> file generated by CAPX, visit the  NutanixCluster and  NutanixMachineTemplate documentation.</p>"},{"location":"capx/v1.0.x/getting_started/#access-a-workload-cluster","title":"Access a workload cluster","text":"<p>To access resources on the cluster, you can get the kubeconfig with the following: <pre><code>clusterctl get kubeconfig ${TEST_CLUSTER_NAME} -n ${TEST_NAMESPACE} &gt; ${TEST_CLUSTER_NAME}.kubeconfig\nkubectl --kubeconfig ./${TEST_CLUSTER_NAME}.kubeconfig get nodes \n</code></pre></p>"},{"location":"capx/v1.0.x/getting_started/#install-cni-on-workload-a-cluster","title":"Install CNI on workload a cluster","text":"<p>You must deploy a Container Network Interface (CNI) based pod network add-on so that your pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed.</p> <p>Note</p> <p>Take care that your pod network must not overlap with any of the host networks. You are likely to see problems if there is any overlap. If you find a collision between your network plugin's preferred pod network and some of your host networks, you must choose a suitable alternative CIDR block to use instead. It can be configured inside the <code>cluster.yaml</code> generated by <code>clusterctl generate cluster</code> before applying it.</p> <p>Several external projects provide Kubernetes pod networks using CNI, some of which also support Network Policy.</p> <p>See a list of add-ons that implement the Kubernetes networking model. At time of writing, the most common are Calico and Cilium.</p> <p>Follow the specific install guide for your selected CNI and install only one pod network per cluster.</p> <p>Once a pod network has been installed, you can confirm that it is working by checking that the CoreDNS pod is running in the output of <code>kubectl get pods --all-namespaces</code>.</p>"},{"location":"capx/v1.0.x/getting_started/#kube-vip-settings","title":"Kube-vip settings","text":"<p>Kube-vip is a true load balancing solution for the Kubernetes control plane. It distributes API requests across control plane nodes. It also has the capability to provide load balancing for Kubernetes services.</p> <p>You can tweak kube-vip settings by using the following properties:</p> <ul> <li><code>KUBEVIP_LB_ENABLE</code></li> </ul> <p>This setting allows control plane load balancing using IPVS. See Control Plane Load-Balancing documentation for further information.</p> <ul> <li><code>KUBEVIP_SVC_ENABLE</code> </li> </ul> <p>This setting enables a service of type LoadBalancer. See Kubernetes Service Load Balancing documentation for further information.</p> <ul> <li><code>KUBEVIP_SVC_ELECTION</code></li> </ul> <p>This setting enables Load Balancing of Load Balancers. See Load Balancing Load Balancers for further information.</p>"},{"location":"capx/v1.0.x/getting_started/#delete-a-workload-cluster","title":"Delete a workload cluster","text":"<p>To remove a workload cluster from your management cluster, remove the cluster object and the provider will clean-up all resources. </p> <pre><code>kubectl delete cluster ${TEST_CLUSTER_NAME} -n $(TEST_NAMESPACE)\n</code></pre> <p>Note</p> <p>Deleting the entire cluster template with <code>kubectl delete -f ./cluster.yaml</code> may lead to pending resources requiring manual cleanup.</p>"},{"location":"capx/v1.0.x/port_requirements/","title":"Port Requirements","text":"<p>CAPX uses the ports documented below to create workload clusters. </p> <p>Note<p>This page only documents the ports specifically required by CAPX and does not provide the full overview of all ports required in the CAPI framework.</p> </p>"},{"location":"capx/v1.0.x/port_requirements/#management-cluster","title":"Management cluster","text":"Source Destination Protocol Port Description Management cluster External Registries TCP 443 Pull container images from CAPX public registries Management cluster Prism Central TCP 9440 Management cluster communication to Prism Central"},{"location":"capx/v1.0.x/port_requirements/#public-registries-utilized-when-using-capx","title":"Public registries utilized when using CAPX","text":"Registry name ghcr.io"},{"location":"capx/v1.0.x/troubleshooting/","title":"Troubleshooting","text":""},{"location":"capx/v1.0.x/troubleshooting/#clusterctl-failed-with-github-rate-limit-error","title":"Clusterctl failed with GitHub rate limit error","text":"<p>By design Clusterctl fetches artifacts from repositories hosted on GitHub, this operation is subject to GitHub API rate limits.</p> <p>While this is generally okay for the majority of users, there is still a chance that some users (especially developers or CI tools) hit this limit:</p> <pre><code>Error: failed to get repository client for the XXX with name YYY: error creating the GitHub repository client: failed to get GitHub latest version: failed to get the list of versions: rate limit for github api has been reached. Please wait one hour or get a personal API tokens a assign it to the GITHUB_TOKEN environment variable\n</code></pre> <p>As explained in the error message, you can increase your API rate limit by creating a GitHub personal token and setting a <code>GITHUB_TOKEN</code> environment variable using the token.</p>"},{"location":"capx/v1.0.x/validated_integrations/","title":"Validated Integrations","text":"<p>Validated integrations are a defined set of specifically tested configurations between technologies that represent the most common combinations that Nutanix customers are using or deploying with CAPX. For these integrations, Nutanix has directly, or through certified partners, exercised a full range of platform tests as part of the product release process.</p>"},{"location":"capx/v1.0.x/validated_integrations/#integration-validation-policy","title":"Integration Validation Policy","text":"<p>Nutanix follows the version validation policies below:</p> <ul> <li> <p>Validate at least one active AOS LTS (long term support) version. Validated AOS LTS version for a specific CAPX version is listed in the AOS section.</p> <p>Note</p> <p>Typically the latest LTS release at time of CAPX release except when latest is initial release in train (eg x.y.0). Exact version depends on timing and customer adoption.</p> </li> <li> <p>Validate the latest AOS STS (short term support) release at time of CAPX release.</p> </li> <li> <p>Validate at least one active Prism Central (PC) version. Validated PC version for a specific CAPX version is listed in the Prism Central section.</p> <p>Note</p> <p>Typically the the latest PC release at time of CAPX release except when latest is initial release in train (eg x.y.0). Exact version depends on timing and customer adoption.</p> </li> <li> <p>At least one active Cluster-API (CAPI) version. Validated CAPI version for a specific CAPX version is listed in the Cluster-API section.</p> <p>Note</p> <p>Typically the the latest Cluster-API release at time of CAPX release except when latest is initial release in train (eg x.y.0). Exact version depends on timing and customer adoption.</p> </li> </ul>"},{"location":"capx/v1.0.x/validated_integrations/#validated-versions","title":"Validated versions","text":""},{"location":"capx/v1.0.x/validated_integrations/#cluster-api","title":"Cluster-API","text":"CAPX CAPI v1.1.4+ CAPI v1.2.x v1.0.x Yes Yes v0.5.x Yes Yes <p>See the Validated Kubernetes Versions page for more information on CAPI validated versions.</p>"},{"location":"capx/v1.0.x/validated_integrations/#aos","title":"AOS","text":"CAPX 5.20.4.5 (LTS) 6.1.1.5 (STS) v1.0.x Yes Yes v0.5.x Yes Yes"},{"location":"capx/v1.0.x/validated_integrations/#prism-central","title":"Prism Central","text":"CAPX 2022.1.0.2 pc.2022.6 v1.0.x Yes Yes v0.5.x Yes Yes"},{"location":"capx/v1.0.x/addons/install_csi_driver/","title":"Nutanix CSI Driver installation with CAPX","text":"<p>The Nutanix CSI driver is fully supported on CAPI/CAPX deployed clusters where all the nodes meet the Nutanix CSI driver prerequisites.</p> <p>There are three methods to install the Nutanix CSI driver on a CAPI/CAPX cluster:</p> <ul> <li>Helm</li> <li>ClusterResourceSet</li> <li>CAPX Flavor</li> </ul> <p>For more information, check the next sections.</p>"},{"location":"capx/v1.0.x/addons/install_csi_driver/#capi-workload-cluster-prerequisites-for-the-nutanix-csi-driver","title":"CAPI Workload cluster prerequisites for the Nutanix CSI Driver","text":"<p>Kubernetes workers need the following prerequisites to use the Nutanix CSI Drivers:</p> <ul> <li>iSCSI initiator package (for Volumes based block storage)</li> <li>NFS client package (for Files based storage)</li> </ul> <p>These packages may already be present in the image you use with your infrastructure provider or you can also rely on your bootstrap provider to install them. More info is available in the Prerequisites docs.</p> <p>The package names and installation method will also vary depending on the operating system you plan to use.</p> <p>In the example below, <code>kubeadm</code> bootstrap provider is used to deploy these packages on top of an Ubuntu 20.04 image. The <code>kubeadm</code> bootstrap provider allows defining <code>preKubeadmCommands</code> that will be launched before Kubernetes cluster creation. These <code>preKubeadmCommands</code> can be defined both in <code>KubeadmControlPlane</code> for master nodes and in <code>KubeadmConfigTemplate</code> for worker nodes.</p> <p>In the example with an Ubuntu 20.04 image, both <code>KubeadmControlPlane</code> and <code>KubeadmConfigTemplate</code> must be modified as in the example below:</p> <pre><code>spec:\ntemplate:\nspec:\n# .......\npreKubeadmCommands:\n- echo \"before kubeadm call\" &gt; /var/log/prekubeadm.log\n- apt update\n- apt install -y nfs-common open-iscsi\n- systemctl enable --now iscsid\n</code></pre>"},{"location":"capx/v1.0.x/addons/install_csi_driver/#install-the-nutanix-csi-driver-with-helm","title":"Install the Nutanix CSI Driver with Helm","text":"<p>A recent Helm version is needed (tested with Helm v3.10.1).</p> <p>The example below must be applied on a ready workload cluster. The workload cluster's kubeconfig can be retrieved and used to connect with the following command:</p> <pre><code>clusterctl get kubeconfig $CLUSTER_NAME -n $CLUSTER_NAMESPACE &gt; $CLUSTER_NAME-KUBECONFIG\nexport KUBECONFIG=$(pwd)/$CLUSTER_NAME-KUBECONFIG\n</code></pre> <p>Once connected to the cluster, follow the CSI documentation. </p> <p>First, install the nutanix-csi-snapshot chart followed by the nutanix-csi-storage chart.</p> <p>See an example below:</p> <pre><code>#Add the official Nutanix Helm repo and get the latest update\nhelm repo add nutanix https://nutanix.github.io/helm/\nhelm repo update\n\n# Install the nutanix-csi-snapshot chart\nhelm install nutanix-csi-snapshot nutanix/nutanix-csi-snapshot -n ntnx-system --create-namespace\n\n# Install the nutanix-csi-storage chart\nhelm install nutanix-storage nutanix/nutanix-csi-storage -n ntnx-system --set createSecret=false\n</code></pre> <p>Warning</p> <p>For correct Nutanix CSI driver deployment, a fully functional CNI deployment must be present.</p>"},{"location":"capx/v1.0.x/addons/install_csi_driver/#install-the-nutanix-csi-driver-with-clusterresourceset","title":"Install the Nutanix CSI Driver with <code>ClusterResourceSet</code>","text":"<p>The <code>ClusterResourceSet</code> feature was introduced to automatically apply a set of resources (such as CNI/CSI) defined by administrators to matching created/existing workload clusters.</p>"},{"location":"capx/v1.0.x/addons/install_csi_driver/#enabling-the-clusterresourceset-feature","title":"Enabling the <code>ClusterResourceSet</code> feature","text":"<p>At the time of writing, <code>ClusterResourceSet</code> is an experimental feature that must be enabled during the initialization of a management cluster with the <code>EXP_CLUSTER_RESOURCE_SET</code> feature gate.</p> <p>To do this, add <code>EXP_CLUSTER_RESOURCE_SET: \"true\"</code> in the <code>clusterctl</code> configuration file or just <code>export EXP_CLUSTER_RESOURCE_SET=true</code> before initializing the management cluster with <code>clusterctl init</code>.</p> <p>If the management cluster is already initialized, the <code>ClusterResourceSet</code> can be enabled by changing the configuration of the <code>capi-controller-manager</code> deployment in the <code>capi-system</code> namespace.</p> <pre><code>kubectl edit deployment -n capi-system capi-controller-manager\n</code></pre> <p>Locate the section below:</p> <pre><code>  - args:\n- --leader-elect\n- --metrics-bind-addr=localhost:8080\n- --feature-gates=MachinePool=false,ClusterResourceSet=true,ClusterTopology=false\n</code></pre> <p>Then replace <code>ClusterResourceSet=false</code> with <code>ClusterResourceSet=true</code>.</p> <p>Note</p> <p>Editing the <code>deployment</code> resource will cause Kubernetes to automatically start new versions of the containers with the feature enabled.</p>"},{"location":"capx/v1.0.x/addons/install_csi_driver/#prepare-the-nutanix-csi-clusterresourceset","title":"Prepare the Nutanix CSI <code>ClusterResourceSet</code>","text":""},{"location":"capx/v1.0.x/addons/install_csi_driver/#create-the-configmap-for-the-cni-plugin","title":"Create the <code>ConfigMap</code> for the CNI Plugin","text":"<p>First, create a <code>ConfigMap</code> that contains a YAML manifest with all resources to install the Nutanix CSI driver.</p> <p>Since the Nutanix CSI Driver is provided as a Helm chart, use <code>helm</code> to extract it before creating the <code>ConfigMap</code>. See an example below:</p> <pre><code>helm repo add nutanix https://nutanix.github.io/helm/\nhelm repo update\n\nkubectl create ns ntnx-system --dry-run=client -o yaml &gt; nutanix-csi-namespace.yaml\nhelm template nutanix-csi-snapshot nutanix/nutanix-csi-snapshot -n ntnx-system &gt; nutanix-csi-snapshot.yaml\nhelm template nutanix-csi-snapshot nutanix/nutanix-csi-storage -n ntnx-system &gt; nutanix-csi-storage.yaml\n\nkubectl create configmap nutanix-csi-crs --from-file=nutanix-csi-namespace.yaml --from-file=nutanix-csi-snapshot.yaml --from-file=nutanix-csi-storage.yaml\n</code></pre>"},{"location":"capx/v1.0.x/addons/install_csi_driver/#create-the-clusterresourceset","title":"Create the <code>ClusterResourceSet</code>","text":"<p>Next, create the <code>ClusterResourceSet</code> resource that will map the <code>ConfigMap</code> defined above to clusters using a <code>clusterSelector</code>.</p> <p>The <code>ClusterResourceSet</code> needs to be created inside the management cluster. See an example below:</p> <pre><code>---\napiVersion: addons.cluster.x-k8s.io/v1alpha3\nkind: ClusterResourceSet\nmetadata:\nname: nutanix-csi-crs\nspec:\nclusterSelector:\nmatchLabels:\ncsi: nutanix resources:\n- kind: ConfigMap\nname: nutanix-csi-crs\n</code></pre> <p>The <code>clusterSelector</code> field controls how Cluster API will match this <code>ClusterResourceSet</code> on one or more workload clusters. In the example scenario, the <code>matchLabels</code> approach is being used where the <code>ClusterResourceSet</code> will be applied to all workload clusters having the <code>csi: nutanix</code> label present. If the label isn't present, the <code>ClusterResourceSet</code> won't apply to that workload cluster.</p> <p>The <code>resources</code> field references the <code>ConfigMap</code> created above, which contains the manifests for installing the Nutanix CSI driver.</p>"},{"location":"capx/v1.0.x/addons/install_csi_driver/#assign-the-clusterresourceset-to-a-workload-cluster","title":"Assign the <code>ClusterResourceSet</code> to a workload cluster","text":"<p>Assign this <code>ClusterResourceSet</code> to the workload cluster by adding the correct label to the <code>Cluster</code> resource.</p> <p>This can be done before workload cluster creation by editing the output of the <code>clusterctl generate cluster</code> command or by modifying an already deployed workload cluster.</p> <p>In both cases, <code>Cluster</code> resources should look like this:</p> <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\nmetadata:\nname: workload-cluster-name\nnamespace: workload-cluster-namespace\nlabels:\ncsi: nutanix\n# ...\n</code></pre> <p>Warning</p> <p>For correct Nutanix CSI driver deployment, a fully functional CNI deployment must be present.</p>"},{"location":"capx/v1.0.x/addons/install_csi_driver/#install-the-nutanix-csi-driver-with-a-capx-flavor","title":"Install the Nutanix CSI Driver with a CAPX flavor","text":"<p>The CAPX provider can utilize a flavor to automatically deploy the Nutanix CSI using a <code>ClusterResourceSet</code>.</p>"},{"location":"capx/v1.0.x/addons/install_csi_driver/#prerequisites","title":"Prerequisites","text":"<p>The following requirements must be met:</p> <ul> <li>The operating system must meet the Nutanix CSI OS prerequisites.</li> <li>The Management cluster must be installed with the <code>CLUSTER_RESOURCE_SET</code> feature gate.</li> </ul>"},{"location":"capx/v1.0.x/addons/install_csi_driver/#installation","title":"Installation","text":"<p>Specify the <code>csi</code> flavor during workload cluster creation. See an example below:</p> <pre><code>clusterctl generate cluster my-cluster -f csi\n</code></pre> <p>Additional environment variables are required:</p> <ul> <li><code>WEBHOOK_CA</code>: Base64 encoded CA certificate used to sign the webhook certificate</li> <li><code>WEBHOOK_CERT</code>: Base64 certificate for the webhook validation component</li> <li><code>WEBHOOK_KEY</code>: Base64 key for the webhook validation component</li> </ul> <p>The three components referenced above can be automatically created and referenced using this script:</p> <pre><code>source scripts/gen-self-cert.sh\n</code></pre> <p>The certificate must reference the following names:</p> <ul> <li>csi-snapshot-webhook</li> <li>csi-snapshot-webhook.ntnx-sytem</li> <li>csi-snapshot-webhook.ntnx-sytem.svc</li> </ul> <p>Warning</p> <p>For correct Nutanix CSI driver deployment, a fully functional CNI deployment must be present.</p>"},{"location":"capx/v1.0.x/addons/install_csi_driver/#nutanix-csi-driver-configuration","title":"Nutanix CSI Driver Configuration","text":"<p>After the driver is installed, it must be configured for use by minimally defining a <code>Secret</code> and <code>StorageClass</code>.</p> <p>This can be done manually in the workload clusters or by using a <code>ClusterResourceSet</code> in the management cluster as explained above.</p> <p>See the Official CSI Driver documentation on the Nutanix Portal for more configuration information. </p>"},{"location":"capx/v1.0.x/experimental/autoscaler/","title":"Using Autoscaler in combination with CAPX","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>Autoscaler can be used in combination with Cluster API to automatically add or remove machines in a cluster. </p> <p>Autoscaler can be used in different deployment scenarios. This page will provide an overview of multiple autoscaler deployment scenarios in combination with CAPX. See the Testing section to see how scale-up/scale-down events can be triggered to validate the autoscaler behaviour.</p> <p>More in-depth information on Autoscaler functionality can be found in the Kubernetes documentation.</p> <p>All Autoscaler configuration parameters can be found here.</p>"},{"location":"capx/v1.0.x/experimental/autoscaler/#scenario-1-management-cluster-managing-an-external-workload-cluster","title":"Scenario 1: Management cluster managing an external workload cluster","text":"<p>In this scenario, Autoscaler will be running on a management cluster and it will manage an external workload cluster. See the management cluster managing an external workload cluster section of Kubernetes documentation for more information.</p>"},{"location":"capx/v1.0.x/experimental/autoscaler/#steps","title":"Steps","text":"<ol> <li> <p>Deploy a management cluster and workload cluster. The CAPI quickstart can be used as a starting point.</p> <p>Note<p>Make sure a CNI is installed in the workload cluster.</p> </p> </li> <li> <p>Download the example Autoscaler deployment file.</p> </li> <li>Modify the <code>deployment.yaml</code> file:<ul> <li>Change the namespace of all resources to the namespaces of the workload cluster.</li> <li>Choose an autoscale image.</li> <li>Change the following parameters in the <code>Deployment</code> resource: <pre><code>        spec:\ncontainers:\nname: cluster-autoscaler\ncommand:\n- /cluster-autoscaler\nargs:\n- --cloud-provider=clusterapi\n- --kubeconfig=/mnt/kubeconfig/kubeconfig.yml\n- --clusterapi-cloud-config-authoritative\n- -v=1\nvolumeMounts:\n- mountPath: /mnt/kubeconfig\nname: kubeconfig\nreadOnly: true\n...\nvolumes:\n- name: kubeconfig\nsecret:\nsecretName: &lt;workload cluster name&gt;-kubeconfig\nitems:\n- key: value\npath: kubeconfig.yml\n</code></pre></li> </ul> </li> <li>Apply the <code>deployment.yaml</code> file. <pre><code>kubectl apply -f deployment.yaml\n</code></pre></li> <li>Add the annotations to the workload cluster <code>MachineDeployment</code> resource.</li> <li>Test Autoscaler. Go to the Testing section.</li> </ol>"},{"location":"capx/v1.0.x/experimental/autoscaler/#scenario-2-autoscaler-running-on-workload-cluster","title":"Scenario 2: Autoscaler running on workload cluster","text":"<p>In this scenario, Autoscaler will be deployed on top of the workload cluster directly. In order for Autoscaler to work, it is required that the workload cluster resources are moved from the management cluster to the workload cluster.</p>"},{"location":"capx/v1.0.x/experimental/autoscaler/#steps_1","title":"Steps","text":"<ol> <li>Deploy a management cluster and workload cluster. The CAPI quickstart can be used as a starting point.</li> <li>Get the kubeconfig file for the workload cluster and use this kubeconfig to login to the workload cluster.  <pre><code>clusterctl get kubeconfig &lt;workload cluster name&gt; -n &lt;workload cluster namespace &gt; /path/to/kubeconfig\n</code></pre></li> <li>Install a CNI in the workload cluster.</li> <li>Initialise the CAPX components on top of the workload cluster: <pre><code>clusterctl init --infrastructure nutanix\n</code></pre></li> <li>Migrate the workload cluster custom resources to the workload cluster. Run following command from the management cluster: <pre><code>clusterctl move -n &lt;workload cluster ns&gt;  --to-kubeconfig /path/to/kubeconfig\n</code></pre></li> <li>Verify if the cluster has been migrated by running following command on the workload cluster: <pre><code>kubectl get cluster -A </code></pre></li> <li>Download the example autoscaler deployment file.</li> <li>Create the Autoscaler namespace: <pre><code>kubectl create ns autoscaler\n</code></pre></li> <li>Apply the <code>deployment.yaml</code> file <pre><code>kubectl apply -f deployment.yaml\n</code></pre></li> <li>Add the annotations to the workload cluster <code>MachineDeployment</code> resource.</li> <li>Test Autoscaler. Go to the Testing section.</li> </ol>"},{"location":"capx/v1.0.x/experimental/autoscaler/#testing","title":"Testing","text":"<ol> <li>Deploy an example Kubernetes application. For example, the one used in the Kubernetes HorizontalPodAutoscaler Walkthrough. <pre><code>kubectl apply -f https://k8s.io/examples/application/php-apache.yaml </code></pre></li> <li>Increase the amount of replicas of the application to trigger a scale-up event: <pre><code>kubectl scale deployment php-apache --replicas 100\n</code></pre></li> <li> <p>Decrease the amount of replicas of the application again to trigger a scale-down event.</p> <p>Note</p> <p>In case of issues check the logs of the Autoscaler pods.</p> </li> <li> <p>After a while CAPX, will add more machines. Refer to the Autoscaler configuration parameters to tweak the behaviour and timeouts.</p> </li> </ol>"},{"location":"capx/v1.0.x/experimental/autoscaler/#autoscaler-node-group-annotations","title":"Autoscaler node group annotations","text":"<p>Autoscaler uses following annotations to define the upper and lower boundries of the managed machines:</p> Annotation Example Value Description cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size 5 Maximum amount of machines in this node group cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size 1 Minimum amount of machines in this node group <p>These annotations must be applied to the <code>MachineDeployment</code> resources of a CAPX cluster. </p>"},{"location":"capx/v1.0.x/experimental/autoscaler/#example","title":"Example","text":"<pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: MachineDeployment\nmetadata:\nannotations:\ncluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \"5\"\ncluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \"1\"\n</code></pre>"},{"location":"capx/v1.0.x/experimental/capx_multi_pe/","title":"Creating a workload CAPX cluster spanning Prism Element clusters","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>This page will explain how to deploy CAPX-based Kubernetes clusters where worker nodes are spanning multiple Prism Element (PE) clusters. </p> <p>Note<p>All the PE clusters must be managed by the same Prism Central (PC) instance.</p> </p> <p>The topology will look like this: </p> <ul> <li>One PC managing multiple PE's</li> <li>One CAPI management cluster</li> <li>One CAPI workload cluster with multiple <code>MachineDeployment</code>resources</li> </ul> <p>Refer to the CAPI quickstart to get started with CAPX. </p> <p>To create workload clusters spanning multiple Prism Element clusters, it is required to create a <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resource for each Prism Element cluster. The Prism Element specific parameters (name/UUID, subnet,...) are referenced in the <code>NutanixMachineTemplate</code>. </p>"},{"location":"capx/v1.0.x/experimental/capx_multi_pe/#steps","title":"Steps","text":"<ol> <li>Create a management cluster that has the CAPX infrastructure provider deployed.</li> <li>Create a <code>cluster.yml</code> file containing the workload cluster definition. Refer to the steps defined in the CAPI quickstart guide to create an example <code>cluster.yml</code> file.</li> <li> <p>Add additional <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resources.</p> <p>By default there is only one machine template and machine deployment defined. To add nodes residing on another Prism Element cluster, a new <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resource needs to be added to the yaml file. The autogenerated <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resource definitions can be used as a baseline.</p> <p>Make sure to modify the <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> parameters.</p> </li> <li> <p>Apply the modified <code>cluster.yml</code> file to the management cluster.</p> </li> </ol>"},{"location":"capx/v1.0.x/tasks/modify_machine_configuration/","title":"Modifying Machine Configurations","text":"<p>Since all attributes of the <code>NutanixMachineTemplate</code> resources are immutable, follow the Updating Infrastructure Machine Templates procedure to modify the configuration of machines in an existing CAPX cluster. See the NutanixMachineTemplate documentation for all supported configuration parameters.</p> <p>Note</p> <p>Manually modifying existing and linked <code>NutanixMachineTemplate</code> resources will not trigger a rolling update of the machines. </p>"},{"location":"capx/v1.0.x/types/nutanix_cluster/","title":"NutanixCluster","text":"<p>The <code>NutanixCluster</code> resource defines the configuration of a CAPX Kubernetes cluster. </p> <p>Example of a <code>NutanixCluster</code> resource:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixCluster\nmetadata:\nname: ${CLUSTER_NAME}\nnamespace: ${NAMESPACE}\nspec:\ncontrolPlaneEndpoint:\nhost: ${CONTROL_PLANE_ENDPOINT_IP}\nport: ${CONTROL_PLANE_ENDPOINT_PORT=6443}\nprismCentral:\naddress: ${NUTANIX_ENDPOINT}\ncredentialRef:\nkind: Secret\nname: ${CLUSTER_NAME}\ninsecure: ${NUTANIX_INSECURE=false}\nport: ${NUTANIX_PORT=9440}\n</code></pre>"},{"location":"capx/v1.0.x/types/nutanix_cluster/#nutanixcluster-spec","title":"NutanixCluster spec","text":"<p>The table below provides an overview of the supported parameters of the <code>spec</code> attribute of a <code>NutanixCluster</code> resource.</p>"},{"location":"capx/v1.0.x/types/nutanix_cluster/#configuration-parameters","title":"Configuration parameters","text":"Key Type Description controlPlaneEndpoint object Defines the host IP and port of the CAPX Kubernetes cluster. controlPlaneEndpoint.host string Host IP to be assigned to the CAPX Kubernetes cluster. controlPlaneEndpoint.port int Port of the CAPX Kubernetes cluster. Default: <code>6443</code> prismCentral object (Optional) Prism Central endpoint definition. prismCentral.address string IP/FQDN of Prism Central. prismCentral.port int Port of Prism Central. Default: <code>9440</code> prismCentral.insecure bool Disable Prism Central certificate checking. Default: <code>false</code> prismCentral.credentialRef object Reference to credentials used for Prism Central connection. prismCentral.credentialRef.kind string Kind of the credentialRef. Allowed value: <code>Secret</code> prismCentral.credentialRef.name string Name of the secret containing the Prism Central credentials. prismCentral.credentialRef.namespace string (Optional) Namespace of the secret containing the Prism Central credentials. <p>Note</p> <p>To prevent duplicate IP assignments, it is required to assign an IP-address to the <code>controlPlaneEndpoint.host</code> variable that is not part of the Nutanix IPAM or DHCP range assigned to the subnet of the CAPX cluster.</p>"},{"location":"capx/v1.0.x/types/nutanix_machine_template/","title":"NutanixMachineTemplate","text":"<p>The <code>NutanixMachineTemplate</code> resource defines the configuration of a CAPX Kubernetes VM. </p> <p>Example of a <code>NutanixMachineTemplate</code> resource.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixMachineTemplate\nmetadata:\nname: \"${CLUSTER_NAME}-mt-0\"\nnamespace: \"${NAMESPACE}\"\nspec:\ntemplate:\nspec:\nproviderID: \"nutanix://${CLUSTER_NAME}-m1\"\n# Supported options for boot type: legacy and uefi\n# Defaults to legacy if not set\nbootType: ${NUTANIX_MACHINE_BOOT_TYPE=legacy}\nvcpusPerSocket: ${NUTANIX_MACHINE_VCPU_PER_SOCKET=1}\nvcpuSockets: ${NUTANIX_MACHINE_VCPU_SOCKET=2}\nmemorySize: \"${NUTANIX_MACHINE_MEMORY_SIZE=4Gi}\"\nsystemDiskSize: \"${NUTANIX_SYSTEMDISK_SIZE=40Gi}\"\nimage:\ntype: name\nname: \"${NUTANIX_MACHINE_TEMPLATE_IMAGE_NAME}\"\ncluster:\ntype: name\nname: \"${NUTANIX_PRISM_ELEMENT_CLUSTER_NAME}\"\nsubnet:\n- type: name\nname: \"${NUTANIX_SUBNET_NAME}\"\n# Adds additional categories to the virtual machines.\n# Note: Categories must already be present in Prism Central\n# additionalCategories:\n#   - key: AppType\n#     value: Kubernetes\n# Adds the cluster virtual machines to a project defined in Prism Central.\n# Replace NUTANIX_PROJECT_NAME with the correct project defined in Prism Central\n# Note: Project must already be present in Prism Central.\n# project:\n#   type: name\n#   name: \"NUTANIX_PROJECT_NAME\"\n</code></pre>"},{"location":"capx/v1.0.x/types/nutanix_machine_template/#nutanixmachinetemplate-spec","title":"NutanixMachineTemplate spec","text":"<p>The table below provides an overview of the supported parameters of the <code>spec</code> attribute of a <code>NutanixMachineTemplate</code> resource.</p>"},{"location":"capx/v1.0.x/types/nutanix_machine_template/#configuration-parameters","title":"Configuration parameters","text":"Key Type Description bootType string Boot type of the VM. Depends on the OS image used. Allowed values: <code>legacy</code>, <code>uefi</code>. Default: <code>legacy</code> vcpusPerSocket int Amount of vCPUs per socket. Default: <code>1</code> vcpuSockets int Amount of vCPU sockets. Default: <code>2</code> memorySize string Amount of Memory. Default: <code>4Gi</code> systemDiskSize string Amount of storage assigned to the system disk. Default: <code>40Gi</code> image object Reference to the OS image used for the system disk. image.type string Type to identify the OS image. Allowed values: <code>name</code> and <code>uuid</code> image.name string Name or UUID of the image. cluster object Reference to the Prism Element cluster. cluster.type string Type to identify the Prism Element cluster. Allowed values: <code>name</code> and <code>uuid</code> cluster.name string Name or UUID of the Prism Element cluster. subnets list Reference to the subnets to be assigned to the VMs. subnets.[].type string Type to identify the subnet. Allowed values: <code>name</code> and <code>uuid</code> subnets.[].name string Name or UUID of the subnet. additionalCategories list Reference to the categories to be assigned to the VMs. These categories already exist in Prism Central. additionalCategories.[].key string Key of the category. additionalCategories.[].value string Value of the category. project object Reference to the project. This project must already exist in Prism Central. project.type string Type to identify the project. Allowed values: <code>name</code> and <code>uuid</code> project.name string Name or UUID of the project."},{"location":"capx/v1.1.x/credential_management/","title":"Credential Management","text":"<p>Cluster API Provider Nutanix Cloud Infrastructure (CAPX) interacts with Nutanix Prism Central (PC) APIs to manage the required Kubernetes cluster infrastructure resources.</p> <p>PC credentials are required to authenticate to the PC APIs. CAPX currently supports two mechanisms to supply the required credentials:</p> <ul> <li>Credentials injected into the CAPX manager deployment</li> <li>Workload cluster specific credentials</li> </ul>"},{"location":"capx/v1.1.x/credential_management/#credentials-injected-into-the-capx-manager-deployment","title":"Credentials injected into the CAPX manager deployment","text":"<p>By default, credentials will be injected into the CAPX manager deployment when CAPX is initialized. See the getting started guide for more information on the initialization.</p> <p>Upon initialization a <code>nutanix-creds</code> secret will automatically be created in the <code>capx-system</code> namespace. This secret will contain the values supplied via the <code>NUTANIX_USER</code> and <code>NUTANIX_PASSWORD</code> parameters. </p> <p>The <code>nutanix-creds</code> secret will be used for workload cluster deployment if no other credential is supplied.</p>"},{"location":"capx/v1.1.x/credential_management/#example","title":"Example","text":"<p>An example of the automatically created <code>nutanix-creds</code> secret can be found below: <pre><code>---\napiVersion: v1\nkind: Secret\ntype: Opaque\nmetadata:\nname: nutanix-creds\nnamespace: capx-system\nstringData:\ncredentials: |\n[\n{\n\"type\": \"basic_auth\", \n\"data\": { \n\"prismCentral\":{\n\"username\": \"&lt;nutanix-user&gt;\",\n\"password\": \"&lt;nutanix-password&gt;\"\n},\n\"prismElements\": null\n}\n}\n]\n</code></pre></p>"},{"location":"capx/v1.1.x/credential_management/#workload-cluster-specific-credentials","title":"Workload cluster specific credentials","text":"<p>Users can override the credentials injected in CAPX manager deployment by supplying a credential specific to a workload cluster. The credentials can be supplied by creating a secret in the same namespace as the <code>NutanixCluster</code> namespace. </p> <p>The secret can be referenced by adding a <code>credentialRef</code> inside the <code>prismCentral</code> attribute contained in the <code>NutanixCluster</code>.  The secret will also be deleted when the <code>NutanixCluster</code> is deleted.</p> <p>Note: There is a 1:1 relation between the secret and the <code>NutanixCluster</code> object. </p>"},{"location":"capx/v1.1.x/credential_management/#example_1","title":"Example","text":"<p>Create a secret in the namespace of the <code>NutanixCluster</code>:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\nname: \"&lt;my-secret&gt;\"\nnamespace: \"&lt;nutanixcluster-namespace&gt;\"\nstringData:\ncredentials: |\n[\n{\n\"type\": \"basic_auth\", \n\"data\": { \n\"prismCentral\":{\n\"username\": \"&lt;nutanix-user&gt;\",\n\"password\": \"&lt;nutanix-password&gt;\"\n},\n\"prismElements\": null\n}\n}\n]\n</code></pre> <p>Add a <code>prismCentral</code> and corresponding <code>credentialRef</code> to the <code>NutanixCluster</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixCluster\nmetadata:\nname: \"&lt;nutanixcluster-name&gt;\"\nnamespace: \"&lt;nutanixcluster-namespace&gt;\"\nspec:\nprismCentral:\n...\ncredentialRef:\nname: \"&lt;my-secret&gt;\"\nkind: Secret\n...\n</code></pre> <p>See the NutanixCluster documentation for all supported configuration parameters for the <code>prismCentral</code> and <code>credentialRef</code> attribute.</p>"},{"location":"capx/v1.1.x/getting_started/","title":"Getting Started","text":"<p>This is a guide on getting started with Cluster API Provider Nutanix Cloud Infrastructure (CAPX). To learn more about cluster API in more depth, check out the Cluster API book.</p> <p>For more information on how install the Nutanix CSI Driver on a CAPX cluster, visit Nutanix CSI Driver installation with CAPX.</p> <p>For more information on how CAPX handles credentials, visit Credential Management.</p> <p>For more information on the port requirements for CAPX, visit Port Requirements.</p>"},{"location":"capx/v1.1.x/getting_started/#production-workflow","title":"Production Workflow","text":""},{"location":"capx/v1.1.x/getting_started/#build-os-image-for-nutanixmachinetemplate-resource","title":"Build OS image for NutanixMachineTemplate resource","text":"<p>Cluster API Provider Nutanix Cloud Infrastructure (CAPX) uses the Image Builder project to build OS images used for the Nutanix machines. </p> <p>Follow the steps detailed in Building CAPI Images for Nutanix Cloud Platform (NCP) to use Image Builder on the Nutanix Cloud Platform.</p> <p>For a list of operating systems visit the OS image Configuration page.</p>"},{"location":"capx/v1.1.x/getting_started/#prerequisites-for-using-cluster-api-provider-nutanix-cloud-infrastructure","title":"Prerequisites for using Cluster API Provider Nutanix Cloud Infrastructure","text":"<p>The Cluster API installation section provides an overview of all required prerequisites:</p> <ul> <li>Common Prerequisites</li> <li>Install and/or configure a Kubernetes cluster</li> <li>Install clusterctl</li> <li>(Optional) Enabling Feature Gates</li> </ul> <p>Make sure these prerequisites have been met before moving to the Configure and Install Cluster API Provider Nutanix Cloud Infrastructure step.</p>"},{"location":"capx/v1.1.x/getting_started/#configure-and-install-cluster-api-provider-nutanix-cloud-infrastructure","title":"Configure and Install Cluster API Provider Nutanix Cloud Infrastructure","text":"<p>To initialize Cluster API Provider Nutanix Cloud Infrastructure, <code>clusterctl</code> requires the following variables, which should be set in either <code>~/.cluster-api/clusterctl.yaml</code> or as environment variables. <pre><code>NUTANIX_ENDPOINT: \"\"    # IP or FQDN of Prism Central\nNUTANIX_USER: \"\"        # Prism Central user\nNUTANIX_PASSWORD: \"\"    # Prism Central password\nNUTANIX_INSECURE: false # or true\n\nKUBERNETES_VERSION: \"v1.22.9\"\nWORKER_MACHINE_COUNT: 3\nNUTANIX_SSH_AUTHORIZED_KEY: \"\"\n\nNUTANIX_PRISM_ELEMENT_CLUSTER_NAME: \"\"\nNUTANIX_MACHINE_TEMPLATE_IMAGE_NAME: \"\"\nNUTANIX_SUBNET_NAME: \"\"\n</code></pre></p> <p>You can also see the required list of variables by running the following: <pre><code>clusterctl generate cluster mycluster -i nutanix --list-variables           \nRequired Variables:\n  - CONTROL_PLANE_ENDPOINT_IP\n  - KUBERNETES_VERSION\n  - NUTANIX_ENDPOINT\n  - NUTANIX_MACHINE_TEMPLATE_IMAGE_NAME\n  - NUTANIX_PASSWORD\n  - NUTANIX_PRISM_ELEMENT_CLUSTER_NAME\n  - NUTANIX_SSH_AUTHORIZED_KEY\n  - NUTANIX_SUBNET_NAME\n  - NUTANIX_USER\n\nOptional Variables:\n  - CONTROL_PLANE_ENDPOINT_PORT      (defaults to \"6443\")\n  - CONTROL_PLANE_MACHINE_COUNT      (defaults to 1)\n  - KUBEVIP_LB_ENABLE                (defaults to \"false\")\n  - KUBEVIP_SVC_ENABLE               (defaults to \"false\")\n  - NAMESPACE                        (defaults to current Namespace in the KubeConfig file)\n  - NUTANIX_INSECURE                 (defaults to \"false\")\n  - NUTANIX_MACHINE_BOOT_TYPE        (defaults to \"legacy\")\n  - NUTANIX_MACHINE_MEMORY_SIZE      (defaults to \"4Gi\")\n  - NUTANIX_MACHINE_VCPU_PER_SOCKET  (defaults to \"1\")\n  - NUTANIX_MACHINE_VCPU_SOCKET      (defaults to \"2\")\n  - NUTANIX_PORT                     (defaults to \"9440\")\n  - NUTANIX_SYSTEMDISK_SIZE          (defaults to \"40Gi\")\n  - WORKER_MACHINE_COUNT             (defaults to 0)\n</code></pre></p> <p>Note</p> <p>To prevent duplicate IP assignments, it is required to assign an IP-address to the <code>CONTROL_PLANE_ENDPOINT_IP</code> variable that is not part of the Nutanix IPAM or DHCP range assigned to the subnet of the CAPX cluster. </p> <p>Now you can instantiate Cluster API with the following: <pre><code>clusterctl init -i nutanix\n</code></pre></p>"},{"location":"capx/v1.1.x/getting_started/#deploy-a-workload-cluster-on-nutanix-cloud-infrastructure","title":"Deploy a workload cluster on Nutanix Cloud Infrastructure","text":"<p><pre><code>export TEST_CLUSTER_NAME=mytestcluster1\nexport TEST_NAMESPACE=mytestnamespace\nCONTROL_PLANE_ENDPOINT_IP=x.x.x.x clusterctl generate cluster ${TEST_CLUSTER_NAME} \\\n    -i nutanix \\\n    --target-namespace ${TEST_NAMESPACE}  \\\n    --kubernetes-version v1.22.9 \\\n    --control-plane-machine-count 1 \\\n    --worker-machine-count 3 &gt; ./cluster.yaml\nkubectl create ns ${TEST_NAMESPACE}\nkubectl apply -f ./cluster.yaml -n ${TEST_NAMESPACE}\n</code></pre> To customize the configuration of the default <code>cluster.yaml</code> file generated by CAPX, visit the  NutanixCluster and  NutanixMachineTemplate documentation.</p>"},{"location":"capx/v1.1.x/getting_started/#access-a-workload-cluster","title":"Access a workload cluster","text":"<p>To access resources on the cluster, you can get the kubeconfig with the following: <pre><code>clusterctl get kubeconfig ${TEST_CLUSTER_NAME} -n ${TEST_NAMESPACE} &gt; ${TEST_CLUSTER_NAME}.kubeconfig\nkubectl --kubeconfig ./${TEST_CLUSTER_NAME}.kubeconfig get nodes \n</code></pre></p>"},{"location":"capx/v1.1.x/getting_started/#install-cni-on-workload-a-cluster","title":"Install CNI on workload a cluster","text":"<p>You must deploy a Container Network Interface (CNI) based pod network add-on so that your pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed.</p> <p>Note</p> <p>Take care that your pod network must not overlap with any of the host networks. You are likely to see problems if there is any overlap. If you find a collision between your network plugin's preferred pod network and some of your host networks, you must choose a suitable alternative CIDR block to use instead. It can be configured inside the <code>cluster.yaml</code> generated by <code>clusterctl generate cluster</code> before applying it.</p> <p>Several external projects provide Kubernetes pod networks using CNI, some of which also support Network Policy.</p> <p>See a list of add-ons that implement the Kubernetes networking model. At time of writing, the most common are Calico and Cilium.</p> <p>Follow the specific install guide for your selected CNI and install only one pod network per cluster.</p> <p>Once a pod network has been installed, you can confirm that it is working by checking that the CoreDNS pod is running in the output of <code>kubectl get pods --all-namespaces</code>.</p>"},{"location":"capx/v1.1.x/getting_started/#kube-vip-settings","title":"Kube-vip settings","text":"<p>Kube-vip is a true load balancing solution for the Kubernetes control plane. It distributes API requests across control plane nodes. It also has the capability to provide load balancing for Kubernetes services.</p> <p>You can tweak kube-vip settings by using the following properties:</p> <ul> <li><code>KUBEVIP_LB_ENABLE</code></li> </ul> <p>This setting allows control plane load balancing using IPVS. See Control Plane Load-Balancing documentation for further information.</p> <ul> <li><code>KUBEVIP_SVC_ENABLE</code> </li> </ul> <p>This setting enables a service of type LoadBalancer. See Kubernetes Service Load Balancing documentation for further information.</p> <ul> <li><code>KUBEVIP_SVC_ELECTION</code></li> </ul> <p>This setting enables Load Balancing of Load Balancers. See Load Balancing Load Balancers for further information.</p>"},{"location":"capx/v1.1.x/getting_started/#delete-a-workload-cluster","title":"Delete a workload cluster","text":"<p>To remove a workload cluster from your management cluster, remove the cluster object and the provider will clean-up all resources. </p> <pre><code>kubectl delete cluster ${TEST_CLUSTER_NAME} -n ${TEST_NAMESPACE}\n</code></pre> <p>Note</p> <p>Deleting the entire cluster template with <code>kubectl delete -f ./cluster.yaml</code> may lead to pending resources requiring manual cleanup.</p>"},{"location":"capx/v1.1.x/pc_certificates/","title":"Certificate Trust","text":"<p>CAPX invokes Prism Central APIs using the HTTPS protocol. CAPX has different methods to handle the trust of the Prism Central certificates:</p> <ul> <li>Enable certificate verification (default)</li> <li>Configure an additional trust bundle</li> <li>Disable certificate verification</li> </ul> <p>See the respective sections below for more information.</p> <p>Note</p> <p>For more information about replacing Prism Central certificates, see the Nutanix AOS Security Guide.</p>"},{"location":"capx/v1.1.x/pc_certificates/#enable-certificate-verification-default","title":"Enable certificate verification (default)","text":"<p>By default CAPX will perform certificate verification when invoking Prism Central API calls. This requires Prism Central to be configured with a publicly trusted certificate authority.  No additional configuration is required in CAPX.</p>"},{"location":"capx/v1.1.x/pc_certificates/#configure-an-additional-trust-bundle","title":"Configure an additional trust bundle","text":"<p>CAPX allows users to configure an additional trust bundle. This will allow CAPX to verify certificates that are not issued by a publicy trusted certificate authority. </p> <p>To configure an additional trust bundle, the <code>NUTANIX_ADDITIONAL_TRUST_BUNDLE</code> environment variable needs to be set. The value of the <code>NUTANIX_ADDITIONAL_TRUST_BUNDLE</code> environment variable contains the trust bundle (PEM format) in base64 encoded format. See the Configuring the trust bundle environment variable section for more information.</p> <p>It is also possible to configure the additional trust bundle manually by creating a custom <code>cluster-template</code>. See the Configuring the additional trust bundle manually  section for more information</p> <p>The <code>NUTANIX_ADDITIONAL_TRUST_BUNDLE</code> environment variable can be set when initializing the CAPX provider or when creating a workload cluster. If the <code>NUTANIX_ADDITIONAL_TRUST_BUNDLE</code> is configured when the CAPX provider is initialized, the additional trust bundle will be used for every CAPX workload cluster. If it is only configured when creating a workload cluster, it will only be applicable for that specific workload cluster.</p>"},{"location":"capx/v1.1.x/pc_certificates/#configuring-the-trust-bundle-environment-variable","title":"Configuring the trust bundle environment variable","text":"<p>Create a PEM encoded file containing the root certificate and all intermediate certificates. Example: <pre><code>$ cat cert.crt\n-----BEGIN CERTIFICATE-----\n&lt;certificate string&gt;\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n&lt;certificate string&gt;\n-----END CERTIFICATE-----\n</code></pre></p> <p>Use a <code>base64</code> tool to encode these contents in base64. The command below will provide a <code>base64</code> string. <pre><code>$ cat cert.crt | base64\n&lt;base64 string&gt;\n</code></pre></p> <p>Note</p> <p>Make sure the <code>base64</code> string does not contain any newlines (<code>\\n</code>). If the output string contains newlines, remove them manually or check the manual of the <code>base64</code> tool on how to generate a <code>base64</code> string without newlines. </p> <p>Use the <code>base64</code> string as value for the <code>NUTANIX_ADDITIONAL_TRUST_BUNDLE</code> environment variable. <pre><code>$ export NUTANIX_ADDITIONAL_TRUST_BUNDLE=\"&lt;base64 string&gt;\"\n</code></pre></p>"},{"location":"capx/v1.1.x/pc_certificates/#configuring-the-additional-trust-bundle-manually","title":"Configuring the additional trust bundle manually","text":"<p>To configure the additional trust bundle manually without using the <code>NUTANIX_ADDITIONAL_TRUST_BUNDLE</code> environment variable present in the default <code>cluster-template</code> files, it is required to:</p> <ul> <li>Create a <code>ConfigMap</code> containing the additional trust bundle.</li> <li>Configure the <code>prismCentral.additionalTrustBundle</code> object in the <code>NutanixCluster</code> spec.</li> </ul>"},{"location":"capx/v1.1.x/pc_certificates/#creating-the-additional-trust-bundle-configmap","title":"Creating the additional trust bundle ConfigMap","text":"<p>CAPX supports two different formats for the ConfigMap containing the additional trust bundle. The first one is to add the additional trust bundle as a multi-line string in the <code>ConfigMap</code>, the second option is to add the trust bundle in <code>base64</code> encoded format. See the examples below.</p> <p>Multi-line string example: <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: user-ca-bundle\nnamespace: ${NAMESPACE}\ndata:\nca.crt: |\n-----BEGIN CERTIFICATE-----\n&lt;certificate string&gt;\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n&lt;certificate string&gt;\n-----END CERTIFICATE-----\n</code></pre></p> <p><code>base64</code> example:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: user-ca-bundle\nnamespace: ${NAMESPACE}\nbinaryData:\nca.crt: &lt;base64 string&gt;\n</code></pre> <p>Note</p> <p>The <code>base64</code> string needs to be added as <code>binaryData</code>.</p>"},{"location":"capx/v1.1.x/pc_certificates/#configuring-the-nutanixcluster-spec","title":"Configuring the NutanixCluster spec","text":"<p>When the additional trust bundle <code>ConfigMap</code> is created, it needs to be referenced in the <code>NutanixCluster</code> spec. Add the <code>prismCentral.additionalTrustBundle</code> object in the <code>NutanixCluster</code> spec as shown below. Make sure the correct additional trust bundle <code>ConfigMap</code> is referenced.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixCluster\nmetadata:\nname: ${CLUSTER_NAME}\nnamespace: ${NAMESPACE}\nspec:\n...\nprismCentral:\n...\nadditionalTrustBundle:\nkind: ConfigMap\nname: user-ca-bundle\ninsecure: false\n</code></pre> <p>Note</p> <p>the default value of <code>prismCentral.insecure</code> attribute is <code>false</code>. It can be omitted when an additional trust bundle is configured. </p> <p>If <code>prismCentral.insecure</code> attribute is set to <code>true</code>, all certificate verification will be disabled. </p>"},{"location":"capx/v1.1.x/pc_certificates/#disable-certificate-verification","title":"Disable certificate verification","text":"<p>Note</p> <p>Disabling certificate verification is not recommended for production purposes and should only be used for testing.</p> <p>Certificate verification can be disabled by setting the <code>prismCentral.insecure</code> attribute to <code>true</code> in the <code>NutanixCluster</code> spec. Certificate verification will be disabled even if an additional trust bundle is configured. </p> <p>Disabled certificate verification example:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixCluster\nmetadata:\nname: ${CLUSTER_NAME}\nnamespace: ${NAMESPACE}\nspec:\ncontrolPlaneEndpoint:\nhost: ${CONTROL_PLANE_ENDPOINT_IP}\nport: ${CONTROL_PLANE_ENDPOINT_PORT=6443}\nprismCentral:\n...\ninsecure: true\n...\n</code></pre>"},{"location":"capx/v1.1.x/port_requirements/","title":"Port Requirements","text":"<p>CAPX uses the ports documented below to create workload clusters. </p> <p>Note<p>This page only documents the ports specifically required by CAPX and does not provide the full overview of all ports required in the CAPI framework.</p> </p>"},{"location":"capx/v1.1.x/port_requirements/#management-cluster","title":"Management cluster","text":"Source Destination Protocol Port Description Management cluster External Registries TCP 443 Pull container images from CAPX public registries Management cluster Prism Central TCP 9440 Management cluster communication to Prism Central"},{"location":"capx/v1.1.x/port_requirements/#public-registries-utilized-when-using-capx","title":"Public registries utilized when using CAPX","text":"Registry name ghcr.io"},{"location":"capx/v1.1.x/troubleshooting/","title":"Troubleshooting","text":""},{"location":"capx/v1.1.x/troubleshooting/#clusterctl-failed-with-github-rate-limit-error","title":"Clusterctl failed with GitHub rate limit error","text":"<p>By design Clusterctl fetches artifacts from repositories hosted on GitHub, this operation is subject to GitHub API rate limits.</p> <p>While this is generally okay for the majority of users, there is still a chance that some users (especially developers or CI tools) hit this limit:</p> <pre><code>Error: failed to get repository client for the XXX with name YYY: error creating the GitHub repository client: failed to get GitHub latest version: failed to get the list of versions: rate limit for github api has been reached. Please wait one hour or get a personal API tokens a assign it to the GITHUB_TOKEN environment variable\n</code></pre> <p>As explained in the error message, you can increase your API rate limit by creating a GitHub personal token and setting a <code>GITHUB_TOKEN</code> environment variable using the token.</p>"},{"location":"capx/v1.1.x/validated_integrations/","title":"Validated Integrations","text":"<p>Validated integrations are a defined set of specifically tested configurations between technologies that represent the most common combinations that Nutanix customers are using or deploying with CAPX. For these integrations, Nutanix has directly, or through certified partners, exercised a full range of platform tests as part of the product release process.</p>"},{"location":"capx/v1.1.x/validated_integrations/#integration-validation-policy","title":"Integration Validation Policy","text":"<p>Nutanix follows the version validation policies below:</p> <ul> <li> <p>Validate at least one active AOS LTS (long term support) version. Validated AOS LTS version for a specific CAPX version is listed in the AOS section.</p> <p>Note</p> <p>Typically the latest LTS release at time of CAPX release except when latest is initial release in train (eg x.y.0). Exact version depends on timing and customer adoption.</p> </li> <li> <p>Validate the latest AOS STS (short term support) release at time of CAPX release.</p> </li> <li> <p>Validate at least one active Prism Central (PC) version. Validated PC version for a specific CAPX version is listed in the Prism Central section.</p> <p>Note</p> <p>Typically the the latest PC release at time of CAPX release except when latest is initial release in train (eg x.y.0). Exact version depends on timing and customer adoption.</p> </li> <li> <p>At least one active Cluster-API (CAPI) version. Validated CAPI version for a specific CAPX version is listed in the Cluster-API section.</p> <p>Note</p> <p>Typically the the latest Cluster-API release at time of CAPX release except when latest is initial release in train (eg x.y.0). Exact version depends on timing and customer adoption.</p> </li> </ul>"},{"location":"capx/v1.1.x/validated_integrations/#validated-versions","title":"Validated versions","text":""},{"location":"capx/v1.1.x/validated_integrations/#cluster-api","title":"Cluster-API","text":"CAPX CAPI v1.1.4+ CAPI v1.2.x v1.0.x Yes Yes v0.5.x Yes Yes <p>See the Validated Kubernetes Versions page for more information on CAPI validated versions.</p>"},{"location":"capx/v1.1.x/validated_integrations/#aos","title":"AOS","text":"CAPX 5.20.4.5 (LTS) 6.1.1.5 (STS) v1.0.x Yes Yes v0.5.x Yes Yes"},{"location":"capx/v1.1.x/validated_integrations/#prism-central","title":"Prism Central","text":"CAPX 2022.1.0.2 pc.2022.6 v1.0.x Yes Yes v0.5.x Yes Yes"},{"location":"capx/v1.1.x/addons/install_csi_driver/","title":"Nutanix CSI Driver installation with CAPX","text":"<p>The Nutanix CSI driver is fully supported on CAPI/CAPX deployed clusters where all the nodes meet the Nutanix CSI driver prerequisites.</p> <p>There are three methods to install the Nutanix CSI driver on a CAPI/CAPX cluster:</p> <ul> <li>Helm</li> <li>ClusterResourceSet</li> <li>CAPX Flavor</li> </ul> <p>For more information, check the next sections.</p>"},{"location":"capx/v1.1.x/addons/install_csi_driver/#capi-workload-cluster-prerequisites-for-the-nutanix-csi-driver","title":"CAPI Workload cluster prerequisites for the Nutanix CSI Driver","text":"<p>Kubernetes workers need the following prerequisites to use the Nutanix CSI Drivers:</p> <ul> <li>iSCSI initiator package (for Volumes based block storage)</li> <li>NFS client package (for Files based storage)</li> </ul> <p>These packages may already be present in the image you use with your infrastructure provider or you can also rely on your bootstrap provider to install them. More info is available in the Prerequisites docs.</p> <p>The package names and installation method will also vary depending on the operating system you plan to use.</p> <p>In the example below, <code>kubeadm</code> bootstrap provider is used to deploy these packages on top of an Ubuntu 20.04 image. The <code>kubeadm</code> bootstrap provider allows defining <code>preKubeadmCommands</code> that will be launched before Kubernetes cluster creation. These <code>preKubeadmCommands</code> can be defined both in <code>KubeadmControlPlane</code> for master nodes and in <code>KubeadmConfigTemplate</code> for worker nodes.</p> <p>In the example with an Ubuntu 20.04 image, both <code>KubeadmControlPlane</code> and <code>KubeadmConfigTemplate</code> must be modified as in the example below:</p> <pre><code>spec:\ntemplate:\nspec:\n# .......\npreKubeadmCommands:\n- echo \"before kubeadm call\" &gt; /var/log/prekubeadm.log\n- apt update\n- apt install -y nfs-common open-iscsi\n- systemctl enable --now iscsid\n</code></pre>"},{"location":"capx/v1.1.x/addons/install_csi_driver/#install-the-nutanix-csi-driver-with-helm","title":"Install the Nutanix CSI Driver with Helm","text":"<p>A recent Helm version is needed (tested with Helm v3.10.1).</p> <p>The example below must be applied on a ready workload cluster. The workload cluster's kubeconfig can be retrieved and used to connect with the following command:</p> <pre><code>clusterctl get kubeconfig $CLUSTER_NAME -n $CLUSTER_NAMESPACE &gt; $CLUSTER_NAME-KUBECONFIG\nexport KUBECONFIG=$(pwd)/$CLUSTER_NAME-KUBECONFIG\n</code></pre> <p>Once connected to the cluster, follow the CSI documentation. </p> <p>First, install the nutanix-csi-snapshot chart followed by the nutanix-csi-storage chart.</p> <p>See an example below:</p> <pre><code>#Add the official Nutanix Helm repo and get the latest update\nhelm repo add nutanix https://nutanix.github.io/helm/\nhelm repo update\n\n# Install the nutanix-csi-snapshot chart\nhelm install nutanix-csi-snapshot nutanix/nutanix-csi-snapshot -n ntnx-system --create-namespace\n\n# Install the nutanix-csi-storage chart\nhelm install nutanix-storage nutanix/nutanix-csi-storage -n ntnx-system --set createSecret=false\n</code></pre> <p>Warning</p> <p>For correct Nutanix CSI driver deployment, a fully functional CNI deployment must be present.</p>"},{"location":"capx/v1.1.x/addons/install_csi_driver/#install-the-nutanix-csi-driver-with-clusterresourceset","title":"Install the Nutanix CSI Driver with <code>ClusterResourceSet</code>","text":"<p>The <code>ClusterResourceSet</code> feature was introduced to automatically apply a set of resources (such as CNI/CSI) defined by administrators to matching created/existing workload clusters.</p>"},{"location":"capx/v1.1.x/addons/install_csi_driver/#enabling-the-clusterresourceset-feature","title":"Enabling the <code>ClusterResourceSet</code> feature","text":"<p>At the time of writing, <code>ClusterResourceSet</code> is an experimental feature that must be enabled during the initialization of a management cluster with the <code>EXP_CLUSTER_RESOURCE_SET</code> feature gate.</p> <p>To do this, add <code>EXP_CLUSTER_RESOURCE_SET: \"true\"</code> in the <code>clusterctl</code> configuration file or just <code>export EXP_CLUSTER_RESOURCE_SET=true</code> before initializing the management cluster with <code>clusterctl init</code>.</p> <p>If the management cluster is already initialized, the <code>ClusterResourceSet</code> can be enabled by changing the configuration of the <code>capi-controller-manager</code> deployment in the <code>capi-system</code> namespace.</p> <pre><code>kubectl edit deployment -n capi-system capi-controller-manager\n</code></pre> <p>Locate the section below:</p> <pre><code>  - args:\n- --leader-elect\n- --metrics-bind-addr=localhost:8080\n- --feature-gates=MachinePool=false,ClusterResourceSet=true,ClusterTopology=false\n</code></pre> <p>Then replace <code>ClusterResourceSet=false</code> with <code>ClusterResourceSet=true</code>.</p> <p>Note</p> <p>Editing the <code>deployment</code> resource will cause Kubernetes to automatically start new versions of the containers with the feature enabled.</p>"},{"location":"capx/v1.1.x/addons/install_csi_driver/#prepare-the-nutanix-csi-clusterresourceset","title":"Prepare the Nutanix CSI <code>ClusterResourceSet</code>","text":""},{"location":"capx/v1.1.x/addons/install_csi_driver/#create-the-configmap-for-the-cni-plugin","title":"Create the <code>ConfigMap</code> for the CNI Plugin","text":"<p>First, create a <code>ConfigMap</code> that contains a YAML manifest with all resources to install the Nutanix CSI driver.</p> <p>Since the Nutanix CSI Driver is provided as a Helm chart, use <code>helm</code> to extract it before creating the <code>ConfigMap</code>. See an example below:</p> <pre><code>helm repo add nutanix https://nutanix.github.io/helm/\nhelm repo update\n\nkubectl create ns ntnx-system --dry-run=client -o yaml &gt; nutanix-csi-namespace.yaml\nhelm template nutanix-csi-snapshot nutanix/nutanix-csi-snapshot -n ntnx-system &gt; nutanix-csi-snapshot.yaml\nhelm template nutanix-csi-snapshot nutanix/nutanix-csi-storage -n ntnx-system &gt; nutanix-csi-storage.yaml\n\nkubectl create configmap nutanix-csi-crs --from-file=nutanix-csi-namespace.yaml --from-file=nutanix-csi-snapshot.yaml --from-file=nutanix-csi-storage.yaml\n</code></pre>"},{"location":"capx/v1.1.x/addons/install_csi_driver/#create-the-clusterresourceset","title":"Create the <code>ClusterResourceSet</code>","text":"<p>Next, create the <code>ClusterResourceSet</code> resource that will map the <code>ConfigMap</code> defined above to clusters using a <code>clusterSelector</code>.</p> <p>The <code>ClusterResourceSet</code> needs to be created inside the management cluster. See an example below:</p> <pre><code>---\napiVersion: addons.cluster.x-k8s.io/v1alpha3\nkind: ClusterResourceSet\nmetadata:\nname: nutanix-csi-crs\nspec:\nclusterSelector:\nmatchLabels:\ncsi: nutanix resources:\n- kind: ConfigMap\nname: nutanix-csi-crs\n</code></pre> <p>The <code>clusterSelector</code> field controls how Cluster API will match this <code>ClusterResourceSet</code> on one or more workload clusters. In the example scenario, the <code>matchLabels</code> approach is being used where the <code>ClusterResourceSet</code> will be applied to all workload clusters having the <code>csi: nutanix</code> label present. If the label isn't present, the <code>ClusterResourceSet</code> won't apply to that workload cluster.</p> <p>The <code>resources</code> field references the <code>ConfigMap</code> created above, which contains the manifests for installing the Nutanix CSI driver.</p>"},{"location":"capx/v1.1.x/addons/install_csi_driver/#assign-the-clusterresourceset-to-a-workload-cluster","title":"Assign the <code>ClusterResourceSet</code> to a workload cluster","text":"<p>Assign this <code>ClusterResourceSet</code> to the workload cluster by adding the correct label to the <code>Cluster</code> resource.</p> <p>This can be done before workload cluster creation by editing the output of the <code>clusterctl generate cluster</code> command or by modifying an already deployed workload cluster.</p> <p>In both cases, <code>Cluster</code> resources should look like this:</p> <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\nmetadata:\nname: workload-cluster-name\nnamespace: workload-cluster-namespace\nlabels:\ncsi: nutanix\n# ...\n</code></pre> <p>Warning</p> <p>For correct Nutanix CSI driver deployment, a fully functional CNI deployment must be present.</p>"},{"location":"capx/v1.1.x/addons/install_csi_driver/#install-the-nutanix-csi-driver-with-a-capx-flavor","title":"Install the Nutanix CSI Driver with a CAPX flavor","text":"<p>The CAPX provider can utilize a flavor to automatically deploy the Nutanix CSI using a <code>ClusterResourceSet</code>.</p>"},{"location":"capx/v1.1.x/addons/install_csi_driver/#prerequisites","title":"Prerequisites","text":"<p>The following requirements must be met:</p> <ul> <li>The operating system must meet the Nutanix CSI OS prerequisites.</li> <li>The Management cluster must be installed with the <code>CLUSTER_RESOURCE_SET</code> feature gate.</li> </ul>"},{"location":"capx/v1.1.x/addons/install_csi_driver/#installation","title":"Installation","text":"<p>Specify the <code>csi</code> flavor during workload cluster creation. See an example below:</p> <pre><code>clusterctl generate cluster my-cluster -f csi\n</code></pre> <p>Additional environment variables are required:</p> <ul> <li><code>WEBHOOK_CA</code>: Base64 encoded CA certificate used to sign the webhook certificate</li> <li><code>WEBHOOK_CERT</code>: Base64 certificate for the webhook validation component</li> <li><code>WEBHOOK_KEY</code>: Base64 key for the webhook validation component</li> </ul> <p>The three components referenced above can be automatically created and referenced using this script:</p> <pre><code>source scripts/gen-self-cert.sh\n</code></pre> <p>The certificate must reference the following names:</p> <ul> <li>csi-snapshot-webhook</li> <li>csi-snapshot-webhook.ntnx-sytem</li> <li>csi-snapshot-webhook.ntnx-sytem.svc</li> </ul> <p>Warning</p> <p>For correct Nutanix CSI driver deployment, a fully functional CNI deployment must be present.</p>"},{"location":"capx/v1.1.x/addons/install_csi_driver/#nutanix-csi-driver-configuration","title":"Nutanix CSI Driver Configuration","text":"<p>After the driver is installed, it must be configured for use by minimally defining a <code>Secret</code> and <code>StorageClass</code>.</p> <p>This can be done manually in the workload clusters or by using a <code>ClusterResourceSet</code> in the management cluster as explained above.</p> <p>See the Official CSI Driver documentation on the Nutanix Portal for more configuration information. </p>"},{"location":"capx/v1.1.x/experimental/autoscaler/","title":"Using Autoscaler in combination with CAPX","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>Autoscaler can be used in combination with Cluster API to automatically add or remove machines in a cluster. </p> <p>Autoscaler can be used in different deployment scenarios. This page will provide an overview of multiple autoscaler deployment scenarios in combination with CAPX. See the Testing section to see how scale-up/scale-down events can be triggered to validate the autoscaler behaviour.</p> <p>More in-depth information on Autoscaler functionality can be found in the Kubernetes documentation.</p> <p>All Autoscaler configuration parameters can be found here.</p>"},{"location":"capx/v1.1.x/experimental/autoscaler/#scenario-1-management-cluster-managing-an-external-workload-cluster","title":"Scenario 1: Management cluster managing an external workload cluster","text":"<p>In this scenario, Autoscaler will be running on a management cluster and it will manage an external workload cluster. See the management cluster managing an external workload cluster section of Kubernetes documentation for more information.</p>"},{"location":"capx/v1.1.x/experimental/autoscaler/#steps","title":"Steps","text":"<ol> <li> <p>Deploy a management cluster and workload cluster. The CAPI quickstart can be used as a starting point.</p> <p>Note<p>Make sure a CNI is installed in the workload cluster.</p> </p> </li> <li> <p>Download the example Autoscaler deployment file.</p> </li> <li>Modify the <code>deployment.yaml</code> file:<ul> <li>Change the namespace of all resources to the namespaces of the workload cluster.</li> <li>Choose an autoscale image.</li> <li>Change the following parameters in the <code>Deployment</code> resource: <pre><code>        spec:\ncontainers:\nname: cluster-autoscaler\ncommand:\n- /cluster-autoscaler\nargs:\n- --cloud-provider=clusterapi\n- --kubeconfig=/mnt/kubeconfig/kubeconfig.yml\n- --clusterapi-cloud-config-authoritative\n- -v=1\nvolumeMounts:\n- mountPath: /mnt/kubeconfig\nname: kubeconfig\nreadOnly: true\n...\nvolumes:\n- name: kubeconfig\nsecret:\nsecretName: &lt;workload cluster name&gt;-kubeconfig\nitems:\n- key: value\npath: kubeconfig.yml\n</code></pre></li> </ul> </li> <li>Apply the <code>deployment.yaml</code> file. <pre><code>kubectl apply -f deployment.yaml\n</code></pre></li> <li>Add the annotations to the workload cluster <code>MachineDeployment</code> resource.</li> <li>Test Autoscaler. Go to the Testing section.</li> </ol>"},{"location":"capx/v1.1.x/experimental/autoscaler/#scenario-2-autoscaler-running-on-workload-cluster","title":"Scenario 2: Autoscaler running on workload cluster","text":"<p>In this scenario, Autoscaler will be deployed on top of the workload cluster directly. In order for Autoscaler to work, it is required that the workload cluster resources are moved from the management cluster to the workload cluster.</p>"},{"location":"capx/v1.1.x/experimental/autoscaler/#steps_1","title":"Steps","text":"<ol> <li>Deploy a management cluster and workload cluster. The CAPI quickstart can be used as a starting point.</li> <li>Get the kubeconfig file for the workload cluster and use this kubeconfig to login to the workload cluster.  <pre><code>clusterctl get kubeconfig &lt;workload cluster name&gt; -n &lt;workload cluster namespace &gt; /path/to/kubeconfig\n</code></pre></li> <li>Install a CNI in the workload cluster.</li> <li>Initialise the CAPX components on top of the workload cluster: <pre><code>clusterctl init --infrastructure nutanix\n</code></pre></li> <li>Migrate the workload cluster custom resources to the workload cluster. Run following command from the management cluster: <pre><code>clusterctl move -n &lt;workload cluster ns&gt;  --to-kubeconfig /path/to/kubeconfig\n</code></pre></li> <li>Verify if the cluster has been migrated by running following command on the workload cluster: <pre><code>kubectl get cluster -A </code></pre></li> <li>Download the example autoscaler deployment file.</li> <li>Create the Autoscaler namespace: <pre><code>kubectl create ns autoscaler\n</code></pre></li> <li>Apply the <code>deployment.yaml</code> file <pre><code>kubectl apply -f deployment.yaml\n</code></pre></li> <li>Add the annotations to the workload cluster <code>MachineDeployment</code> resource.</li> <li>Test Autoscaler. Go to the Testing section.</li> </ol>"},{"location":"capx/v1.1.x/experimental/autoscaler/#testing","title":"Testing","text":"<ol> <li>Deploy an example Kubernetes application. For example, the one used in the Kubernetes HorizontalPodAutoscaler Walkthrough. <pre><code>kubectl apply -f https://k8s.io/examples/application/php-apache.yaml </code></pre></li> <li>Increase the amount of replicas of the application to trigger a scale-up event: <pre><code>kubectl scale deployment php-apache --replicas 100\n</code></pre></li> <li> <p>Decrease the amount of replicas of the application again to trigger a scale-down event.</p> <p>Note</p> <p>In case of issues check the logs of the Autoscaler pods.</p> </li> <li> <p>After a while CAPX, will add more machines. Refer to the Autoscaler configuration parameters to tweak the behaviour and timeouts.</p> </li> </ol>"},{"location":"capx/v1.1.x/experimental/autoscaler/#autoscaler-node-group-annotations","title":"Autoscaler node group annotations","text":"<p>Autoscaler uses following annotations to define the upper and lower boundries of the managed machines:</p> Annotation Example Value Description cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size 5 Maximum amount of machines in this node group cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size 1 Minimum amount of machines in this node group <p>These annotations must be applied to the <code>MachineDeployment</code> resources of a CAPX cluster. </p>"},{"location":"capx/v1.1.x/experimental/autoscaler/#example","title":"Example","text":"<pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: MachineDeployment\nmetadata:\nannotations:\ncluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \"5\"\ncluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \"1\"\n</code></pre>"},{"location":"capx/v1.1.x/experimental/capx_multi_pe/","title":"Creating a workload CAPX cluster spanning Prism Element clusters","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>This page will explain how to deploy CAPX-based Kubernetes clusters where worker nodes are spanning multiple Prism Element (PE) clusters. </p> <p>Note<p>All the PE clusters must be managed by the same Prism Central (PC) instance.</p> </p> <p>The topology will look like this: </p> <ul> <li>One PC managing multiple PE's</li> <li>One CAPI management cluster</li> <li>One CAPI workload cluster with multiple <code>MachineDeployment</code>resources</li> </ul> <p>Refer to the CAPI quickstart to get started with CAPX. </p> <p>To create workload clusters spanning multiple Prism Element clusters, it is required to create a <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resource for each Prism Element cluster. The Prism Element specific parameters (name/UUID, subnet,...) are referenced in the <code>NutanixMachineTemplate</code>. </p>"},{"location":"capx/v1.1.x/experimental/capx_multi_pe/#steps","title":"Steps","text":"<ol> <li>Create a management cluster that has the CAPX infrastructure provider deployed.</li> <li>Create a <code>cluster.yml</code> file containing the workload cluster definition. Refer to the steps defined in the CAPI quickstart guide to create an example <code>cluster.yml</code> file.</li> <li> <p>Add additional <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resources.</p> <p>By default there is only one machine template and machine deployment defined. To add nodes residing on another Prism Element cluster, a new <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resource needs to be added to the yaml file. The autogenerated <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> resource definitions can be used as a baseline.</p> <p>Make sure to modify the <code>MachineDeployment</code> and <code>NutanixMachineTemplate</code> parameters.</p> </li> <li> <p>Apply the modified <code>cluster.yml</code> file to the management cluster.</p> </li> </ol>"},{"location":"capx/v1.1.x/experimental/oidc/","title":"OIDC integration","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>Kubernetes allows users to authenticate using various authentication mechanisms. One of these mechanisms is OIDC. Information on how Kubernetes interacts with OIDC providers can be found in the OpenID Connect Tokens section of the official Kubernetes documentation. </p> <p>Follow the steps below to configure a CAPX cluster to use an OIDC identity provider.</p>"},{"location":"capx/v1.1.x/experimental/oidc/#steps","title":"Steps","text":"<ol> <li>Generate a <code>cluster.yaml</code> file with the required CAPX cluster configuration. Refer to the Getting Started page for more information on how to generate a <code>cluster.yaml</code> file. Do not apply the <code>cluster.yaml</code> file. </li> <li>Edit the <code>cluster.yaml</code> file and search for the <code>KubeadmControlPlane</code> resource.</li> <li>Modify/add the <code>spec.kubeadmConfigSpec.clusterConfiguration.apiServer.extraArgs</code> attribute and add the required API server parameters. See the example below.</li> <li>Apply the <code>cluster.yaml</code> file </li> <li>Log in with the OIDC provider once the cluster is provisioned</li> </ol>"},{"location":"capx/v1.1.x/experimental/oidc/#example","title":"Example","text":"<pre><code>kind: KubeadmControlPlane\nspec:\nkubeadmConfigSpec:\nclusterConfiguration:\napiServer:\nextraArgs:\n...\noidc-client-id: &lt;oidc-client-id&gt;\noidc-issuer-url: &lt;oidc-issuer-url&gt;\n...\n</code></pre>"},{"location":"capx/v1.1.x/experimental/proxy/","title":"Proxy configuration","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>CAPX can be configured to use a proxy to connect to external networks. This proxy configuration needs to be applied to control plane and worker nodes. </p> <p>Follow the steps below to configure a CAPX cluster to use a proxy.</p>"},{"location":"capx/v1.1.x/experimental/proxy/#steps","title":"Steps","text":"<ol> <li>Generate a <code>cluster.yaml</code> file with the required CAPX cluster configuration. Refer to the Getting Started page for more information on how to generate a <code>cluster.yaml</code> file. Do not apply the <code>cluster.yaml</code> file. </li> <li>Edit the <code>cluster.yaml</code> file and modify the following resources as shown in the example below to add the proxy configuration.<ol> <li><code>KubeadmControlPlane</code>: <ul> <li>Add the proxy configuration to the <code>spec.kubeadmConfigSpec.files</code> list. Do not modify other items in the list.</li> <li>Add <code>systemctl</code> commands to apply the proxy config in <code>spec.kubeadmConfigSpec.preKubeadmCommands</code>. Do not modify other items in the list.</li> </ul> </li> <li><code>KubeadmConfigTemplate</code>: <ul> <li>Add the proxy configuration to the <code>spec.template.spec.files</code> list. Do not modify other items in the list.</li> <li>Add <code>systemctl</code> commands to apply the proxy config in <code>spec.template.spec.preKubeadmCommands</code>. Do not modify other items in the list.</li> </ul> </li> </ol> </li> <li>Apply the <code>cluster.yaml</code> file </li> </ol>"},{"location":"capx/v1.1.x/experimental/proxy/#example","title":"Example","text":"<pre><code>---\n# controlplane proxy settings\nkind: KubeadmControlPlane\nspec:\nkubeadmConfigSpec:\nfiles:\n- content: |\n[Service]\nEnvironment=\"HTTP_PROXY=&lt;my-http-proxy-configuration&gt;\"\nEnvironment=\"HTTPS_PROXY=&lt;my-https-proxy-configuration&gt;\"\nEnvironment=\"NO_PROXY=&lt;my-no-proxy-configuration&gt;\"\nowner: root:root\npath: /etc/systemd/system/containerd.service.d/http-proxy.conf\n...\npreKubeadmCommands:\n- sudo systemctl daemon-reload\n- sudo systemctl restart containerd\n...\n---\n# worker proxy settings\nkind: KubeadmConfigTemplate\nspec:\ntemplate:\nspec:\nfiles:\n- content: |\n[Service]\nEnvironment=\"HTTP_PROXY=&lt;my-http-proxy-configuration&gt;\"\nEnvironment=\"HTTPS_PROXY=&lt;my-https-proxy-configuration&gt;\"\nEnvironment=\"NO_PROXY=&lt;my-no-proxy-configuration&gt;\"\nowner: root:root\npath: /etc/systemd/system/containerd.service.d/http-proxy.conf\n...\npreKubeadmCommands:\n- sudo systemctl daemon-reload\n- sudo systemctl restart containerd\n...\n</code></pre>"},{"location":"capx/v1.1.x/experimental/vpc/","title":"Creating a workload CAPX cluster in a Nutanix Flow VPC","text":"<p>Warning<p>The scenario and features described on this page are experimental and should not be deployed in production environments.</p> </p> <p>Note</p> <p>Nutanix Flow VPCs are only validated with CAPX 1.1.3+</p> <p>Nutanix Flow Virtual Networking allows users to create Virtual Private Clouds (VPCs) with Overlay networking.  The steps below will illustrate how a CAPX cluster can be deployed inside an overlay subnet (NAT) inside a VPC while the management cluster resides outside of the VPC.</p>"},{"location":"capx/v1.1.x/experimental/vpc/#steps","title":"Steps","text":"<ol> <li>Request a floating IP</li> <li>Link the floating IP to an internal IP address inside the overlay subnet that will be used to deploy the CAPX cluster. This address will be assigned to the CAPX loadbalancer. To prevent IP conflicts, make sure the IP address is not part of the IP-pool defined in the subnet. </li> <li>Generate a <code>cluster.yaml</code> file with the required CAPX cluster configuration where the <code>CONTROL_PLANE_ENDPOINT_IP</code> is set to the floating IP requested in the first step. Refer to the Getting Started page for more information on how to generate a <code>cluster.yaml</code> file. Do not apply the <code>cluster.yaml</code> file. </li> <li>Edit the <code>cluster.yaml</code> file and search for the <code>KubeadmControlPlane</code> resource.</li> <li>Modify the <code>spec.kubeadmConfigSpec.files.*.content</code> attribute and change the <code>kube-vip</code> definition similar to the example below.</li> <li>Apply the <code>cluster.yaml</code> file.</li> <li>When the CAPX workload cluster is deployed, it will be reachable via the floating IP.</li> </ol>"},{"location":"capx/v1.1.x/experimental/vpc/#example","title":"Example","text":"<pre><code>kind: KubeadmControlPlane\nspec:\nkubeadmConfigSpec:\nfiles:\n- content: |\napiVersion: v1\nkind: Pod\nmetadata:\nname: kube-vip\nnamespace: kube-system\nspec:\ncontainers:\n- env:\n- name: address\nvalue: \"&lt;internal overlay subnet address&gt;\"                  \n</code></pre>"},{"location":"capx/v1.1.x/tasks/modify_machine_configuration/","title":"Modifying Machine Configurations","text":"<p>Since all attributes of the <code>NutanixMachineTemplate</code> resources are immutable, follow the Updating Infrastructure Machine Templates procedure to modify the configuration of machines in an existing CAPX cluster. See the NutanixMachineTemplate documentation for all supported configuration parameters.</p> <p>Note</p> <p>Manually modifying existing and linked <code>NutanixMachineTemplate</code> resources will not trigger a rolling update of the machines. </p> <p>Note</p> <p>Do not modify the virtual machine configuration of CAPX cluster nodes manually in Prism/Prism Central.  CAPX will not automatically revert the configuration change but performing scale-up/scale-down/upgrade operations will override manual modifications. Only use the <code>Updating Infrastructure Machine</code> procedure referenced above to perform configuration changes.</p>"},{"location":"capx/v1.1.x/types/nutanix_cluster/","title":"NutanixCluster","text":"<p>The <code>NutanixCluster</code> resource defines the configuration of a CAPX Kubernetes cluster. </p> <p>Example of a <code>NutanixCluster</code> resource:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixCluster\nmetadata:\nname: ${CLUSTER_NAME}\nnamespace: ${NAMESPACE}\nspec:\ncontrolPlaneEndpoint:\nhost: ${CONTROL_PLANE_ENDPOINT_IP}\nport: ${CONTROL_PLANE_ENDPOINT_PORT=6443}\nprismCentral:\naddress: ${NUTANIX_ENDPOINT}\nadditionalTrustBundle:\nkind: ConfigMap\nname: user-ca-bundle\ncredentialRef:\nkind: Secret\nname: ${CLUSTER_NAME}\ninsecure: ${NUTANIX_INSECURE=false}\nport: ${NUTANIX_PORT=9440}\n</code></pre>"},{"location":"capx/v1.1.x/types/nutanix_cluster/#nutanixcluster-spec","title":"NutanixCluster spec","text":"<p>The table below provides an overview of the supported parameters of the <code>spec</code> attribute of a <code>NutanixCluster</code> resource.</p>"},{"location":"capx/v1.1.x/types/nutanix_cluster/#configuration-parameters","title":"Configuration parameters","text":"Key Type Description controlPlaneEndpoint object Defines the host IP and port of the CAPX Kubernetes cluster. controlPlaneEndpoint.host string Host IP to be assigned to the CAPX Kubernetes cluster. controlPlaneEndpoint.port int Port of the CAPX Kubernetes cluster. Default: <code>6443</code> prismCentral object (Optional) Prism Central endpoint definition. prismCentral.address string IP/FQDN of Prism Central. prismCentral.port int Port of Prism Central. Default: <code>9440</code> prismCentral.insecure bool Disable Prism Central certificate checking. Default: <code>false</code> prismCentral.credentialRef object Reference to credentials used for Prism Central connection. prismCentral.credentialRef.kind string Kind of the credentialRef. Allowed value: <code>Secret</code> prismCentral.credentialRef.name string Name of the secret containing the Prism Central credentials. prismCentral.credentialRef.namespace string (Optional) Namespace of the secret containing the Prism Central credentials. prismCentral.additionalTrustBundle object Reference to the certificate trust bundle used for Prism Central connection. prismCentral.additionalTrustBundle.kind string Kind of the additionalTrustBundle. Allowed value: <code>ConfigMap</code> prismCentral.additionalTrustBundle.name string Name of the <code>ConfigMap</code> containing the Prism Central trust bundle. prismCentral.additionalTrustBundle.namespace string (Optional) Namespace of the <code>ConfigMap</code> containing the Prism Central trust bundle. <p>Note</p> <p>To prevent duplicate IP assignments, it is required to assign an IP-address to the <code>controlPlaneEndpoint.host</code> variable that is not part of the Nutanix IPAM or DHCP range assigned to the subnet of the CAPX cluster.</p>"},{"location":"capx/v1.1.x/types/nutanix_machine_template/","title":"NutanixMachineTemplate","text":"<p>The <code>NutanixMachineTemplate</code> resource defines the configuration of a CAPX Kubernetes VM. </p> <p>Example of a <code>NutanixMachineTemplate</code> resource.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: NutanixMachineTemplate\nmetadata:\nname: \"${CLUSTER_NAME}-mt-0\"\nnamespace: \"${NAMESPACE}\"\nspec:\ntemplate:\nspec:\nproviderID: \"nutanix://${CLUSTER_NAME}-m1\"\n# Supported options for boot type: legacy and uefi\n# Defaults to legacy if not set\nbootType: ${NUTANIX_MACHINE_BOOT_TYPE=legacy}\nvcpusPerSocket: ${NUTANIX_MACHINE_VCPU_PER_SOCKET=1}\nvcpuSockets: ${NUTANIX_MACHINE_VCPU_SOCKET=2}\nmemorySize: \"${NUTANIX_MACHINE_MEMORY_SIZE=4Gi}\"\nsystemDiskSize: \"${NUTANIX_SYSTEMDISK_SIZE=40Gi}\"\nimage:\ntype: name\nname: \"${NUTANIX_MACHINE_TEMPLATE_IMAGE_NAME}\"\ncluster:\ntype: name\nname: \"${NUTANIX_PRISM_ELEMENT_CLUSTER_NAME}\"\nsubnet:\n- type: name\nname: \"${NUTANIX_SUBNET_NAME}\"\n# Adds additional categories to the virtual machines.\n# Note: Categories must already be present in Prism Central\n# additionalCategories:\n#   - key: AppType\n#     value: Kubernetes\n# Adds the cluster virtual machines to a project defined in Prism Central.\n# Replace NUTANIX_PROJECT_NAME with the correct project defined in Prism Central\n# Note: Project must already be present in Prism Central.\n# project:\n#   type: name\n#   name: \"NUTANIX_PROJECT_NAME\"\n</code></pre>"},{"location":"capx/v1.1.x/types/nutanix_machine_template/#nutanixmachinetemplate-spec","title":"NutanixMachineTemplate spec","text":"<p>The table below provides an overview of the supported parameters of the <code>spec</code> attribute of a <code>NutanixMachineTemplate</code> resource.</p>"},{"location":"capx/v1.1.x/types/nutanix_machine_template/#configuration-parameters","title":"Configuration parameters","text":"Key Type Description bootType string Boot type of the VM. Depends on the OS image used. Allowed values: <code>legacy</code>, <code>uefi</code>. Default: <code>legacy</code> vcpusPerSocket int Amount of vCPUs per socket. Default: <code>1</code> vcpuSockets int Amount of vCPU sockets. Default: <code>2</code> memorySize string Amount of Memory. Default: <code>4Gi</code> systemDiskSize string Amount of storage assigned to the system disk. Default: <code>40Gi</code> image object Reference to the OS image used for the system disk. image.type string Type to identify the OS image. Allowed values: <code>name</code> and <code>uuid</code> image.name string Name or UUID of the image. cluster object Reference to the Prism Element cluster. cluster.type string Type to identify the Prism Element cluster. Allowed values: <code>name</code> and <code>uuid</code> cluster.name string Name or UUID of the Prism Element cluster. subnets list Reference to the subnets to be assigned to the VMs. subnets.[].type string Type to identify the subnet. Allowed values: <code>name</code> and <code>uuid</code> subnets.[].name string Name or UUID of the subnet. additionalCategories list Reference to the categories to be assigned to the VMs. These categories already exist in Prism Central. additionalCategories.[].key string Key of the category. additionalCategories.[].value string Value of the category. project object Reference to the project. This project must already exist in Prism Central. project.type string Type to identify the project. Allowed values: <code>name</code> and <code>uuid</code> project.name string Name or UUID of the project."},{"location":"ccm/latest/ccm_configuration/","title":"Nutanix CCM Configuration","text":"<p>Nutanix CCM can be configured via a <code>JSON</code> formated file stored in a configmap called <code>nutanix-config</code>. This configmap is located in the same namespace as the Nutanix CCM deployment. See the <code>manifests/cloud-provider-nutanix-deployment.yaml</code> file for details on the Nutanix CCM deployment. </p> <p>Example <code>nutanix-config</code> configmap: <pre><code>---\nkind: ConfigMap\napiVersion: v1\nmetadata:\nname: nutanix-config\nnamespace: kube-system\ndata:\nnutanix_config.json: |-\n{\n\"prismCentral\": {\n\"address\": \"${NUTANIX_ENDPOINT}\",\n\"port\": ${NUTANIX_PORT},\n\"insecure\": ${NUTANIX_INSECURE},\n\"credentialRef\": {\n\"kind\": \"secret\",\n\"name\": \"nutanix-creds\"\n},\n\"additionalTrustBundle\": {\n\"kind\": \"ConfigMap\",\n\"name\": \"user-ca-bundle\"\n}\n},\n\"enableCustomLabeling\": false,\n\"topologyDiscovery\": {\n\"type\": \"Categories\",\n\"topologyCategories\": {\n\"regionCategory\": \"${NUTANIX_REGION_CATEGORY}\",\n\"zoneCategory\": \"${NUTANIX_ZONE_CATEGORY}\"\n}\n}\n}\n</code></pre></p> <p>The table below provides an overview of the supported configuration parameters.</p>"},{"location":"ccm/latest/ccm_configuration/#configuration-parameters","title":"Configuration parameters","text":"Key Type Description topologyDiscovery object (Optional) Configures the topology discovery mode.<code>Prism</code> topology discovery is used by default if <code>topologyDiscovery</code> attribute is not passed. topologyDiscovery.type string Topology Discovery mode. Can be <code>Prism</code> or <code>Categories</code>. See Topology Discovery for more information. topologyDiscovery.topologyCategories object Required if topology discovery mode is <code>Categories</code>. topologyDiscovery.topologyCategories.regionCategory string Category key defining the region of the Kubernetes node. topologyDiscovery.topologyCategories.zoneCategory string Category key defining the zone of the Kubernetes node. enableCustomLabeling bool Boolean value to enable custom labeling. See Custom Labeling for more information.Default: <code>false</code> prismCentral object Prism Central endpoint configuration. prismCentral.address string FQDN/IP of the Prism Central endpoint. prismCentral.port int Port to connect to Prism Central.Default: <code>9440</code> prismCentral.insecure bool Disable Prism Central certificate checking.Default: <code>false</code> prismCentral.credentialRef object Prism Central credential configuration. See Credentials for more information. prismCentral.credentialRef.kind string Credential kind.Allowed value: <code>secret</code> prismCentral.credentialRef.name string Name of the secret. prismCentral.credentialRef.namespace string (Optional) Namespace of the secret. prismCentral.additionalTrustBundle object Reference to the certificate trust bundle used for Prism Central connection. prismCentral.additionalTrustBundle.kind string Kind of the additionalTrustBundle. Allowed value: <code>ConfigMap</code> prismCentral.additionalTrustBundle.name string Name of the <code>ConfigMap</code> containing the Prism Central trust bundle. prismCentral.additionalTrustBundle.namespace string (Optional) Namespace of the <code>ConfigMap</code> containing the Prism Central trust bundle. See Certificate Trust for more information."},{"location":"ccm/latest/ccm_credentials/","title":"Credentials","text":"<p>Nutanix CCM requires credentials to connect to Prism Central. These credentials need to be stored in a secret in following format:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\nname: nutanix-creds\nnamespace: kube-system\nstringData:\ncredentials: |\n[\n{\n\"type\": \"basic_auth\", \n\"data\": { \n\"prismCentral\":{\n\"username\": \"$NUTANIX_USERNAME\", \n\"password\": \"$NUTANIX_PASSWORD\"\n},\n\"prismElements\": null\n}\n}\n]\n</code></pre> <p>See Requirements for more information on the required permissions.</p>"},{"location":"ccm/latest/custom_labeling/","title":"Custom Labeling","text":"<p>Enabling the Nutanix CCM custom labeling feature will add additional labels to the Kubernetes nodes. See Nutanix CCM Configuration for more information on how to configure CCM to enable custom labeling.</p> <p>The following labels will be added:</p> Label Description nutanix.com/prism-element-uuid UUID of the Prism Element cluster hosting the Kubernetes node VM. nutanix.com/prism-element-name Name of the Prism Element cluster hosting the Kubernetes node VM. nutanix.com/prism-host-uuid UUID of the Prism AHV host hosting the Kubernetes node VM. nutanix.com/prism-host-name Name of the Prism AHV host hosting the Kubernetes node VM. <p>Nutanix CCM will reconcile the labels periodically. </p>"},{"location":"ccm/latest/overview/","title":"Overview","text":"<p>Nutanix CCM provides Cloud Controller Manager functionality to Kubernetes clusters running on the Nutanix AHV hypervisor. Visit the Kubernetes Cloud Controller Manager documentation for more information about the general design of a Kubernetes CCM.</p> <p>Nutanix CCM communicates with Prism Central (CCM) to fetch all required information. See the Requirements page for more details.</p>"},{"location":"ccm/latest/overview/#nutanix-ccm-functionality","title":"Nutanix CCM functionality","text":"Version Node Controller Route Controller Service Controller v0.3.x Yes No No v0.2.x Yes No No <p>Nutanix CCM specific features:</p> Version Topology Discovery Custom Labeling v0.3.x Prism, Categories Yes v0.2.x Prism, Categories Yes"},{"location":"ccm/latest/pc_certificates/","title":"Certificate Trust","text":"<p>CCM invokes Prism Central APIs using the HTTPS protocol. CCM has different methods to handle the trust of the Prism Central certificates:</p> <ul> <li>Enable certificate verification (default)</li> <li>Configure an additional trust bundle</li> <li>Disable certificate verification</li> </ul> <p>See the respective sections below for more information.</p>"},{"location":"ccm/latest/pc_certificates/#enable-certificate-verification-default","title":"Enable certificate verification (default)","text":"<p>By default CCM will perform certificate verification when invoking Prism Central API calls. This requires Prism Central to be configured with a publicly trusted certificate authority.  No additional configuration is required in CCM.</p>"},{"location":"ccm/latest/pc_certificates/#configure-an-additional-trust-bundle","title":"Configure an additional trust bundle","text":"<p>CCM allows users to configure an additional trust bundle. This will allow CCM to verify certificates that are not issued by a publicy trusted certificate authority. </p> <p>To configure an additional trust bundle, see the Configuring the additional trust bundle section for more information.</p>"},{"location":"ccm/latest/pc_certificates/#configuring-the-additional-trust-bundle","title":"Configuring the additional trust bundle","text":"<p>To configure the additional trust bundle it is required to:</p> <ul> <li>Create a <code>ConfigMap</code> containing the additional trust bundle</li> <li>Configure the <code>prismCentral.additionalTrustBundle</code> object in the CCM <code>ConfigMap</code> called <code>nutanix-config</code>.</li> </ul>"},{"location":"ccm/latest/pc_certificates/#creating-the-additional-trust-bundle-configmap","title":"Creating the additional trust bundle ConfigMap","text":"<p>CCM supports two different formats for the <code>ConfigMap</code> containing the additional trust bundle. The first one is to add the additional trust bundle as a multi-line string in the <code>ConfigMap</code>, the second option is to add the trust bundle in <code>base64</code> encoded format. See the examples below.</p> <p>Multi-line string example: <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: user-ca-bundle\nnamespace: ${NAMESPACE}\ndata:\nca.crt: |\n-----BEGIN CERTIFICATE-----\n&lt;certificate string&gt;\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n&lt;certificate string&gt;\n-----END CERTIFICATE-----\n</code></pre></p> <p><code>base64</code> example:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: user-ca-bundle\nnamespace: ${NAMESPACE}\nbinaryData:\nca.crt: &lt;base64 string&gt;\n</code></pre> <p>Note</p> <p>The <code>base64</code> string needs to be added as <code>binaryData</code>.</p>"},{"location":"ccm/latest/pc_certificates/#configuring-the-ccm-for-an-additional-trust-bundle","title":"Configuring the CCM for an additional trust bundle","text":"<p>When the additional trust bundle <code>ConfigMap</code> is created, it needs to be referenced in the <code>nutanix-config</code> <code>ConfigMap</code>. Add the <code>prismCentral.additionalTrustBundle</code> object as shown below. Make sure the correct additional trust bundle <code>ConfigMap</code> is referenced.</p> <pre><code> ...\n\"prismCentral\": {\n...\n\"additionalTrustBundle\": {\n\"kind\": \"ConfigMap\",\n\"name\": \"user-ca-bundle\"\n}\n},\n...\n</code></pre> <p>Note</p> <p>The default value of <code>prismCentral.insecure</code> attribute is <code>false</code>. It can be omitted when an additional trust bundle is configured.  If <code>prismCentral.insecure</code> attribute is set to <code>true</code>, all certificate verification will be disabled. </p>"},{"location":"ccm/latest/pc_certificates/#disable-certificate-verification","title":"Disable certificate verification","text":"<p>Note</p> <p>Disabling certificate verification is not recommended for production purposes and should only be used for testing.</p> <p>Certificate verification can be disabled by setting the <code>prismCentral.insecure</code> attribute to <code>true</code> in the <code>nutanix-config</code> <code>ConfigMap</code>. Certificate verification will be disabled even if an additional trust bundle is configured and the <code>prismCentral.insecure</code> attribute is set to <code>true</code>. </p> <p>Example of how to disable certificate verification:</p> <pre><code>...\n\"prismCentral\": {\n...\n\"insecure\": true\n},\n...\n</code></pre>"},{"location":"ccm/latest/requirements/","title":"Requirements","text":"<p>This section provides an overview of the requirements for Nutanix CCM:</p>"},{"location":"ccm/latest/requirements/#port-requirements","title":"Port requirements","text":"<p>Nutanix CCM uses Prism Central APIs to fetch the required information for the Kubernetes nodes. As a result, the Kubernetes nodes need to have access to the Prism Central endpoint that is configured in the <code>nutanix-config</code> configmap.</p> Source Destination Protocol Port Description Kubernetes nodes Prism Central TCP 9440 Nutanix CCM communication to Prism Central"},{"location":"ccm/latest/requirements/#user-permissions","title":"User permissions","text":"<p>Nutanix CCM will only perform read operations and requires a user account with an assigned <code>Viewer</code> role to consume Prism Central APIs.</p>"},{"location":"ccm/latest/requirements/#required-roles-local-user","title":"Required roles: Local user","text":"Role Required User Admin No Prism Central Admin No <p>Note</p> <p>For local users, if no role is assigned, the local user will only get <code>Viewer</code> permissions</p>"},{"location":"ccm/latest/requirements/#required-roles-directory-user","title":"Required roles: Directory user","text":"<p>Assign following role in the user role-mapping if a non-local user is required: </p> Role Required Viewer Yes"},{"location":"ccm/latest/topology_discovery/","title":"Topology Discovery","text":"<p>One of the responsibilities of the CCM node controller is to annotate and label the nodes in a Kubernetes cluster with toplogy (region and zone) information. The Nutanix Cloud Controller Manager supports following topology discovery methods:</p> <ul> <li>Prism</li> <li>Categories</li> </ul> <p>The topology discovery method can be configured via the <code>nutanix-config</code> configmap. See Nutanix CCM Configuration for more information on the configuration parameters.</p>"},{"location":"ccm/latest/topology_discovery/#prism","title":"Prism","text":"<p>Prism-based topology discovery is the default mode for Nutanix CCM. In this mode CCM will discover the Prism Element (PE) cluster and Prism Central (PC) instance that host the Kubernetes node VM. Prism Central is configured as the region for the node, while Prism Element is configured as the zone.</p> <p>Prism-based topology discovery can be configured by omitting the <code>topologyDiscovery</code> attribute from the <code>nutanix-config</code> configmap or by passing following object: <pre><code>      \"topologyDiscovery\": {\n\"type\": \"Prism\"\n}\n</code></pre></p>"},{"location":"ccm/latest/topology_discovery/#example","title":"Example","text":"<p>If a Kubernetes Node VM is hosted on PC <code>my-pc-instance</code> and PE <code>my-pe-cluster-1</code>, Nutanix CCM will assign following labels to the Kubernetes node:</p> Key Value topology.kubernetes.io/region my-pc-instance topology.kubernetes.io/zone my-pe-cluster-1"},{"location":"ccm/latest/topology_discovery/#categories","title":"Categories","text":"<p>The category-based topology discovery mode allows users to assign categories to Prism Element clusters and Kubernetes Node VMs to define a custom topology. Nutanix CCM will hierarchically search for the required categories on the VM/PE.</p> <p>Note</p> <p>Categories assigned to the VM object will take precedence over the categories assigned to the PE cluster.</p> <p>It is required for the categories to exist inside of the PC environment. CCM will not create and assign the categories. Visit the Prism Central documentation for more information regarding categories.</p> <p>To enable the Categories topology discovery mode for Nutanix CCM, provide following information in the <code>topologyDiscovery</code> attribute:</p> <pre><code>      \"topologyDiscovery\": {\n\"type\": \"Categories\",\n\"topologyCategories\": {\n\"regionCategory\": \"${NUTANIX_REGION_CATEGORY}\",\n\"zoneCategory\": \"${NUTANIX_ZONE_CATEGORY}\"\n}\n}\n</code></pre>"},{"location":"ccm/latest/topology_discovery/#example_1","title":"Example","text":"<p>Define a set of categories in PC that will be used for topology discovery:</p> Key Value my-region-category region-1, region-2 my-zone-category zone-1, zone-2, zone-3 <p>Assign the categories to the Nutanix entities:</p> Nutanix entity Categories my-pe-cluster-1 my-region-category:region-1my-zone-category:zone-2 my-pe-cluster-2 my-region-category:region-2my-zone-category:zone-3 k8s-node-3 my-region-category:region-2my-zone-category:zone-2 k8s-node-4 my-zone-category:zone-1 <p>Configure CCM to use categories for topology discovery: <pre><code>      \"topologyDiscovery\": {\n\"type\": \"Categories\",\n\"topologyCategories\": {\n\"regionCategory\": \"my-region-category\",\n\"zoneCategory\": \"my-zone-category\"\n}\n}\n</code></pre></p> <p>Scenario 1: Kubernetes node k8s-node-1 is running on my-pe-cluster-1<p>Following topology labels will be assigned to Kubernetes node <code>k8s-node-1</code>:</p> </p> Key Value topology.kubernetes.io/region region-1 topology.kubernetes.io/zone zone-2 <p>Categories assigned to PE will be used.</p> <p>Scenario 2: Kubernetes node k8s-node-2 is running on my-pe-cluster-2</p> <p>Following topology labels will be assigned to Kubernetes node <code>k8s-node-2</code>:</p> Key Value topology.kubernetes.io/region region-2 topology.kubernetes.io/zone zone-3 <p>Categories assigned to PE will be used.</p> <p>Scenario 3: Kubernetes node k8s-node-3 is running on my-pe-cluster-2</p> <p>Following topology labels will be assigned to Kubernetes node <code>k8s-node-3</code>:</p> Key Value topology.kubernetes.io/region region-2 topology.kubernetes.io/zone zone-2 <p>Categories assigned to the VM will be used.</p> <p>Scenario 4: Kubernetes node k8s-node-4 is running on my-pe-cluster-1</p> <p>Following topology labels will be assigned to Kubernetes node <code>k8s-node-4</code>:</p> Key Value topology.kubernetes.io/region region-1 topology.kubernetes.io/zone zone-1 <p>In this scenario Nutanix CCM will use the value of the <code>my-zone-category</code> category that is assigned to the VM. Since the <code>my-region-category</code>is not assigned to the VM, Nutanix CCM will search for the category on PE and use the corresponding category value.</p>"},{"location":"ccm/v0.2.x/ccm_configuration/","title":"Nutanix CCM Configuration","text":"<p>Nutanix CCM can be configured via a <code>JSON</code> formated file stored in a configmap called <code>nutanix-config</code>. This configmap is located in the same namespace as the Nutanix CCM deployment. See the <code>manifests/cloud-provider-nutanix-deployment.yaml</code> file for details on the Nutanix CCM deployment. </p> <p>Example <code>nutanix-config</code> configmap: <pre><code>---\nkind: ConfigMap\napiVersion: v1\nmetadata:\nname: nutanix-config\nnamespace: kube-system\ndata:\nnutanix_config.json: |-\n{\n\"prismCentral\": {\n\"address\": \"${NUTANIX_ENDPOINT}\",\n\"port\": ${NUTANIX_PORT},\n\"insecure\": ${NUTANIX_INSECURE},\n\"credentialRef\": {\n\"kind\": \"secret\",\n\"name\": \"nutanix-creds\"\n}\n},\n\"enableCustomLabeling\": false,\n\"topologyDiscovery\": {\n\"type\": \"Categories\",\n\"topologyCategories\": {\n\"regionCategory\": \"${NUTANIX_REGION_CATEGORY}\",\n\"zoneCategory\": \"${NUTANIX_ZONE_CATEGORY}\"\n}\n}\n}\n</code></pre></p> <p>The table below provides an overview of the supported configuration parameters.</p>"},{"location":"ccm/v0.2.x/ccm_configuration/#configuration-parameters","title":"Configuration parameters","text":"Key Type Description topologyDiscovery object (Optional) Configures the topology discovery mode.<code>Prism</code> topology discovery is used by default if <code>topologyDiscovery</code> attribute is not passed. topologyDiscovery.type string Topology Discovery mode. Can be <code>Prism</code> or <code>Categories</code>. See Topology Discovery for more information. topologyDiscovery.topologyCategories object Required if topology discovery mode is <code>Categories</code>. topologyDiscovery.topologyCategories.regionCategory string Category key defining the region of the Kubernetes node. topologyDiscovery.topologyCategories.zoneCategory string Category key defining the zone of the Kubernetes node. enableCustomLabeling bool Boolean value to enable custom labeling. See Custom Labeling for more information.Default: <code>false</code> prismCentral object Prism Central endpoint configuration. prismCentral.address string FQDN/IP of the Prism Central endpoint. prismCentral.port int Port to connect to Prism Central.Default: <code>9440</code> prismCentral.insecure bool Disable Prism Central certificate checking.Default: <code>false</code> prismCentral.credentialRef object Prism Central credential configuration. See Credentials for more information. prismCentral.credentialRef.kind string Credential kind.Allowed value: <code>secret</code> prismCentral.credentialRef.name string Name of the secret. prismCentral.credentialRef.namespace string (Optional) Namespace of the secret."},{"location":"ccm/v0.2.x/ccm_credentials/","title":"Credentials","text":"<p>Nutanix CCM requires credentials to connect to Prism Central. These credentials need to be stored in a secret in following format:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\nname: nutanix-creds\nnamespace: kube-system\nstringData:\ncredentials: |\n[\n{\n\"type\": \"basic_auth\", \n\"data\": { \n\"prismCentral\":{\n\"username\": \"$NUTANIX_USERNAME\", \n\"password\": \"$NUTANIX_PASSWORD\"\n},\n\"prismElements\": null\n}\n}\n]\n</code></pre> <p>See Requirements for more information on the required permissions.</p>"},{"location":"ccm/v0.2.x/custom_labeling/","title":"Custom Labeling","text":"<p>Enabling the Nutanix CCM custom labeling feature will add additional labels to the Kubernetes nodes. See Nutanix CCM Configuration for more information on how to configure CCM to enable custom labeling.</p> <p>The following labels will be added:</p> Label Description nutanix.com/prism-element-uuid UUID of the Prism Element cluster hosting the Kubernetes node VM. nutanix.com/prism-element-name Name of the Prism Element cluster hosting the Kubernetes node VM. nutanix.com/prism-host-uuid UUID of the Prism AHV host hosting the Kubernetes node VM. nutanix.com/prism-host-name Name of the Prism AHV host hosting the Kubernetes node VM. <p>Nutanix CCM will reconcile the labels periodically. </p>"},{"location":"ccm/v0.2.x/overview/","title":"Overview","text":"<p>Nutanix CCM provides Cloud Controller Manager functionality to Kubernetes clusters running on the Nutanix AHV hypervisor. Visit the Kubernetes Cloud Controller Manager documentation for more information about the general design of a Kubernetes CCM.</p> <p>Nutanix CCM communicates with Prism Central (CCM) to fetch all required information. See the Requirements page for more details.</p>"},{"location":"ccm/v0.2.x/overview/#nutanix-ccm-functionality","title":"Nutanix CCM functionality","text":"Version Node Controller Route Controller Service Controller v0.2.x Yes No No <p>Nutanix CCM specific features:</p> Version Topology Discovery Custom Labeling v0.2.x Prism, Categories Yes"},{"location":"ccm/v0.2.x/requirements/","title":"Requirements","text":"<p>This section provides an overview of the requirements for Nutanix CCM:</p>"},{"location":"ccm/v0.2.x/requirements/#port-requirements","title":"Port requirements","text":"<p>Nutanix CCM uses Prism Central APIs to fetch the required information for the Kubernetes nodes. As a result, the Kubernetes nodes need to have access to the Prism Central endpoint that is configured in the <code>nutanix-config</code> configmap.</p> Source Destination Protocol Port Description Kubernetes nodes Prism Central TCP 9440 Nutanix CCM communication to Prism Central"},{"location":"ccm/v0.2.x/requirements/#user-permissions","title":"User permissions","text":"<p>Nutanix CCM will only perform read operations and requires a user account with an assigned <code>Viewer</code> role to consume Prism Central APIs.</p>"},{"location":"ccm/v0.2.x/requirements/#required-roles-local-user","title":"Required roles: Local user","text":"Role Required User Admin No Prism Central Admin No <p>Note</p> <p>For local users, if no role is assigned, the local user will only get <code>Viewer</code> permissions</p>"},{"location":"ccm/v0.2.x/requirements/#required-roles-directory-user","title":"Required roles: Directory user","text":"<p>Assign following role in the user role-mapping if a non-local user is required: </p> Role Required Viewer Yes"},{"location":"ccm/v0.2.x/topology_discovery/","title":"Topology Discovery","text":"<p>One of the responsibilities of the CCM node controller is to annotate and label the nodes in a Kubernetes cluster with toplogy (region and zone) information. The Nutanix Cloud Controller Manager supports following topology discovery methods:</p> <ul> <li>Prism</li> <li>Categories</li> </ul> <p>The topology discovery method can be configured via the <code>nutanix-config</code> configmap. See Nutanix CCM Configuration for more information on the configuration parameters.</p>"},{"location":"ccm/v0.2.x/topology_discovery/#prism","title":"Prism","text":"<p>Prism-based topology discovery is the default mode for Nutanix CCM. In this mode CCM will discover the Prism Element (PE) cluster and Prism Central (PC) instance that host the Kubernetes node VM. Prism Central is configured as the region for the node, while Prism Element is configured as the zone.</p> <p>Prism-based topology discovery can be configured by omitting the <code>topologyDiscovery</code> attribute from the <code>nutanix-config</code> configmap or by passing following object: <pre><code>      \"topologyDiscovery\": {\n\"type\": \"Prism\"\n}\n</code></pre></p>"},{"location":"ccm/v0.2.x/topology_discovery/#example","title":"Example","text":"<p>If a Kubernetes Node VM is hosted on PC <code>my-pc-instance</code> and PE <code>my-pe-cluster-1</code>, Nutanix CCM will assign following labels to the Kubernetes node:</p> Key Value topology.kubernetes.io/region my-pc-instance topology.kubernetes.io/zone my-pe-cluster-1"},{"location":"ccm/v0.2.x/topology_discovery/#categories","title":"Categories","text":"<p>The category-based topology discovery mode allows users to assign categories to Prism Element clusters and Kubernetes Node VMs to define a custom topology. Nutanix CCM will hierarchically search for the required categories on the VM/PE.</p> <p>Note</p> <p>Categories assigned to the VM object will take precedence over the categories assigned to the PE cluster.</p> <p>It is required for the categories to exist inside of the PC environment. CCM will not create and assign the categories. Visit the Prism Central documentation for more information regarding categories.</p> <p>To enable the Categories topology discovery mode for Nutanix CCM, provide following information in the <code>topologyDiscovery</code> attribute:</p> <pre><code>      \"topologyDiscovery\": {\n\"type\": \"Categories\",\n\"topologyCategories\": {\n\"regionCategory\": \"${NUTANIX_REGION_CATEGORY}\",\n\"zoneCategory\": \"${NUTANIX_ZONE_CATEGORY}\"\n}\n}\n</code></pre>"},{"location":"ccm/v0.2.x/topology_discovery/#example_1","title":"Example","text":"<p>Define a set of categories in PC that will be used for topology discovery:</p> Key Value my-region-category region-1, region-2 my-zone-category zone-1, zone-2, zone-3 <p>Assign the categories to the Nutanix entities:</p> Nutanix entity Categories my-pe-cluster-1 my-region-category:region-1my-zone-category:zone-2 my-pe-cluster-2 my-region-category:region-2my-zone-category:zone-3 k8s-node-3 my-region-category:region-2my-zone-category:zone-2 k8s-node-4 my-zone-category:zone-1 <p>Configure CCM to use categories for topology discovery: <pre><code>      \"topologyDiscovery\": {\n\"type\": \"Categories\",\n\"topologyCategories\": {\n\"regionCategory\": \"my-region-category\",\n\"zoneCategory\": \"my-zone-category\"\n}\n}\n</code></pre></p> <p>Scenario 1: Kubernetes node k8s-node-1 is running on my-pe-cluster-1<p>Following topology labels will be assigned to Kubernetes node <code>k8s-node-1</code>:</p> </p> Key Value topology.kubernetes.io/region region-1 topology.kubernetes.io/zone zone-2 <p>Categories assigned to PE will be used.</p> <p>Scenario 2: Kubernetes node k8s-node-2 is running on my-pe-cluster-2</p> <p>Following topology labels will be assigned to Kubernetes node <code>k8s-node-2</code>:</p> Key Value topology.kubernetes.io/region region-2 topology.kubernetes.io/zone zone-3 <p>Categories assigned to PE will be used.</p> <p>Scenario 3: Kubernetes node k8s-node-3 is running on my-pe-cluster-2</p> <p>Following topology labels will be assigned to Kubernetes node <code>k8s-node-3</code>:</p> Key Value topology.kubernetes.io/region region-2 topology.kubernetes.io/zone zone-2 <p>Categories assigned to the VM will be used.</p> <p>Scenario 4: Kubernetes node k8s-node-4 is running on my-pe-cluster-1</p> <p>Following topology labels will be assigned to Kubernetes node <code>k8s-node-4</code>:</p> Key Value topology.kubernetes.io/region region-1 topology.kubernetes.io/zone zone-1 <p>In this scenario Nutanix CCM will use the value of the <code>my-zone-category</code> category that is assigned to the VM. Since the <code>my-region-category</code>is not assigned to the VM, Nutanix CCM will search for the category on PE and use the corresponding category value.</p>"},{"location":"ccm/v0.3.x/ccm_configuration/","title":"Nutanix CCM Configuration","text":"<p>Nutanix CCM can be configured via a <code>JSON</code> formated file stored in a configmap called <code>nutanix-config</code>. This configmap is located in the same namespace as the Nutanix CCM deployment. See the <code>manifests/cloud-provider-nutanix-deployment.yaml</code> file for details on the Nutanix CCM deployment. </p> <p>Example <code>nutanix-config</code> configmap: <pre><code>---\nkind: ConfigMap\napiVersion: v1\nmetadata:\nname: nutanix-config\nnamespace: kube-system\ndata:\nnutanix_config.json: |-\n{\n\"prismCentral\": {\n\"address\": \"${NUTANIX_ENDPOINT}\",\n\"port\": ${NUTANIX_PORT},\n\"insecure\": ${NUTANIX_INSECURE},\n\"credentialRef\": {\n\"kind\": \"secret\",\n\"name\": \"nutanix-creds\"\n},\n\"additionalTrustBundle\": {\n\"kind\": \"ConfigMap\",\n\"name\": \"user-ca-bundle\"\n}\n},\n\"enableCustomLabeling\": false,\n\"topologyDiscovery\": {\n\"type\": \"Categories\",\n\"topologyCategories\": {\n\"regionCategory\": \"${NUTANIX_REGION_CATEGORY}\",\n\"zoneCategory\": \"${NUTANIX_ZONE_CATEGORY}\"\n}\n}\n}\n</code></pre></p> <p>The table below provides an overview of the supported configuration parameters.</p>"},{"location":"ccm/v0.3.x/ccm_configuration/#configuration-parameters","title":"Configuration parameters","text":"Key Type Description topologyDiscovery object (Optional) Configures the topology discovery mode.<code>Prism</code> topology discovery is used by default if <code>topologyDiscovery</code> attribute is not passed. topologyDiscovery.type string Topology Discovery mode. Can be <code>Prism</code> or <code>Categories</code>. See Topology Discovery for more information. topologyDiscovery.topologyCategories object Required if topology discovery mode is <code>Categories</code>. topologyDiscovery.topologyCategories.regionCategory string Category key defining the region of the Kubernetes node. topologyDiscovery.topologyCategories.zoneCategory string Category key defining the zone of the Kubernetes node. enableCustomLabeling bool Boolean value to enable custom labeling. See Custom Labeling for more information.Default: <code>false</code> prismCentral object Prism Central endpoint configuration. prismCentral.address string FQDN/IP of the Prism Central endpoint. prismCentral.port int Port to connect to Prism Central.Default: <code>9440</code> prismCentral.insecure bool Disable Prism Central certificate checking.Default: <code>false</code> prismCentral.credentialRef object Prism Central credential configuration. See Credentials for more information. prismCentral.credentialRef.kind string Credential kind.Allowed value: <code>secret</code> prismCentral.credentialRef.name string Name of the secret. prismCentral.credentialRef.namespace string (Optional) Namespace of the secret. prismCentral.additionalTrustBundle object Reference to the certificate trust bundle used for Prism Central connection. prismCentral.additionalTrustBundle.kind string Kind of the additionalTrustBundle. Allowed value: <code>ConfigMap</code> prismCentral.additionalTrustBundle.name string Name of the <code>ConfigMap</code> containing the Prism Central trust bundle. prismCentral.additionalTrustBundle.namespace string (Optional) Namespace of the <code>ConfigMap</code> containing the Prism Central trust bundle. See Certificate Trust for more information."},{"location":"ccm/v0.3.x/ccm_credentials/","title":"Credentials","text":"<p>Nutanix CCM requires credentials to connect to Prism Central. These credentials need to be stored in a secret in following format:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\nname: nutanix-creds\nnamespace: kube-system\nstringData:\ncredentials: |\n[\n{\n\"type\": \"basic_auth\", \n\"data\": { \n\"prismCentral\":{\n\"username\": \"$NUTANIX_USERNAME\", \n\"password\": \"$NUTANIX_PASSWORD\"\n},\n\"prismElements\": null\n}\n}\n]\n</code></pre> <p>See Requirements for more information on the required permissions.</p>"},{"location":"ccm/v0.3.x/custom_labeling/","title":"Custom Labeling","text":"<p>Enabling the Nutanix CCM custom labeling feature will add additional labels to the Kubernetes nodes. See Nutanix CCM Configuration for more information on how to configure CCM to enable custom labeling.</p> <p>The following labels will be added:</p> Label Description nutanix.com/prism-element-uuid UUID of the Prism Element cluster hosting the Kubernetes node VM. nutanix.com/prism-element-name Name of the Prism Element cluster hosting the Kubernetes node VM. nutanix.com/prism-host-uuid UUID of the Prism AHV host hosting the Kubernetes node VM. nutanix.com/prism-host-name Name of the Prism AHV host hosting the Kubernetes node VM. <p>Nutanix CCM will reconcile the labels periodically. </p>"},{"location":"ccm/v0.3.x/overview/","title":"Overview","text":"<p>Nutanix CCM provides Cloud Controller Manager functionality to Kubernetes clusters running on the Nutanix AHV hypervisor. Visit the Kubernetes Cloud Controller Manager documentation for more information about the general design of a Kubernetes CCM.</p> <p>Nutanix CCM communicates with Prism Central (CCM) to fetch all required information. See the Requirements page for more details.</p>"},{"location":"ccm/v0.3.x/overview/#nutanix-ccm-functionality","title":"Nutanix CCM functionality","text":"Version Node Controller Route Controller Service Controller v0.3.x Yes No No v0.2.x Yes No No <p>Nutanix CCM specific features:</p> Version Topology Discovery Custom Labeling v0.3.x Prism, Categories Yes v0.2.x Prism, Categories Yes"},{"location":"ccm/v0.3.x/pc_certificates/","title":"Certificate Trust","text":"<p>CCM invokes Prism Central APIs using the HTTPS protocol. CCM has different methods to handle the trust of the Prism Central certificates:</p> <ul> <li>Enable certificate verification (default)</li> <li>Configure an additional trust bundle</li> <li>Disable certificate verification</li> </ul> <p>See the respective sections below for more information.</p>"},{"location":"ccm/v0.3.x/pc_certificates/#enable-certificate-verification-default","title":"Enable certificate verification (default)","text":"<p>By default CCM will perform certificate verification when invoking Prism Central API calls. This requires Prism Central to be configured with a publicly trusted certificate authority.  No additional configuration is required in CCM.</p>"},{"location":"ccm/v0.3.x/pc_certificates/#configure-an-additional-trust-bundle","title":"Configure an additional trust bundle","text":"<p>CCM allows users to configure an additional trust bundle. This will allow CCM to verify certificates that are not issued by a publicy trusted certificate authority. </p> <p>To configure an additional trust bundle, see the Configuring the additional trust bundle section for more information.</p>"},{"location":"ccm/v0.3.x/pc_certificates/#configuring-the-additional-trust-bundle","title":"Configuring the additional trust bundle","text":"<p>To configure the additional trust bundle it is required to:</p> <ul> <li>Create a <code>ConfigMap</code> containing the additional trust bundle</li> <li>Configure the <code>prismCentral.additionalTrustBundle</code> object in the CCM <code>ConfigMap</code> called <code>nutanix-config</code>.</li> </ul>"},{"location":"ccm/v0.3.x/pc_certificates/#creating-the-additional-trust-bundle-configmap","title":"Creating the additional trust bundle ConfigMap","text":"<p>CCM supports two different formats for the <code>ConfigMap</code> containing the additional trust bundle. The first one is to add the additional trust bundle as a multi-line string in the <code>ConfigMap</code>, the second option is to add the trust bundle in <code>base64</code> encoded format. See the examples below.</p> <p>Multi-line string example: <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: user-ca-bundle\nnamespace: ${NAMESPACE}\ndata:\nca.crt: |\n-----BEGIN CERTIFICATE-----\n&lt;certificate string&gt;\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n&lt;certificate string&gt;\n-----END CERTIFICATE-----\n</code></pre></p> <p><code>base64</code> example:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: user-ca-bundle\nnamespace: ${NAMESPACE}\nbinaryData:\nca.crt: &lt;base64 string&gt;\n</code></pre> <p>Note</p> <p>The <code>base64</code> string needs to be added as <code>binaryData</code>.</p>"},{"location":"ccm/v0.3.x/pc_certificates/#configuring-the-ccm-for-an-additional-trust-bundle","title":"Configuring the CCM for an additional trust bundle","text":"<p>When the additional trust bundle <code>ConfigMap</code> is created, it needs to be referenced in the <code>nutanix-config</code> <code>ConfigMap</code>. Add the <code>prismCentral.additionalTrustBundle</code> object as shown below. Make sure the correct additional trust bundle <code>ConfigMap</code> is referenced.</p> <pre><code> ...\n\"prismCentral\": {\n...\n\"additionalTrustBundle\": {\n\"kind\": \"ConfigMap\",\n\"name\": \"user-ca-bundle\"\n}\n},\n...\n</code></pre> <p>Note</p> <p>The default value of <code>prismCentral.insecure</code> attribute is <code>false</code>. It can be omitted when an additional trust bundle is configured.  If <code>prismCentral.insecure</code> attribute is set to <code>true</code>, all certificate verification will be disabled. </p>"},{"location":"ccm/v0.3.x/pc_certificates/#disable-certificate-verification","title":"Disable certificate verification","text":"<p>Note</p> <p>Disabling certificate verification is not recommended for production purposes and should only be used for testing.</p> <p>Certificate verification can be disabled by setting the <code>prismCentral.insecure</code> attribute to <code>true</code> in the <code>nutanix-config</code> <code>ConfigMap</code>. Certificate verification will be disabled even if an additional trust bundle is configured and the <code>prismCentral.insecure</code> attribute is set to <code>true</code>. </p> <p>Example of how to disable certificate verification:</p> <pre><code>...\n\"prismCentral\": {\n...\n\"insecure\": true\n},\n...\n</code></pre>"},{"location":"ccm/v0.3.x/requirements/","title":"Requirements","text":"<p>This section provides an overview of the requirements for Nutanix CCM:</p>"},{"location":"ccm/v0.3.x/requirements/#port-requirements","title":"Port requirements","text":"<p>Nutanix CCM uses Prism Central APIs to fetch the required information for the Kubernetes nodes. As a result, the Kubernetes nodes need to have access to the Prism Central endpoint that is configured in the <code>nutanix-config</code> configmap.</p> Source Destination Protocol Port Description Kubernetes nodes Prism Central TCP 9440 Nutanix CCM communication to Prism Central"},{"location":"ccm/v0.3.x/requirements/#user-permissions","title":"User permissions","text":"<p>Nutanix CCM will only perform read operations and requires a user account with an assigned <code>Viewer</code> role to consume Prism Central APIs.</p>"},{"location":"ccm/v0.3.x/requirements/#required-roles-local-user","title":"Required roles: Local user","text":"Role Required User Admin No Prism Central Admin No <p>Note</p> <p>For local users, if no role is assigned, the local user will only get <code>Viewer</code> permissions</p>"},{"location":"ccm/v0.3.x/requirements/#required-roles-directory-user","title":"Required roles: Directory user","text":"<p>Assign following role in the user role-mapping if a non-local user is required: </p> Role Required Viewer Yes"},{"location":"ccm/v0.3.x/topology_discovery/","title":"Topology Discovery","text":"<p>One of the responsibilities of the CCM node controller is to annotate and label the nodes in a Kubernetes cluster with toplogy (region and zone) information. The Nutanix Cloud Controller Manager supports following topology discovery methods:</p> <ul> <li>Prism</li> <li>Categories</li> </ul> <p>The topology discovery method can be configured via the <code>nutanix-config</code> configmap. See Nutanix CCM Configuration for more information on the configuration parameters.</p>"},{"location":"ccm/v0.3.x/topology_discovery/#prism","title":"Prism","text":"<p>Prism-based topology discovery is the default mode for Nutanix CCM. In this mode CCM will discover the Prism Element (PE) cluster and Prism Central (PC) instance that host the Kubernetes node VM. Prism Central is configured as the region for the node, while Prism Element is configured as the zone.</p> <p>Prism-based topology discovery can be configured by omitting the <code>topologyDiscovery</code> attribute from the <code>nutanix-config</code> configmap or by passing following object: <pre><code>      \"topologyDiscovery\": {\n\"type\": \"Prism\"\n}\n</code></pre></p>"},{"location":"ccm/v0.3.x/topology_discovery/#example","title":"Example","text":"<p>If a Kubernetes Node VM is hosted on PC <code>my-pc-instance</code> and PE <code>my-pe-cluster-1</code>, Nutanix CCM will assign following labels to the Kubernetes node:</p> Key Value topology.kubernetes.io/region my-pc-instance topology.kubernetes.io/zone my-pe-cluster-1"},{"location":"ccm/v0.3.x/topology_discovery/#categories","title":"Categories","text":"<p>The category-based topology discovery mode allows users to assign categories to Prism Element clusters and Kubernetes Node VMs to define a custom topology. Nutanix CCM will hierarchically search for the required categories on the VM/PE.</p> <p>Note</p> <p>Categories assigned to the VM object will take precedence over the categories assigned to the PE cluster.</p> <p>It is required for the categories to exist inside of the PC environment. CCM will not create and assign the categories. Visit the Prism Central documentation for more information regarding categories.</p> <p>To enable the Categories topology discovery mode for Nutanix CCM, provide following information in the <code>topologyDiscovery</code> attribute:</p> <pre><code>      \"topologyDiscovery\": {\n\"type\": \"Categories\",\n\"topologyCategories\": {\n\"regionCategory\": \"${NUTANIX_REGION_CATEGORY}\",\n\"zoneCategory\": \"${NUTANIX_ZONE_CATEGORY}\"\n}\n}\n</code></pre>"},{"location":"ccm/v0.3.x/topology_discovery/#example_1","title":"Example","text":"<p>Define a set of categories in PC that will be used for topology discovery:</p> Key Value my-region-category region-1, region-2 my-zone-category zone-1, zone-2, zone-3 <p>Assign the categories to the Nutanix entities:</p> Nutanix entity Categories my-pe-cluster-1 my-region-category:region-1my-zone-category:zone-2 my-pe-cluster-2 my-region-category:region-2my-zone-category:zone-3 k8s-node-3 my-region-category:region-2my-zone-category:zone-2 k8s-node-4 my-zone-category:zone-1 <p>Configure CCM to use categories for topology discovery: <pre><code>      \"topologyDiscovery\": {\n\"type\": \"Categories\",\n\"topologyCategories\": {\n\"regionCategory\": \"my-region-category\",\n\"zoneCategory\": \"my-zone-category\"\n}\n}\n</code></pre></p> <p>Scenario 1: Kubernetes node k8s-node-1 is running on my-pe-cluster-1<p>Following topology labels will be assigned to Kubernetes node <code>k8s-node-1</code>:</p> </p> Key Value topology.kubernetes.io/region region-1 topology.kubernetes.io/zone zone-2 <p>Categories assigned to PE will be used.</p> <p>Scenario 2: Kubernetes node k8s-node-2 is running on my-pe-cluster-2</p> <p>Following topology labels will be assigned to Kubernetes node <code>k8s-node-2</code>:</p> Key Value topology.kubernetes.io/region region-2 topology.kubernetes.io/zone zone-3 <p>Categories assigned to PE will be used.</p> <p>Scenario 3: Kubernetes node k8s-node-3 is running on my-pe-cluster-2</p> <p>Following topology labels will be assigned to Kubernetes node <code>k8s-node-3</code>:</p> Key Value topology.kubernetes.io/region region-2 topology.kubernetes.io/zone zone-2 <p>Categories assigned to the VM will be used.</p> <p>Scenario 4: Kubernetes node k8s-node-4 is running on my-pe-cluster-1</p> <p>Following topology labels will be assigned to Kubernetes node <code>k8s-node-4</code>:</p> Key Value topology.kubernetes.io/region region-1 topology.kubernetes.io/zone zone-1 <p>In this scenario Nutanix CCM will use the value of the <code>my-zone-category</code> category that is assigned to the VM. Since the <code>my-region-category</code>is not assigned to the VM, Nutanix CCM will search for the category on PE and use the corresponding category value.</p>"},{"location":"openshift/install/agnostic/","title":"Red Hat OpenShift Container Platform Manual Installation on Nutanix AOS (AHV)","text":"<p>Note: Red Hat OpenShift Container Platform has been tested for specific compatibility on following AOS and Prism Central versions:</p> Openshift AOS Prism Central 4.6 - 4.11 5.20.4+ or 6.1.1+ 2022.4+"},{"location":"openshift/install/agnostic/#installation-prerequisites","title":"Installation Prerequisites","text":"<ol> <li>Before installing OpenShift Container Platform, download the installation file on a local computer. A computer that runs Linux or macOS, with 500 MB of local disk space is required.</li> <li>Access the Platform Agnostic User-Provisioned Infrastructure page on the Red Hat OpenShift Cluster Manager site. If you have a Red Hat account, log in with your credentials. If you do not, create an account.</li> <li>Follow the steps to download the OpenShift installer, pull secret, command-line tools (CLI), and Red Hat Enterprise Linux CoreOS (RHCOS) ISO. Additional details can be found in the OpenShift documentation.</li> <li>Review the User Provisioned Infrastructure documentation for the version of OpenShift Container Platform you wish to install.</li> </ol>"},{"location":"openshift/install/agnostic/#additional-infrastructure-requirements","title":"Additional Infrastructure Requirements","text":"<ol> <li>Follow OpenShift documentation to:<ol> <li>Configure Networking requirements for user-provisioned infrastructure (documentation). This includes:<ol> <li>Network topology requirements<ol> <li>API load balancer</li> <li>Application Ingress load balancer</li> </ol> </li> <li>NTP configuration</li> <li>DNS requirements</li> </ol> </li> <li>Generate an SSH private key and add it to the agent (documentation). This key is used to access the bootstrap machine in a public cluster to troubleshoot installation issues.</li> </ol> </li> </ol>"},{"location":"openshift/install/agnostic/#create-and-stage-the-installation-config-and-manifests","title":"Create and stage the Installation Config and Manifests","text":"<ol> <li>Follow OpenShift documentation to:<ol> <li>Manually create the installation configuration file (documentation).</li> <li>Create the Kubernetes manifest and Ignition config files (documentation).</li> </ol> </li> <li>Configure a web server that is accessible from the network attached to the OpenShift VMs. This server will host the ignition config files.</li> <li>Copy the ignition files onto the web server so they can be passed to the OpenShift VMs for installation.</li> </ol>"},{"location":"openshift/install/agnostic/#rhcos-virtual-machine-creation","title":"RHCOS Virtual Machine Creation","text":"<ol> <li>Access the Prism Central or Prism Element instance managing the AOS cluster where you would like to install OpenShift.</li> <li>From the main menu, browse to Virtual Infrastructure \u2192 Images, and follow prompts to upload the RHCOS iso downloaded as part of the prerequisites. Prism Element users will find image configuration by browsing to \u2699 \u2192 Image Configuration. </li> <li>If a new subnet is required, browse to Network &amp; Security \u2192 Subnets and create it. If this network is not configured with Nutanix IPAM and doesn\u2019t have DHCP (with reservation) available, VMs will require manual network configuration at boot.</li> <li> <p>Create the VMs that will be used in the OpenShift cluster. Machine requirements and their minimum resource requirements can be found in the OpenShift documentation.</p> <ol> <li> <p>Browse to Virtual Infrastructure \u2192 VMs, then Create VM and follow prompts to create VMs using the settings below.</p> <ol> <li>Specify the number of vCPUs and memory as required.</li> <li> <p>Attach the RHCOS iso by cloning the uploaded image.</p> <p></p> </li> <li> <p>Attach an additional disk meeting the storage requirements of the VM.</p> <p></p> </li> <li> <p>Attach the required subnet to the VM. If this network is configured with IP Address Management (IPAM), choose Assign Static IP and specify the appropriate address. Note that for subnets without IPAM or DHCP (with reservation), VMs will initially require manual network configuration at boot.</p> <p></p> </li> <li> <p>Under Boot Configuration, choose Legacy BIOS Mode with Default Boot Order.</p> </li> <li>Add any required Categories, choose UTC Timezone, and No Guest Customization.</li> <li>Review settings and complete VM creation.</li> </ol> </li> </ol> </li> <li> <p>From the main menu, browse to Virtual Infrastructure \u2192 VMs. From the List view, select each VM, power it on, and launch the console from the Action menu. Depending on the type of machine, run the following example command in the console:</p> <p>Note: The commands below assume the provisioned subnet has IPAM or DHCP (with reservation) configured and VMs have a static IP assigned. If manual network configuration is required, follow the steps documented in OpenShift documentation .</p> <ul> <li> <p>Bootstrap</p> <pre><code>sudo coreos-installer install /dev/sda --copy-network --ignition-url=https://&lt;host+path&gt;/bootstrap.ign\n</code></pre> </li> <li> <p>Control Plane</p> <pre><code>sudo coreos-installer install /dev/sda --copy-network --ignition-url=https://&lt;host+path&gt;/master.ign\n</code></pre> </li> <li> <p>Compute</p> <pre><code>sudo coreos-installer install /dev/sda --copy-network --ignition-url=https://&lt;host+path&gt;/worker.ign\n</code></pre> </li> </ul> </li> <li> <p>After RHCOS installs, unmount the iso and reboot the VM. During the reboot, the Ignition config file is applied. </p> </li> </ol>"},{"location":"openshift/install/agnostic/#creating-the-openshift-cluster","title":"Creating the OpenShift Cluster","text":"<ol> <li>Follow OpenShift documentation to:<ol> <li>Create and log into the cluster (documentation).</li> <li>Approve certificate signing requests and watch the cluster components come online (documentation).</li> <li>Monitor for cluster completion (documentation).</li> </ol> </li> </ol>"},{"location":"openshift/install/agnostic/#post-install","title":"Post Install","text":"<ol> <li>Follow the post install instructions to complete cluster configuration.</li> </ol>"},{"location":"openshift/install/ipi/","title":"Red Hat OpenShift Container Platform Installer Provisioned Installation on Nutanix AOS (AHV)","text":"<p>Note: Red Hat OpenShift Container Platform IPI has been tested for specific compatibility on following AOS and Prism Central versions:</p> Openshift AOS Prism Central 4.12 5.20.4+ or 6.5.1+ 2022.4+ 4.11 5.20.4+ or 6.1.1+ 2022.4+"},{"location":"openshift/install/ipi/#installation-prerequisites","title":"Installation Prerequisites","text":""},{"location":"openshift/install/ipi/#certificate-requirements","title":"Certificate Requirements","text":"<p>If your Prism Central instance is using the default self-signed SSL certificate, the certificate must be replaced with one signed by a publicly trusted CA. The installation program requires a valid public CA-signed certificate to access to the Prism Central API. For more information about replacing the self-signed certificate, see the Nutanix AOS Security Guide.</p> <p>Prism Central certificates created using Let's Encrypt may need to be added to your system trust before you install an OpenShift Container Platform cluster. If you do not already have access to the Prism Central CA certificate bundle, it can often be exported from your browser after visiting the Prism Central URL.</p> <p>If your Prism Central certificate is not chained to a trusted public CA, the CA certificate must be added to the <code>additionalTrustBundle</code> section of <code>install-config.yaml</code> after it is created. Follow the process documented in OpenShift documentation to add the certificate. It is not required to configure the documented <code>proxy</code> sections, only to add the certificate. </p> <p>Additionally, after installation manfiests are created, the proxy spec in the cluster proxy manifest must be updated to specify that the <code>user-ca-bundle</code> CA bundle is trusted. For example, in <code>manifests/cluster-proxy-01-config.yaml</code>:</p> <pre><code>    apiVersion: config.openshift.io/v1\n    kind: Proxy\n    metadata:\n      creationTimestamp: null\n      name: cluster\n    spec:\n      trustedCA:\n        name: \"user-ca-bundle\"\n</code></pre> <p>Note</p> <p>Starting from Openshift 4.12, <code>additionalTrustBundlePolicy</code> can be specified in <code>install-config.yaml</code>. When setting the <code>additionalTrustBundlePolicy</code> to <code>Always</code>, the <code>user-ca-bundle</code> will automatically be configured in the <code>manifests/cluster-proxy-01-config.yaml</code> file. </p>"},{"location":"openshift/install/ipi/#firewall-requirements","title":"Firewall Requirements","text":"<p>During an IPI installation, Prism Central's Image Service directly downloads the Red Hat Enterprise Linux CoreOS (RHCOS) image that is required to install the cluster. The Image Service must have access to download the RHCOS image from <code>rhcos.mirror.openshift.com</code>.</p> <ol> <li>Review the OpenShift documentation for further steps on preparing your environment for installation.</li> </ol>"},{"location":"openshift/install/ipi/#installation","title":"Installation","text":"<ol> <li>Review the OpenShift documentation to complete the installation.</li> </ol>"},{"location":"openshift/install/ipi/#post-install","title":"Post Install","text":"<ol> <li>Follow the post install instructions to complete cluster configuration.</li> </ol>"},{"location":"openshift/operators/csi/","title":"Nutanix CSI Operator","text":""},{"location":"openshift/operators/csi/#overview","title":"Overview","text":"<p>The Nutanix CSI Operator for Kubernetes packages, deploys, manages, and upgrades the Nutanix CSI Driver on Kubernetes and OpenShift for dynamic provisioning of persistent volumes on the Nutanix Enterprise Cloud platform.</p>"},{"location":"openshift/operators/csi/#nutanix-csi-driver","title":"Nutanix CSI Driver","text":"<p>The Nutanix Container Storage Interface (CSI) Driver for Kubernetes leverages Nutanix Volumes and Nutanix Files to provide scalable and persistent storage for stateful applications.</p> <p>With Nutanix CSI Provider you can:</p> <ul> <li> <p>Provide persistent storage to your containers</p> </li> <li> <p>Leverage PVC ressources to consume dynamicaly Nutanix storage</p> </li> <li> <p>With Files storage classes, applications on multiple pods can access the same storage, and also have the benefit of multi-pod read and write access.</p> </li> </ul>"},{"location":"openshift/operators/csi/#installing-the-operator","title":"Installing the Operator","text":"<ol> <li> <p>Follow OpenShift documentation for adding Operators to a cluster.</p> <ol> <li> <p>Filter by the keyword \"Nutanix\" to find the CSI Operator.</p> <p> <li> <p>Install the Operator by using the \"openshift-cluster-csi-drivers\" namespace and selecting defaults.</p> </li>"},{"location":"openshift/operators/csi/#installing-the-csi-driver-using-the-operator","title":"Installing the CSI Driver using the Operator","text":"<ol> <li>In the OpenShift web console, navigate to the Operators \u2192 Installed Operators page.</li> <li>Select Nutanix CSI Operator.</li> <li> <p>Select Create instance and then Create.</p> <p>"},{"location":"openshift/operators/csi/#configuring-the-k8s-secret-and-storage-class","title":"Configuring the K8s secret and storage class","text":"<p>In order to use this driver, create the relevant storage classes and secrets using the OpenShift CLI, by followinig the below section:</p> <ol> <li> <p>Create a secret yaml file like the below example and apply (<code>oc -n openshift-cluster-csi-drivers apply -f &lt;filename&gt;</code>).</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: ntnx-secret\n  namespace: openshift-cluster-csi-drivers\nstringData:\n  # prism-element-ip:prism-port:admin:password\n  key: 10.0.0.14:9440:admin:password\n</code></pre> </li> <li> <p>Create storage class yaml like the below example and apply (<code>oc apply -f &lt;filename&gt;</code>).</p> <pre><code>kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: nutanix-volume\nprovisioner: csi.nutanix.com\nparameters:\n  csi.storage.k8s.io/provisioner-secret-name: ntnx-secret\n  csi.storage.k8s.io/provisioner-secret-namespace: openshift-cluster-csi-drivers\n  csi.storage.k8s.io/node-publish-secret-name: ntnx-secret\n  csi.storage.k8s.io/node-publish-secret-namespace: openshift-cluster-csi-drivers\n  csi.storage.k8s.io/controller-expand-secret-name: ntnx-secret\n  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-cluster-csi-drivers\n  csi.storage.k8s.io/fstype: ext4\n  dataServiceEndPoint: 10.0.0.15:3260\n  storageContainer: default-container\n  storageType: NutanixVolumes\n  #whitelistIPMode: ENABLED\n  #chapAuth: ENABLED\nallowVolumeExpansion: true\nreclaimPolicy: Delete\n</code></pre> </li> </ol> <p>Note: By default, new RHCOS based nodes are provisioned with the required <code>scsi-initiator-utils</code> package installed, but with the <code>iscsid</code> service disabled. This can result in messages like <code>iscsiadm: can not connect to iSCSI daemon (111)!</code>. When this occurs, confirm that the <code>iscsid.service</code> is running on worker nodes. It can be enabled and started globally using the Machine Config Operator or directly on each node using systemctl (<code>sudo systemctl enable --now iscsid</code>).</p> <p>See the Managing Storage section of CSI Driver documentation on the Nutanix Portal for more information on configuring storage classes. </p>"},{"location":"openshift/operators/csi/#using-the-nutanix-csi-operator-on-restricted-networks","title":"Using the Nutanix CSI Operator on restricted networks","text":"<p>For OpenShift Container Platform clusters that are installed on restricted networks, also known as disconnected clusters, Operator Lifecycle Manager (OLM) by default cannot access the Red Hat-provided OperatorHub sources hosted on remote registries because those remote sources require full internet connectivity.</p> <p>The Nutanix CSI Operator is fully compatible with a restricted networks architecture and supported in disconnected mode. Follow the OpenShift documentation to configure.</p> <p>You need to mirror the <code>certified-operator-index</code> and keep the <code>nutanixcsioperator</code> package in your pruned index.</p>"},{"location":"openshift/post-install/","title":"Post Install","text":""},{"location":"openshift/post-install/#install-the-nutanix-csi-operator-optional","title":"Install the Nutanix CSI Operator (Optional)","text":"<ol> <li>Follow documentation to install the CSI Operator and provision the driver.</li> </ol>"},{"location":"openshift/post-install/#openshift-image-registry-configuration-optional","title":"OpenShift Image registry configuration (Optional)","text":"<p>Based on requirements, choose one of the following options:</p> <p>Note: Block storage volumes like Nutanix Volumes with ReadWriteOnce configuration are supported but not recommended for use with the image registry on production clusters. An installation where the registry is configured on block storage is not highly available because the registry cannot have more than one replica.</p>"},{"location":"openshift/post-install/#option-a-provision-a-nutanix-volumes-pvc-and-modify-the-openshift-image-registry-configuration","title":"Option A: Provision a Nutanix Volumes PVC and modify the OpenShift Image registry configuration","text":"<ol> <li> <p>Create storage class yaml like the below example and apply (<code>oc apply -f &lt;filename&gt;</code>).</p> <pre><code>kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n    name: nutanix-volume\nprovisioner: csi.nutanix.com\nparameters:\n    csi.storage.k8s.io/provisioner-secret-name: ntnx-secret\n    csi.storage.k8s.io/provisioner-secret-namespace: openshift-cluster-csi-drivers\n    csi.storage.k8s.io/node-publish-secret-name: ntnx-secret\n    csi.storage.k8s.io/node-publish-secret-namespace: openshift-cluster-csi-drivers\n    csi.storage.k8s.io/controller-expand-secret-name: ntnx-secret\n    csi.storage.k8s.io/controller-expand-secret-namespace: openshift-cluster-csi-drivers\n    csi.storage.k8s.io/fstype: ext4\n    dataServiceEndPoint: 10.0.0.15:3260\n    storageContainer: default-container\n    storageType: NutanixVolumes\n    #whitelistIPMode: ENABLED\n    #chapAuth: ENABLED\nallowVolumeExpansion: true\nreclaimPolicy: Delete\n</code></pre> </li> <li> <p>Create a PVC yaml file like the below example and apply in the openshift-image-registry namespace (<code>oc -n openshift-image-registry apply -f &lt;filename&gt;</code>).</p> <pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n    name: image-registry-claim\n    namespace: openshift-image-registry\nspec:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n    requests:\n        storage: 100Gi\n    storageClassName: nutanix-volume\n</code></pre> </li> <li> <p>Configure OpenShift registry storage similarly to OpenShift documentation:</p> <ol> <li> <p>Modify the registry storage configuration:</p> <pre><code>oc edit configs.imageregistry.operator.openshift.io\n</code></pre> <p>Change the line:</p> <pre><code>storage: {}\n</code></pre> <p>To:</p> <pre><code>storage:\n    pvc:\n     claim: image-registry-claim\n</code></pre> <p>Change the line:</p> <pre><code>managementState: Removed\n</code></pre> <p>To:</p> <pre><code>managementState: Managed\n</code></pre> <p>Change the line:</p> <pre><code>rolloutStrategy: RollingUpdate\n</code></pre> <p>To:</p> <pre><code>rolloutStrategy: Recreate\n</code></pre> </li> </ol> </li> </ol> <p>Note: Block storage volumes like Nutanix Volumes with ReadWriteOnce configuration are supported but not recommended for use with the image registry on production clusters. An installation where the registry is configured on block storage is not highly available because the registry cannot have more than one replica. </p>"},{"location":"openshift/post-install/#option-b-provision-a-nutanix-files-pvc-and-modify-the-openshift-image-registry-configuration","title":"Option B: Provision a Nutanix Files PVC and modify the OpenShift Image registry configuration","text":"<p>Note: The below steps assume Nutanix Files is enabled in your cluster. Files provides a highly available and massively scalable data repository for a wide range of deployments and applications.</p> <ol> <li> <p>Create a dynamicly provisioned NFS storage class yaml file like the below example and apply (<code>oc apply -f &lt;filename&gt;</code>).</p> <pre><code>kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n    name: nutanix-files-dynamic\nprovisioner: csi.nutanix.com\nparameters:\n    dynamicProv: ENABLED\n    nfsServerName: nfs01\n    #nfsServerName above is File Server Name in Prism without DNS suffix, not the FQDN.\n    csi.storage.k8s.io/provisioner-secret-name: ntnx-secret\n    csi.storage.k8s.io/provisioner-secret-namespace: openshift-cluster-csi-drivers\nstorageType: NutanixFiles\n</code></pre> </li> <li> <p>Create a PVC yaml file like the below example and apply in the openshift-image-registry namespace (<code>oc -n openshift-image-registry apply -f &lt;filename&gt;</code>).</p> <pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n    name: image-registry-claim\n    namespace: openshift-image-registry\nspec:\n    accessModes:\n    - ReadWriteMany\n    resources:\n    requests:\n        storage: 100Gi\n    storageClassName: nutanix-files-dynamic\n</code></pre> </li> <li> <p>Configure OpenShift registry storage similarly to OpenShift documentation:</p> <ol> <li> <p>Modify the registry storage configuration:</p> <pre><code>oc edit configs.imageregistry.operator.openshift.io\n</code></pre> <p>Change the line:</p> <pre><code>storage: {}\n</code></pre> <p>To:</p> <pre><code>storage:\n    pvc:\n    claim: image-registry-claim\n</code></pre> <p>Change the line:</p> <pre><code>managementState: Removed\n</code></pre> <p>To:</p> <pre><code>managementState: Managed\n</code></pre> <p>Change the line:</p> <pre><code>replicas: 1\n</code></pre> <p>To:</p> <pre><code>replicas: 2\n</code></pre> </li> </ol> </li> </ol>"},{"location":"openshift/post-install/#option-c-using-nutanix-objects-as-backend-storage-for-openshift-image-registry","title":"Option C: Using Nutanix Objects as backend storage for OpenShift Image registry","text":"<p>Note: The below steps assume Nutanix Objects is enabled in your cluster. Objects provides a highly available and massively scalable S3-compatible Object Store.</p> <p>Prerequisite:</p> <ul> <li>Create a bucket used for the Image registry</li> <li>Create and assign a user with R/W access to the bucket</li> <li>Download the SSL CA cert from Nutanix Object Store</li> </ul> <p>Optional: Assign a trusted SSL certificate to your Object Store which contains a SAN for the used DNS name or an IP-SAN if no DNS is used.</p> <ol> <li> <p>Add the Nutanix Objects CA certificate to OpenShift (only needed if not using a trusted certificate):</p> <pre><code>oc create configmap custom-ca \\\n    --from-file=ca-bundle.crt=objectca.crt \\\n    -n openshift-config\n\noc patch proxy/cluster \\\n    --type=merge \\\n    --patch='{\"spec\":{\"trustedCA\":{\"name\":\"custom-ca\"}}}'\n</code></pre> </li> <li> <p>Create a secret containing the S3 credentials:</p> <pre><code>oc create secret generic image-registry-private-configuration-user \\\n--from-literal=REGISTRY_STORAGE_S3_ACCESSKEY=your-access-key \\\n--from-literal=REGISTRY_STORAGE_S3_SECRETKEY=your-secret-key \\\n--namespace openshift-image-registry\n</code></pre> </li> <li> <p>Modify the Image registry storage configuration:</p> <pre><code>oc edit configs.imageregistry.operator.openshift.io\n</code></pre> <p>Change the line:</p> <pre><code>storage: {}\n</code></pre> <p>To:</p> <pre><code>    storage:\n    s3:\n        bucket: \"your-bucket\"\n        regionEndpoint: \"https://path-to-your-object-store\"\n        region: \"us-east-1\"\n</code></pre> <p>Change the line:</p> <pre><code>managementState: Removed\n</code></pre> <p>To:</p> <pre><code>managementState: Managed\n</code></pre> </li> </ol>"}]}